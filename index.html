<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-19T00:00:00Z">2024-12-19</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">117</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tokenisation is NP-Complete 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15210v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15210v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Whittington, Gregor Bachmann, Tiago Pimentel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we prove the NP-completeness of two variants of tokenisation,
defined as the problem of compressing a dataset to at most $\delta$ symbols by
either finding a vocabulary directly (direct tokenisation), or selecting a
sequence of merge operations (bottom-up tokenisation).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LongBench v2: Towards Deeper Understanding and Reasoning on Realistic
  Long-context Multitasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15204v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15204v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, Jie Tang, Juanzi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces LongBench v2, a benchmark designed to assess the
ability of LLMs to handle long-context problems requiring deep understanding
and reasoning across real-world multitasks. LongBench v2 consists of 503
challenging multiple-choice questions, with contexts ranging from 8k to 2M
words, across six major task categories: single-document QA, multi-document QA,
long in-context learning, long-dialogue history understanding, code repository
understanding, and long structured data understanding. To ensure the breadth
and the practicality, we collect data from nearly 100 highly educated
individuals with diverse professional backgrounds. We employ both automated and
manual review processes to maintain high quality and difficulty, resulting in
human experts achieving only 53.7% accuracy under a 15-minute time constraint.
Our evaluation reveals that the best-performing model, when directly answers
the questions, achieves only 50.1% accuracy. In contrast, the o1-preview model,
which includes longer reasoning, achieves 57.7%, surpassing the human baseline
by 4%. These results highlight the importance of enhanced reasoning ability and
scaling inference-time compute to tackle the long-context challenges in
LongBench v2. The project is available at https://longbench2.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MMLU-CF: A Contamination-free Multi-task Language Understanding
  Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qihao Zhao, Yangyu Huang, Tengchao Lv, Lei Cui, Qinzheng Sun, Shaoguang Mao, Xin Zhang, Ying Xin, Qiufeng Yin, Scarlett Li, Furu Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multiple-choice question (MCQ) datasets like Massive Multitask Language
Understanding (MMLU) are widely used to evaluate the commonsense,
understanding, and problem-solving abilities of large language models (LLMs).
However, the open-source nature of these benchmarks and the broad sources of
training data for LLMs have inevitably led to benchmark contamination,
resulting in unreliable evaluation results. To alleviate this issue, we propose
a contamination-free and more challenging MCQ benchmark called MMLU-CF. This
benchmark reassesses LLMs' understanding of world knowledge by averting both
unintentional and malicious data leakage. To avoid unintentional data leakage,
we source data from a broader domain and design three decontamination rules. To
prevent malicious data leakage, we divide the benchmark into validation and
test sets with similar difficulty and subject distributions. The test set
remains closed-source to ensure reliable results, while the validation set is
publicly available to promote transparency and facilitate independent
verification. Our evaluation of mainstream LLMs reveals that the powerful
GPT-4o achieves merely a 5-shot score of 73.4% and a 0-shot score of 71.9% on
the test set, which indicates the effectiveness of our approach in creating a
more rigorous and contamination-free evaluation standard. The GitHub repository
is available at https://github.com/microsoft/MMLU-CF and the dataset refers to
https://huggingface.co/datasets/microsoft/MMLU-CF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Face the Facts! Evaluating RAG-based Fact-checking Pipelines in
  Realistic Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Russo, Stefano Menini, Jacopo Staiano, Marco Guerini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language Processing and Generation systems have recently shown the
potential to complement and streamline the costly and time-consuming job of
professional fact-checkers. In this work, we lift several constraints of
current state-of-the-art pipelines for automated fact-checking based on the
Retrieval-Augmented Generation (RAG) paradigm. Our goal is to benchmark, under
more realistic scenarios, RAG-based methods for the generation of verdicts -
i.e., short texts discussing the veracity of a claim - evaluating them on
stylistically complex claims and heterogeneous, yet reliable, knowledge bases.
Our findings show a complex landscape, where, for example, LLM-based retrievers
outperform other retrieval techniques, though they still struggle with
heterogeneous knowledge bases; larger models excel in verdict faithfulness,
while smaller models provide better context adherence, with human evaluations
favouring zero-shot and one-shot approaches for informativeness, and fine-tuned
models for emotional alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code and data at https://github.com/drusso98/face-the-facts</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LlamaFusion: Adapting <span class="highlight-title">Pretrain</span>ed Language Models for Multimodal
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, Lili Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LlamaFusion, a framework for empowering pretrained text-only large
language models (LLMs) with multimodal generative capabilities, enabling them
to understand and generate both text and images in arbitrary sequences.
LlamaFusion leverages existing Llama-3's weights for processing texts
autoregressively while introducing additional and parallel transformer modules
for processing images with diffusion. During training, the data from each
modality is routed to its dedicated modules: modality-specific feedforward
layers, query-key-value projections, and normalization layers process each
modality independently, while the shared self-attention layers allow
interactions across text and image features. By freezing the text-specific
modules and only training the image-specific modules, LlamaFusion preserves the
language capabilities of text-only LLMs while developing strong visual
understanding and generation abilities. Compared to methods that pretrain
multimodal generative models from scratch, our experiments demonstrate that,
LlamaFusion improves image understanding by 20% and image generation by 3.6%
using only 50% of the FLOPs while maintaining Llama-3's language capabilities.
We also demonstrate that this framework can adapt existing vision-language
models with multimodal generation ability. Overall, this framework not only
leverages existing computational investments in text-only LLMs but also enables
the parallel development of language and vision capabilities, presenting a
promising direction for efficient multimodal model development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Critical-Questions-of-Thought: Steering LLM reasoning with Argumentative
  Querying 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15177v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15177v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Castagna, Isabel Sassoon, Simon Parsons
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Studies have underscored how, regardless of the recent breakthrough and swift
advances in AI research, even state-of-the-art Large Language models (LLMs)
continue to struggle when performing logical and mathematical reasoning. The
results seem to suggest that LLMs still work as (highly advanced) data pattern
identifiers, scoring poorly when attempting to generalise and solve reasoning
problems the models have never previously seen or that are not close to samples
presented in their training data. To address this compelling concern, this
paper makes use of the notion of critical questions from the literature on
argumentation theory, focusing in particular on Toulmin's model of
argumentation. We show that employing these critical questions can improve the
reasoning capabilities of LLMs. By probing the rationale behind the models'
reasoning process, the LLM can assess whether some logical mistake is occurring
and correct it before providing the final reply to the user prompt. The
underlying idea is drawn from the gold standard of any valid argumentative
procedure: the conclusion is valid if it is entailed by accepted premises. Or,
to paraphrase such Aristotelian principle in a real-world approximation,
characterised by incomplete information and presumptive logic, the conclusion
is valid if not proved otherwise. This approach successfully steers the models'
output through a reasoning pipeline, resulting in better performance against
the baseline and its Chain-of-Thought (CoT) implementation. To this end, an
extensive evaluation of the proposed approach on the MT-Bench Reasoning and
Math tasks across a range of LLMs is provided.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>-A-Video: <span class="highlight-title">Prompt</span> Your Video Diffusion Model via Preference-Aligned
  LLM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yatai Ji, Jiacheng Zhang, Jie Wu, Shilong Zhang, Shoufa Chen, Chongjian GE, Peize Sun, Weifeng Chen, Wenqi Shao, Xuefeng Xiao, Weilin Huang, Ping Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-video models have made remarkable advancements through optimization
on high-quality text-video pairs, where the textual prompts play a pivotal role
in determining quality of output videos. However, achieving the desired output
often entails multiple revisions and iterative inference to refine
user-provided prompts. Current automatic methods for refining prompts encounter
challenges such as Modality-Inconsistency, Cost-Discrepancy, and Model-Unaware
when applied to text-to-video diffusion models. To address these problem, we
introduce an LLM-based prompt adaptation framework, termed as Prompt-A-Video,
which excels in crafting Video-Centric, Labor-Free and Preference-Aligned
prompts tailored to specific video diffusion model. Our approach involves a
meticulously crafted two-stage optimization and alignment system. Initially, we
conduct a reward-guided prompt evolution pipeline to automatically create
optimal prompts pool and leverage them for supervised fine-tuning (SFT) of the
LLM. Then multi-dimensional rewards are employed to generate pairwise data for
the SFT model, followed by the direct preference optimization (DPO) algorithm
to further facilitate preference alignment. Through extensive experimentation
and comparative analyses, we validate the effectiveness of Prompt-A-Video
across diverse generation models, highlighting its potential to push the
boundaries of video generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models as Continuous Self-Evolving Data Engineers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peidong Wang, Ming Wang, Zhiming Ma, Xiaocui Yang, Shi Feng, Daling Wang, Yifei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities on
various tasks, while the further evolvement is limited to the lack of
high-quality training data. In addition, traditional training approaches rely
too much on expert-labeled data, setting an upper limit on the performance of
LLMs. To address this issue, we propose a novel paradigm that enables LLMs to
train itself by autonomously generating, cleaning, reviewing, and annotating
data with preference information, named LANCE. Our approach demonstrates that
LLMs can serve as continuous self-evolving data engineers, significantly
reducing the time and cost of the post-training data construction process.
Through iterative fine-tuning on different variants of the Qwen2, we validate
the effectiveness of LANCE across various tasks, showing that it can
continuously improve model performance and maintain high-quality data
generation. Across eight benchmark dimensions, LANCE resulted in an average
score enhancement of 3.36 for Qwen2-7B and 2.70 for Qwen2-7B-Instruct. This
training paradigm with autonomous data construction not only reduces the
reliance on human experts or external models but also ensures that the data
aligns with human values and preferences, paving the way for the development of
future superintelligent systems that can exceed human capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Pruning for Large Language Models with Structural Importance
  Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotian Zheng, Jinke Ren, Yushan Sun, Ruichen Zhang, Wenbo Zhang, Zhen Li, Dusit Niyato, Shuguang Cui, Yatong Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advancements in large language models (LLMs) have significantly
improved language understanding and generation capabilities. However, it is
difficult to deploy LLMs on resource-constrained edge devices due to their high
computational and storage resource demands. To address this issue, we propose a
novel LLM model pruning method, namely structurally-aware adaptive pruning
(SAAP), to significantly reduce the computational and memory costs while
maintaining model performance. We first define an adaptive importance fusion
metric to evaluate the importance of all coupled structures in LLMs by
considering their homoscedastic uncertainty. Then, we rank the importance of
all modules to determine the specific layers that should be pruned to meet
particular performance requirements. Furthermore, we develop a new group
fine-tuning strategy to improve the inference efficiency of LLMs. Finally, we
evaluate the proposed SAAP method on multiple LLMs across two common tasks,
i.e., zero-shot classification and text generation. Experimental results show
that our SAAP method outperforms several state-of-the-art baseline methods,
achieving 2.17%, 2.37%, and 2.39% accuracy gains on LLaMA-7B, Vicuna-7B, and
LLaMA-13B. Additionally, SAAP improves the token generation speed by 5%,
showcasing its practical advantages in resource-constrained scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Outcome-Refining Process Supervision for Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuohao Yu, Weizheng Gu, Yidong Wang, Zhengran Zeng, Jindong Wang, Wei Ye, Shikun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models have demonstrated remarkable capabilities in code
generation, yet they often struggle with complex programming tasks that require
deep algorithmic reasoning. While process supervision through learned reward
models shows promise in guiding reasoning steps, it requires expensive training
data and suffers from unreliable evaluation. We propose Outcome-Refining
Process Supervision, a novel paradigm that treats outcome refinement itself as
the process to be supervised. Our framework leverages concrete execution
signals to ground the supervision of reasoning steps, while using
tree-structured exploration to maintain multiple solution trajectories
simultaneously. Experiments demonstrate that our approach enables even smaller
models to achieve high success accuracy and performance metrics on competitive
programming tasks, creates more reliable verification than traditional reward
models without requiring training PRMs. Our approach achieves significant
improvements across 5 models and 3 datasets: an average of 26.9% increase in
correctness and 42.2% in efficiency. The results suggest that providing
structured reasoning space with concrete verification signals is crucial for
solving complex programming tasks. We open-source all our code and data at:
https://github.com/zhuohaoyu/ORPS
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 5 figures, Code: https://github.com/zhuohaoyu/ORPS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Qwen2.5 Technical Report 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Qwen,  :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zihan Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this report, we introduce Qwen2.5, a comprehensive series of large
language models (LLMs) designed to meet diverse needs. Compared to previous
iterations, Qwen 2.5 has been significantly improved during both the
pre-training and post-training stages. In terms of pre-training, we have scaled
the high-quality pre-training datasets from the previous 7 trillion tokens to
18 trillion tokens. This provides a strong foundation for common sense, expert
knowledge, and reasoning capabilities. In terms of post-training, we implement
intricate supervised finetuning with over 1 million samples, as well as
multistage reinforcement learning. Post-training techniques enhance human
preference, and notably improve long text generation, structural data analysis,
and instruction following. To handle diverse and varied use cases effectively,
we present Qwen2.5 LLM series in rich sizes. Open-weight offerings include base
and instruction-tuned models, with quantized versions available. In addition,
for hosted solutions, the proprietary models currently include two
mixture-of-experts (MoE) variants: Qwen2.5-Turbo and Qwen2.5-Plus, both
available from Alibaba Cloud Model Studio. Qwen2.5 has demonstrated top-tier
performance on a wide range of benchmarks evaluating language understanding,
reasoning, mathematics, coding, human preference alignment, etc. Specifically,
the open-weight flagship Qwen2.5-72B-Instruct outperforms a number of open and
proprietary models and demonstrates competitive performance to the
state-of-the-art open-weight model, Llama-3-405B-Instruct, which is around 5
times larger. Qwen2.5-Turbo and Qwen2.5-Plus offer superior cost-effectiveness
while performing competitively against GPT-4o-mini and GPT-4o respectively.
Additionally, as the foundation, Qwen2.5 models have been instrumental in
training specialized models such as Qwen2.5-Math, Qwen2.5-Coder, QwQ, and
multimodal models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Associative memory inspires improvements for in-context learning using a
  novel attention residual stream architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas F Burns, Tomoki Fukai, Christopher J Earls
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) demonstrate an impressive ability to utilise
information within the context of their input sequences to appropriately
respond to data unseen by the LLM during its training procedure. This ability
is known as in-context learning (ICL). Humans and non-human animals demonstrate
similar abilities, however their neural architectures differ substantially from
LLMs. Despite this, a critical component within LLMs, the attention mechanism,
resembles modern associative memory models, widely used in and influenced by
the computational neuroscience community to model biological memory systems.
Using this connection, we introduce an associative memory model capable of
performing ICL. We use this as inspiration for a novel residual stream
architecture which allows information to directly flow between attention heads.
We test this architecture during training within a two-layer Transformer and
show its ICL abilities manifest more quickly than without this modification. We
then apply our architecture in small language models with 8 million parameters,
focusing on attention head values, with results also indicating improved ICL
performance at this larger and more naturalistic scale.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Review</span>-Then-Refine: A Dynamic Framework for Multi-Hop Question Answering
  with Temporal Adaptability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangsen Chen, Xuming Hu, Nan Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieve-augmented generation (RAG) frameworks have emerged as a promising
solution to multi-hop question answering(QA) tasks since it enables large
language models (LLMs) to incorporate external knowledge and mitigate their
inherent knowledge deficiencies. Despite this progress, existing RAG
frameworks, which usually follows the retrieve-then-read paradigm, often
struggle with multi-hop QA with temporal information since it has difficulty
retrieving and synthesizing accurate time-related information. To address the
challenge, this paper proposes a novel framework called review-then-refine,
which aims to enhance LLM performance in multi-hop QA scenarios with temporal
information. Our approach begins with a review phase, where decomposed
sub-queries are dynamically rewritten with temporal information, allowing for
subsequent adaptive retrieval and reasoning process. In addition, we implement
adaptive retrieval mechanism to minimize unnecessary retrievals, thus reducing
the potential for hallucinations. In the subsequent refine phase, the LLM
synthesizes the retrieved information from each sub-query along with its
internal knowledge to formulate a coherent answer. Extensive experimental
results across multiple datasets demonstrate the effectiveness of our proposed
framework, highlighting its potential to significantly improve multi-hop QA
capabilities in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Cross-Domain Study of the Use of Persuasion Techniques in Online
  Disinformation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João A. Leite, Olesya Razuvayevskaya, Carolina Scarton, Kalina Bontcheva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Disinformation, irrespective of domain or language, aims to deceive or
manipulate public opinion, typically through employing advanced persuasion
techniques. Qualitative and quantitative research on the weaponisation of
persuasion techniques in disinformation has been mostly topic-specific (e.g.,
COVID-19) with limited cross-domain studies, resulting in a lack of
comprehensive understanding of these strategies. This study employs a
state-of-the-art persuasion technique classifier to conduct a large-scale,
multi-domain analysis of the role of 16 persuasion techniques in disinformation
narratives. It shows how different persuasion techniques are employed
disproportionately in different disinformation domains. We also include a
detailed case study on climate change disinformation, highlighting how
linguistic, psychological, and cultural factors shape the adaptation of
persuasion strategies to fit unique thematic contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce AceMath, a suite of frontier math models that
excel in solving complex math problems, along with highly effective reward
models capable of evaluating generated solutions and reliably identifying the
correct ones. To develop the instruction-tuned math models, we propose a
supervised fine-tuning (SFT) process that first achieves competitive
performance across general domains, followed by targeted fine-tuning for the
math domain using a carefully curated set of prompts and synthetically
generated responses. The resulting model, AceMath-72B-Instruct greatly
outperforms Qwen2.5-Math-72B-Instruct, GPT-4o and Claude-3.5 Sonnet. To develop
math-specialized reward model, we first construct AceMath-RewardBench, a
comprehensive and robust benchmark for evaluating math reward models across
diverse problems and difficulty levels. After that, we present a systematic
approach to build our math reward models. The resulting model, AceMath-72B-RM,
consistently outperforms state-of-the-art reward models. Furthermore, when
combining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highest
average rm@8 score across the math reasoning benchmarks. We will release model
weights, training data, and evaluation benchmarks at:
https://research.nvidia.com/labs/adlr/acemath
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Till the Layers Collapse: Compressing a Deep Neural Network through the
  Lenses of Batch Normalization Layers <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhu Liao, Nour Hezbri, Victor Quétu, Van-Tam Nguyen, Enzo Tartaglione
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today, deep neural networks are widely used since they can handle a variety
of complex tasks. Their generality makes them very powerful tools in modern
technology. However, deep neural networks are often overparameterized. The
usage of these large models consumes a lot of computation resources. In this
paper, we introduce a method called \textbf{T}ill the \textbf{L}ayers
\textbf{C}ollapse (TLC), which compresses deep neural networks through the
lenses of batch normalization layers. By reducing the depth of these networks,
our method decreases deep neural networks' computational requirements and
overall latency. We validate our method on popular models such as Swin-T,
MobileNet-V2, and RoBERTa, across both image classification and natural
language processing (NLP) tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Confli<span class="highlight-title">BERT</span>: A Language Model for Political Conflict 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick T. Brandt, Sultan Alsarra, Vito J. D`Orazio, Dagmar Heintze, Latifur Khan, Shreyas Meher, Javier Osorio, Marcus Sianan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conflict scholars have used rule-based approaches to extract information
about political violence from news reports and texts. Recent Natural Language
Processing developments move beyond rigid rule-based approaches. We review our
recent ConfliBERT language model (Hu et al. 2022) to process political and
violence related texts. The model can be used to extract actor and action
classifications from texts about political conflict. When fine-tuned, results
show that ConfliBERT has superior performance in accuracy, precision and recall
over other large language models (LLM) like Google's Gemma 2 (9B), Meta's Llama
3.1 (7B), and Alibaba's Qwen 2.5 (14B) within its relevant domains. It is also
hundreds of times faster than these more generalist LLMs. These results are
illustrated using texts from the BBC, re3d, and the Global Terrorism Dataset
(GTD).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 4 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Gaps 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Friedrich, Simone Tedeschi, Patrick Schramowski, Manuel Brack, Roberto Navigli, Huu Nguyen, Bo Li, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building safe Large Language Models (LLMs) across multiple languages is
essential in ensuring both safe access and linguistic diversity. To this end,
we introduce M-ALERT, a multilingual benchmark that evaluates the safety of
LLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT
includes 15k high-quality prompts per language, totaling 75k, following the
detailed ALERT taxonomy. Our extensive experiments on 10 state-of-the-art LLMs
highlight the importance of language-specific safety analysis, revealing that
models often exhibit significant inconsistencies in safety across languages and
categories. For instance, Llama3.2 shows high unsafety in the category
crime_tax for Italian but remains safe in other languages. Similar differences
can be observed across all models. In contrast, certain categories, such as
substance_cannabis and crime_propaganda, consistently trigger unsafe responses
across models and languages. These findings underscore the need for robust
multilingual safety practices in LLMs to ensure safe and responsible usage
across diverse user communities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models and Code Security: A Systematic Literature <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15004v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15004v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enna Basic, Alberto Giaretta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have emerged as powerful tools for automating
various programming tasks, including security-related ones, such as detecting
and fixing vulnerabilities. Despite their promising capabilities, when required
to produce or modify pre-existing code, LLMs could introduce vulnerabilities
unbeknown to the programmer. When analyzing code, they could miss clear
vulnerabilities or signal nonexistent ones. In this Systematic Literature
Review (SLR), we aim to investigate both the security benefits and potential
drawbacks of using LLMs for a variety of code-related tasks. In particular,
first we focus on the types of vulnerabilities that could be introduced by
LLMs, when used for producing code. Second, we analyze the capabilities of LLMs
to detect and fix vulnerabilities, in any given code, and how the prompting
strategy of choice impacts their performance in these two tasks. Last, we
provide an in-depth analysis on how data poisoning attacks on LLMs can impact
performance in the aforementioned tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chain-of-MetaWriting: Linguistic and Textual Analysis of How Small
  Language Models Write Young Students Texts <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioana Buhnila, Georgeta Cislaru, Amalia Todirascu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have been used to generate texts in response to
different writing tasks: reports, essays, story telling. However, language
models do not have a meta-representation of the text writing process, nor
inherent communication learning needs, comparable to those of young human
students. This paper introduces a fine-grained linguistic and textual analysis
of multilingual Small Language Models' (SLMs) writing. With our method,
Chain-of-MetaWriting, SLMs can imitate some steps of the human writing process,
such as planning and evaluation. We mainly focused on short story and essay
writing tasks in French for schoolchildren and undergraduate students
respectively. Our results show that SLMs encounter difficulties in assisting
young students on sensitive topics such as violence in the schoolyard, and they
sometimes use words too complex for the target audience. In particular, the
output is quite different from the human produced texts in term of text
cohesion and coherence regarding temporal connectors, topic progression,
reference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WRAICOGS 2025 (Writing Aids at the Crossroads of AI,
  Cognitive Science, and NLP) co-located with COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Movie2Story: A framework for understanding videos and telling stories in
  the form of novel text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kangning Li, Zheyang Jia, Anyu Ying
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal video-to-text models have made considerable progress, primarily in
generating brief descriptions of video content. However, there is still a
deficiency in generating rich long-form text descriptions that integrate both
video and audio. In this paper, we introduce a framework called M2S, designed
to generate novel-length text by combining audio, video, and character
recognition. M2S includes modules for video long-form text description and
comprehension, audio-based analysis of emotion, speech rate, and character
alignment, and visual-based character recognition alignment. By integrating
multimodal information using the large language model GPT4o, M2S stands out in
the field of multimodal text generation. We demonstrate the effectiveness and
accuracy of M2S through comparative experiments and human evaluation.
Additionally, the model framework has good scalability and significant
potential for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Injection via <span class="highlight-title">Prompt</span> Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kalle Kujanpää, Harri Valpola, Alexander Ilin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many practical applications, large language models (LLMs) need to
incorporate new knowledge not present in their pre-training data. The primary
methods for this are fine-tuning and retrieval-augmented generation (RAG).
Although RAG has emerged as the industry standard for knowledge injection,
fine-tuning has not yet achieved comparable success. In this paper, we propose
a new fine-tuning technique for learning new knowledge and show that it can
reach the performance of RAG. The proposed method is based on the
self-distillation approach, which we call prompt distillation. First, we
generate question-answer pairs about the new knowledge. Then, we fine-tune a
student model on the question-answer pairs to imitate the output distributions
of a teacher model, which additionally receives the new knowledge in its
prompt. The student model is identical to the teacher, except it is equipped
with a LoRA adapter. This training procedure facilitates distilling the new
knowledge from the teacher's prompt into the student's weights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Dark Side of LLMs' Intrinsic Self-Correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingjie Zhang, Han Qiu, Di Wang, Haoting Qian, Yiming Li, Tianwei Zhang, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intrinsic self-correction was proposed to improve LLMs' responses via
feedback prompts solely based on their inherent capability. However, recent
works show that LLMs' intrinsic self-correction fails without oracle labels as
feedback prompts. In this paper, we aim to interpret LLMs' intrinsic
self-correction for different tasks, especially for those failure cases. By
including one simple task and three complex tasks with state-of-the-art (SOTA)
LLMs like ChatGPT families (o1, 4o, 3.5-turbo) and Llama families (2-7B, 3-8B,
and 3.1-8B), we design three interpretation methods to reveal the dark side of
LLMs' intrinsic self-correction. We identify intrinsic self-correction can (1)
cause LLMs to waver both intermedia and final answers and lead to prompt bias
on simple factual questions; (2) introduce human-like cognitive bias on complex
tasks. In light of our findings, we also provide two simple yet effective
strategies for alleviation: question repeating and supervised fine-tuning with
a few samples. We open-source our work at https://x-isc.info/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RobustFT: Robust Supervised Fine-tuning for Large Language Models under
  Noisy Response 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14922v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14922v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Luo, Xiao Luo, Kaize Ding, Jingyang Yuan, Zhiping Xiao, Ming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised fine-tuning (SFT) plays a crucial role in adapting large language
models (LLMs) to specific domains or tasks. However, as demonstrated by
empirical experiments, the collected data inevitably contains noise in
practical applications, which poses significant challenges to model performance
on downstream tasks. Therefore, there is an urgent need for a noise-robust SFT
framework to enhance model capabilities in downstream tasks. To address this
challenge, we introduce a robust SFT framework (RobustFT) that performs noise
detection and relabeling on downstream task data. For noise identification, our
approach employs a multi-expert collaborative system with inference-enhanced
models to achieve superior noise detection. In the denoising phase, we utilize
a context-enhanced strategy, which incorporates the most relevant and confident
knowledge followed by careful assessment to generate reliable annotations.
Additionally, we introduce an effective data selection mechanism based on
response entropy, ensuring only high-quality samples are retained for
fine-tuning. Extensive experiments conducted on multiple LLMs across five
datasets demonstrate RobustFT's exceptional performance in noisy scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dehallucinating Parallel Context Extension for Retrieval-Augmented
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zexiong Ma, Shengnan An, Zeqi Lin, Yanzhen Zou, Jian-Guang Lou, Bing Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are susceptible to generating hallucinated
information, despite the integration of retrieval-augmented generation (RAG).
Parallel context extension (PCE) is a line of research attempting to
effectively integrating parallel (unordered) contexts, while it still suffers
from hallucinations when adapted to RAG scenarios. In this paper, we propose
DePaC (Dehallucinating Parallel Context Extension), which alleviates the
hallucination problem with context-aware negative training and
information-calibrated aggregation. DePaC is designed to alleviate two types of
in-context hallucination: fact fabrication (i.e., LLMs present claims that are
not supported by the contexts) and fact omission (i.e., LLMs fail to present
claims that can be supported by the contexts). Specifically, (1) for fact
fabrication, we apply the context-aware negative training that fine-tunes the
LLMs with negative supervisions, thus explicitly guiding the LLMs to refuse to
answer when contexts are not related to questions; (2) for fact omission, we
propose the information-calibrated aggregation which prioritizes context
windows with higher information increment from their contexts. The experimental
results on nine RAG tasks demonstrate that DePaC significantly alleviates the
two types of hallucination and consistently achieves better performances on
these tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why language models collapse when trained on recursively generated text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lecheng Wang, Xianjie Shi, Ge Li, Jia Li, Yihong Dong, Xuanming Zhang, Wenpin Jiao, Hong Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs) have been widely used to generate text on the Internet.
The generated text is often collected into the training corpus of the next
generations of LMs. Previous work has experimentally found that LMs collapse
when trained on recursively generated text. This paper contributes to existing
knowledge from two aspects. We present a theoretical proof of LM collapse. Our
proof reveals the cause of LM collapse and proves that all auto-regressive LMs
will definitely collapse. We present a new finding: the performance of LMs
gradually declines when trained on recursively generated text until they
perform no better than a randomly initialized LM. The trained LMs produce large
amounts of repetitive text and perform poorly across a wide range of natural
language tasks. The above proof and new findings deepen our understanding of LM
collapse and offer valuable insights that may inspire new training techniques
to mitigate this threat.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-Convolutional Networks: Named Entity Recognition and Large
  Language Model Embedding in Document Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14867v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14867v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Imed Keraghel, Mohamed Nadif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in machine learning, particularly Large Language Models
(LLMs) such as BERT and GPT, provide rich contextual embeddings that improve
text representation. However, current document clustering approaches often
ignore the deeper relationships between named entities (NEs) and the potential
of LLM embeddings. This paper proposes a novel approach that integrates Named
Entity Recognition (NER) and LLM embeddings within a graph-based framework for
document clustering. The method builds a graph with nodes representing
documents and edges weighted by named entity similarity, optimized using a
graph-convolutional network (GCN). This ensures a more effective grouping of
semantically related documents. Experimental results indicate that our approach
outperforms conventional co-occurrence-based methods in clustering, notably for
documents rich in named entities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Think&Cite: Improving Attributed Text Generation with Self-Guided Tree
  Search and Progress Reward Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14860v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14860v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyi Li, Hwee Tou Ng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their outstanding capabilities, large language models (LLMs) are
prone to hallucination and producing factually incorrect information. This
challenge has spurred efforts in attributed text generation, which prompts LLMs
to generate content with supporting evidence. In this paper, we propose a novel
framework, called Think&Cite, and formulate attributed text generation as a
multi-step reasoning problem integrated with search. Specifically, we propose
Self-Guided Monte Carlo Tree Search (SG-MCTS), which capitalizes on the
self-reflection capability of LLMs to reflect on the intermediate states of
MCTS for guiding the tree expansion process. To provide reliable and
comprehensive feedback, we introduce Progress Reward Models to measure the
progress of tree search from the root to the current state from two aspects,
i.e., generation and attribution progress. We conduct extensive experiments on
three datasets and the results show that our approach significantly outperforms
baseline approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DS$^2$-ABSA: Dual-Stream Data Synthesis with Label Refinement for
  Few-Shot Aspect-Based Sentiment Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongling Xu, Yice Zhang, Qianlong Wang, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently developed large language models (LLMs) have presented promising new
avenues to address data scarcity in low-resource scenarios. In few-shot
aspect-based sentiment analysis (ABSA), previous efforts have explored data
augmentation techniques, which prompt LLMs to generate new samples by modifying
existing ones. However, these methods fail to produce adequately diverse data,
impairing their effectiveness. Besides, some studies apply in-context learning
for ABSA by using specific instructions and a few selected examples as prompts.
Though promising, LLMs often yield labels that deviate from task requirements.
To overcome these limitations, we propose DS$^2$-ABSA, a dual-stream data
synthesis framework targeted for few-shot ABSA. It leverages LLMs to synthesize
data from two complementary perspectives: \textit{key-point-driven} and
\textit{instance-driven}, which effectively generate diverse and high-quality
ABSA samples in low-resource settings. Furthermore, a \textit{label refinement}
module is integrated to improve the synthetic labels. Extensive experiments
demonstrate that DS$^2$-ABSA significantly outperforms previous few-shot ABSA
solutions and other LLM-oriented data generation methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of RWKV 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14847v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14847v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Li, Tingyu Xia, Yi Chang, Yuan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Receptance Weighted Key Value (RWKV) model offers a novel alternative to
the Transformer architecture, merging the benefits of recurrent and
attention-based systems. Unlike conventional Transformers, which depend heavily
on self-attention, RWKV adeptly captures long-range dependencies with minimal
computational demands. By utilizing a recurrent framework, RWKV addresses some
computational inefficiencies found in Transformers, particularly in tasks with
long sequences. RWKV has recently drawn considerable attention for its robust
performance across multiple domains. Despite its growing popularity, no
systematic review of the RWKV model exists. This paper seeks to fill this gap
as the first comprehensive review of the RWKV architecture, its core
principles, and its varied applications, such as natural language generation,
natural language understanding, and computer vision. We assess how RWKV
compares to traditional Transformer models, highlighting its capability to
manage long sequences efficiently and lower computational costs. Furthermore,
we explore the challenges RWKV encounters and propose potential directions for
future research and advancement. We consistently maintain the related
open-source materials at: https://github.com/MLGroupJLU/RWKV-Survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mapping and Influencing the Political Ideology of Large Language Models
  using Synthetic Personas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pietro Bernardelle, Leon Fröhling, Stefano Civelli, Riccardo Lunardi, Kevin Roiter, Gianluca Demartini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The analysis of political biases in large language models (LLMs) has
primarily examined these systems as single entities with fixed viewpoints.
While various methods exist for measuring such biases, the impact of
persona-based prompting on LLMs' political orientation remains unexplored. In
this work we leverage PersonaHub, a collection of synthetic persona
descriptions, to map the political distribution of persona-based prompted LLMs
using the Political Compass Test (PCT). We then examine whether these initial
compass distributions can be manipulated through explicit ideological prompting
towards diametrically opposed political orientations: right-authoritarian and
left-libertarian. Our experiments reveal that synthetic personas predominantly
cluster in the left-libertarian quadrant, with models demonstrating varying
degrees of responsiveness when prompted with explicit ideological descriptors.
While all models demonstrate significant shifts towards right-authoritarian
positions, they exhibit more limited shifts towards left-libertarian positions,
suggesting an asymmetric response to ideological manipulation that may reflect
inherent biases in model training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 2 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DynamicKV: Task-Aware Adaptive KV Cache Compression for Long Context
  LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiabin Zhou, Wenbin Wang, Minyan Zeng, Jiaxian Guo, Xuebo Liu, Li Shen, Min Zhang, Liang Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient KV cache management in LLMs is crucial for long-context tasks like
RAG and summarization. Existing KV cache compression methods enforce a fixed
pattern, neglecting task-specific characteristics and reducing the retention of
essential information. However, we observe distinct activation patterns across
layers in various tasks, highlighting the need for adaptive strategies tailored
to each task's unique demands. Based on this insight, we propose DynamicKV, a
method that dynamically optimizes token retention by adjusting the number of
tokens retained at each layer to adapt to the specific task. DynamicKV
establishes global and per-layer maximum KV cache budgets, temporarily
retaining the maximum budget for the current layer, and periodically updating
the KV cache sizes of all preceding layers during inference. Our method retains
only 1.7% of the KV cache size while achieving ~85% of the Full KV cache
performance on LongBench. Notably, even under extreme compression (0.9%),
DynamicKV surpasses state-of-the-art (SOTA) methods by 11% in the
Needle-in-a-Haystack test using Mistral-7B-Instruct-v0.2. The code will be
released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Multimodal Reasoning via Active Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14835v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14835v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanting Dong, Chenghao Zhang, Mengjie Deng, Yutao Zhu, Zhicheng Dou, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-step multimodal reasoning tasks pose significant challenges for
multimodal large language models (MLLMs), and finding effective ways to enhance
their performance in such scenarios remains an unresolved issue. In this paper,
we propose AR-MCTS, a universal framework designed to progressively improve the
reasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo
Tree Search (MCTS). Our approach begins with the development of a unified
retrieval module that retrieves key supporting insights for solving complex
reasoning problems from a hybrid-modal retrieval corpus. To bridge the gap in
automated multimodal reasoning verification, we employ the MCTS algorithm
combined with an active retrieval mechanism, which enables the automatic
generation of step-wise annotations. This strategy dynamically retrieves key
insights for each reasoning step, moving beyond traditional beam search
sampling to improve the diversity and reliability of the reasoning space.
Additionally, we introduce a process reward model that aligns progressively to
support the automatic verification of multimodal reasoning tasks. Experimental
results across three complex multimodal reasoning benchmarks confirm the
effectiveness of the AR-MCTS framework in enhancing the performance of various
multimodal models. Further analysis demonstrates that AR-MCTS can optimize
sampling diversity and accuracy, yielding reliable multimodal reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mention Attention for Pronoun Translation <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14829v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14829v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gongbo Tang, Christian Hardmeier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most pronouns are referring expressions, computers need to resolve what do
the pronouns refer to, and there are divergences on pronoun usage across
languages. Thus, dealing with these divergences and translating pronouns is a
challenge in machine translation. Mentions are referring candidates of pronouns
and have closer relations with pronouns compared to general tokens. We assume
that extracting additional mention features can help pronoun translation.
Therefore, we introduce an additional mention attention module in the decoder
to pay extra attention to source mentions but not non-mention tokens. Our
mention attention module not only extracts features from source mentions, but
also considers target-side context which benefits pronoun translation. In
addition, we also introduce two mention classifiers to train models to
recognize mentions, whose outputs guide the mention attention. We conduct
experiments on the WMT17 English-German translation task, and evaluate our
models on general translation and pronoun translation, using BLEU, APT, and
contrastive evaluation metrics. Our proposed model outperforms the baseline
Transformer model in terms of APT and BLEU scores, this confirms our hypothesis
that we can improve pronoun translation by paying additional attention to
source mentions, and shows that our introduced additional modules do not have
negative effect on the general translation quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>camera-ready version of the paper accepted by JCRAI-23 conference, in
  ACL format</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ResoFilter: Rine-grained Synthetic Data Filtering for Large Language
  Models through Data-Parameter Resonance Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14809v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14809v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeao Tu, Xiangdi Meng, Yu He, Zihan Yao, Tianyu Qi, Jun Liu, Ming Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable effectiveness across
various domains, with data augmentation methods utilizing GPT for synthetic
data generation becoming prevalent. However, the quality and utility of
augmented data remain questionable, and current methods lack clear metrics for
evaluating data characteristics. To address these challenges, we propose
ResoFilter, a novel method that integrates models, data, and tasks to refine
datasets. ResoFilter leverages the fine-tuning process to obtain Data-Parameter
features for data selection, offering improved interpretability by representing
data characteristics through model weights. Our experiments demonstrate that
ResoFilter achieves comparable results to full-scale fine-tuning using only
half the data in mathematical tasks and exhibits strong generalization across
different models and domains. This method provides valuable insights for
constructing synthetic datasets and evaluating high-quality data, offering a
promising solution for enhancing data augmentation techniques and improving
training dataset quality for LLMs. For reproducibility, we will release our
code and data upon acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Disentangling Reasoning Tokens and Boilerplate Tokens For Language Model
  Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14780v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14780v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziang Ye, Zhenru Zhang, Yang Zhang, Jianxin Ma, Junyang Lin, Fuli Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When using agent-task datasets to enhance agent capabilities for Large
Language Models (LLMs), current methodologies often treat all tokens within a
sample equally. However, we argue that tokens serving different roles -
specifically, reasoning tokens versus boilerplate tokens (e.g., those governing
output format) - differ significantly in importance and learning complexity,
necessitating their disentanglement and distinct treatment. To address this, we
propose a novel Shuffle-Aware Discriminator (SHAD) for adaptive token
discrimination. SHAD classifies tokens by exploiting predictability differences
observed after shuffling input-output combinations across samples: boilerplate
tokens, due to their repetitive nature among samples, maintain predictability,
whereas reasoning tokens do not. Using SHAD, we propose the
Reasoning-highlighted Fine-Tuning (RFT) method, which adaptively emphasizes
reasoning tokens during fine-tuning, yielding notable performance gains over
common Supervised Fine-Tuning (SFT).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ALKAFI-LLAMA3: Fine-Tuning LLMs for Precise Legal Understanding in
  Palestine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rabee Qasem, Mohannad Hendi, Banan Tantour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable potential in
diverse domains, yet their application in the legal sector, particularly in
low-resource contexts, remains limited. This study addresses the challenges of
adapting LLMs to the Palestinian legal domain, where political instability,
fragmented legal frameworks, and limited AI resources hinder effective
machine-learning applications. We present a fine-tuned model based on a
quantized version of Llama-3.2-1B-Instruct, trained on a synthetic data set
derived from Palestinian legal texts. Using smaller-scale models and
strategically generated question-answer pairs, we achieve a cost-effective,
locally sustainable solution that provides accurate and contextually relevant
legal guidance. Our experiments demonstrate promising performance on various
query types, ranging from yes/no questions and narrative explanations to
complex legal differentiations, while highlighting areas for improvement, such
as handling calculation-based inquiries and structured list formatting. This
work provides a pathway for the deployment of AI-driven legal assistance tools
tailored to the needs of resource-constrained environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PsyDraw: A Multi-Agent Multimodal System for Mental Health Screening in
  Left-Behind Children 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiqun Zhang, Xiaocui Yang, Xiaobai Li, Siyuan Yu, Yi Luan, Shi Feng, Daling Wang, Yifei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Left-behind children (LBCs), numbering over 66 million in China, face severe
mental health challenges due to parental migration for work. Early screening
and identification of at-risk LBCs is crucial, yet challenging due to the
severe shortage of mental health professionals, especially in rural areas.
While the House-Tree-Person (HTP) test shows higher child participation rates,
its requirement for expert interpretation limits its application in
resource-scarce regions. To address this challenge, we propose PsyDraw, a
multi-agent system based on Multimodal Large Language Models that assists
mental health professionals in analyzing HTP drawings. The system employs
specialized agents for feature extraction and psychological interpretation,
operating in two stages: comprehensive feature analysis and professional report
generation. Evaluation of HTP drawings from 290 primary school students reveals
that 71.03% of the analyzes achieved High Consistency with professional
evaluations, 26.21% Moderate Consistency and only 2.41% Low Consistency. The
system identified 31.03% of cases requiring professional attention,
demonstrating its effectiveness as a preliminary screening tool. Currently
deployed in pilot schools, \method shows promise in supporting mental health
professionals, particularly in resource-limited areas, while maintaining high
professional standards in psychological assessment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Query pipeline optimization for cancer patient question answering
  systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maolin He, Rena Gao, Mike Conway, Brian E. Chapman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) mitigates hallucination in Large
Language Models (LLMs) by using query pipelines to retrieve relevant external
information and grounding responses in retrieved knowledge. However, query
pipeline optimization for cancer patient question-answering (CPQA) systems
requires separately optimizing multiple components with domain-specific
considerations. We propose a novel three-aspect optimization approach for the
RAG query pipeline in CPQA systems, utilizing public biomedical databases like
PubMed and PubMed Central. Our optimization includes: (1) document retrieval,
utilizing a comparative analysis of NCBI resources and introducing Hybrid
Semantic Real-time Document Retrieval (HSRDR); (2) passage retrieval,
identifying optimal pairings of dense retrievers and rerankers; and (3)
semantic representation, introducing Semantic Enhanced Overlap Segmentation
(SEOS) for improved contextual understanding. On a custom-developed dataset
tailored for cancer-related inquiries, our optimized RAG approach improved the
answer accuracy of Claude-3-haiku by 5.24% over chain-of-thought prompting and
about 3% over a naive RAG setup. This study highlights the importance of
domain-specific query optimization in realizing the full potential of RAG and
provides a robust framework for building more accurate and reliable CPQA
systems, advancing the development of RAG-based biomedical systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Verbalized Confidence Scores for LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Yang, Yao-Hung Hubert Tsai, Makoto Yamada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of large language models (LLMs) and their tight integration into our
daily life make it essential to dedicate efforts towards their trustworthiness.
Uncertainty quantification for LLMs can establish more human trust into their
responses, but also allows LLM agents to make more informed decisions based on
each other's uncertainty. To estimate the uncertainty in a response, internal
token logits, task-specific proxy models, or sampling of multiple responses are
commonly used. This work focuses on asking the LLM itself to verbalize its
uncertainty with a confidence score as part of its output tokens, which is a
promising way for prompt- and model-agnostic uncertainty quantification with
low overhead. Using an extensive benchmark, we assess the reliability of
verbalized confidence scores with respect to different datasets, models, and
prompt methods. Our results reveal that the reliability of these scores
strongly depends on how the model is asked, but also that it is possible to
extract well-calibrated confidence scores with certain prompt methods. We argue
that verbalized confidence scores can become a simple but effective and
versatile uncertainty quantification method in the future. Our code is
available at https://github.com/danielyxyang/llm-verbalized-uq .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to Synthesize Text Data without Model Collapse? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuekai Zhu, Daixuan Cheng, Hengli Li, Kaiyan Zhang, Ermo Hua, Xingtai Lv, Ning Ding, Zhouhan Lin, Zilong Zheng, Bowen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model collapse in synthetic data indicates that iterative training on
self-generated data leads to a gradual decline in performance. With the
proliferation of AI models, synthetic data will fundamentally reshape the web
data ecosystem. Future GPT-$\{n\}$ models will inevitably be trained on a blend
of synthetic and human-produced data. In this paper, we focus on two questions:
what is the impact of synthetic data on language model training, and how to
synthesize data without model collapse? We first pre-train language models
across different proportions of synthetic data, revealing a negative
correlation between the proportion of synthetic data and model performance. We
further conduct statistical analysis on synthetic data to uncover
distributional shift phenomenon and over-concentration of n-gram features.
Inspired by the above findings, we propose token editing on human-produced data
to obtain semi-synthetic data. As a proof of concept, we theoretically
demonstrate that token-level editing can prevent model collapse, as the test
error is constrained by a finite upper bound. We conduct extensive experiments
on pre-training from scratch, continual pre-training, and supervised
fine-tuning. The results validate our theoretical proof that token-level
editing improves data quality and enhances model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Each Fake News is Fake in its Own Way: An Attribution Multi-Granularity
  Benchmark for Multimodal Fake News Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Guo, Zihan Ma, Zhi Zeng, Minnan Luo, Weixin Zeng, Jiuyang Tang, Xiang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social platforms, while facilitating access to information, have also become
saturated with a plethora of fake news, resulting in negative consequences.
Automatic multimodal fake news detection is a worthwhile pursuit. Existing
multimodal fake news datasets only provide binary labels of real or fake.
However, real news is alike, while each fake news is fake in its own way. These
datasets fail to reflect the mixed nature of various types of multimodal fake
news. To bridge the gap, we construct an attributing multi-granularity
multimodal fake news detection dataset \amg, revealing the inherent fake
pattern. Furthermore, we propose a multi-granularity clue alignment model \our
to achieve multimodal fake news detection and attribution. Experimental results
demonstrate that \amg is a challenging dataset, and its attribution setting
opens up new avenues for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLMs as mediators: Can they diagnose conflicts accurately? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Özgecan Koçak, Phanish Puranam, Afşar Yegin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior research indicates that to be able to mediate conflict, observers of
disagreements between parties must be able to reliably distinguish the sources
of their disagreement as stemming from differences in beliefs about what is
true (causality) vs. differences in what they value (morality). In this paper,
we test if OpenAI's Large Language Models GPT 3.5 and GPT 4 can perform this
task and whether one or other type of disagreement proves particularly
challenging for LLM's to diagnose. We replicate study 1 in Ko\c{c}ak et al.
(2003), which employes a vignette design, with OpenAI's GPT 3.5 and GPT 4. We
find that both LLMs have similar semantic understanding of the distinction
between causal and moral codes as humans and can reliably distinguish between
them. When asked to diagnose the source of disagreement in a conversation, both
LLMs, compared to humans, exhibit a tendency to overestimate the extent of
causal disagreement and underestimate the extent of moral disagreement in the
moral misalignment condition. This tendency is especially pronounced for GPT 4
when using a proximate scale that relies on concrete language specific to an
issue. GPT 3.5 does not perform as well as GPT4 or humans when using either the
proximate or the distal scale. The study provides a first test of the potential
for using LLMs to mediate conflict by diagnosing the root of disagreements in
causal and evaluative codes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 2 appendices, 21 tables (incl appendices)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysis and Visualization of Linguistic Structures in Large Language
  Models: Neural Representations of Verb-Particle Constructions in <span class="highlight-title">BERT</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hassane Kissane, Achim Schilling, Patrick Krauss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the internal representations of verb-particle
combinations within transformer-based large language models (LLMs),
specifically examining how these models capture lexical and syntactic nuances
at different neural network layers. Employing the BERT architecture, we analyse
the representational efficacy of its layers for various verb-particle
constructions such as 'agree on', 'come back', and 'give up'. Our methodology
includes a detailed dataset preparation from the British National Corpus,
followed by extensive model training and output analysis through techniques
like multi-dimensional scaling (MDS) and generalized discrimination value (GDV)
calculations. Results show that BERT's middle layers most effectively capture
syntactic structures, with significant variability in representational accuracy
across different verb categories. These findings challenge the conventional
uniformity assumed in neural network processing of linguistic elements and
suggest a complex interplay between network architecture and linguistic
representation. Our research contributes to a better understanding of how deep
learning models comprehend and process language, offering insights into the
potential and limitations of current neural approaches to linguistic analysis.
This study not only advances our knowledge in computational linguistics but
also prompts further research into optimizing neural architectures for enhanced
linguistic precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Uncertainty: A Deep Dive into Calibration and Performance of
  Multimodal Large Language Models <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijun Chen, Wenbo Hu, Guande He, Zhijie Deng, Zheng Zhang, Richang Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) combine visual and textual data for
tasks such as image captioning and visual question answering. Proper
uncertainty calibration is crucial, yet challenging, for reliable use in areas
like healthcare and autonomous driving. This paper investigates representative
MLLMs, focusing on their calibration across various scenarios, including before
and after visual fine-tuning, as well as before and after multimodal training
of the base LLMs. We observed miscalibration in their performance, and at the
same time, no significant differences in calibration across these scenarios. We
also highlight how uncertainty differs between text and images and how their
integration affects overall uncertainty. To better understand MLLMs'
miscalibration and their ability to self-assess uncertainty, we construct the
IDK (I don't know) dataset, which is key to evaluating how they handle
unknowns. Our findings reveal that MLLMs tend to give answers rather than admit
uncertainty, but this self-assessment improves with proper prompt adjustments.
Finally, to calibrate MLLMs and enhance model reliability, we propose
techniques such as temperature scaling and iterative prompt optimization. Our
results provide insights into improving MLLMs for effective and responsible
deployment in multimodal applications. Code and IDK dataset:
\href{https://github.com/hfutml/Calibration-MLLM}{https://github.com/hfutml/Calibration-MLLM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Length Controlled Generation for Black-box LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14656v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14656v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Gu, Wenjie Wang, Xiaocheng Feng, Weihong Zhong, Kun Zhu, Lei Huang, Tat-Seng Chua, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated impressive instruction
following capabilities, while still struggling to accurately manage the length
of the generated text, which is a fundamental requirement in many real-world
applications. Existing length control methods involve fine-tuning the
parameters of LLMs, which is inefficient and suboptimal for practical use. In
this paper, we propose a novel iterative sampling framework for text length
control, integrating the Metropolis-Hastings algorithm with an importance
sampling acceleration strategy. This framework efficiently and reliably
regulates LLMs to generate length-constrained text without modifying the
underlying parameters, thereby preserving the original capabilities of LLMs.
Experimental results demonstrate that our framework achieves almost 100\%
success rates of length control on Llama3.1 for tasks such as length-controlled
abstractive summarization and length-constrained instruction following, with
minimal additional computational overhead. This also highlights the significant
potential of our method for precise length control across a broader range of
applications, without compromising the versatility of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TOMG-Bench: Evaluating LLMs on Text-based Open Molecule Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiatong Li, Junxian Li, Yunqing Liu, Dongzhan Zhou, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose Text-based Open Molecule Generation Benchmark
(TOMG-Bench), the first benchmark to evaluate the open-domain molecule
generation capability of LLMs. TOMG-Bench encompasses a dataset of three major
tasks: molecule editing (MolEdit), molecule optimization (MolOpt), and
customized molecule generation (MolCustom). Each task further contains three
subtasks, with each subtask comprising 5,000 test samples. Given the inherent
complexity of open molecule generation, we have also developed an automated
evaluation system that helps measure both the quality and the accuracy of the
generated molecules. Our comprehensive benchmarking of 25 LLMs reveals the
current limitations and potential areas for improvement in text-guided molecule
discovery. Furthermore, with the assistance of OpenMolIns, a specialized
instruction tuning dataset proposed for solving challenges raised by
TOMG-Bench, Llama3.1-8B could outperform all the open-source general LLMs, even
surpassing GPT-3.5-turbo by 46.5\% on TOMG-Bench. Our codes and datasets are
available through https://github.com/phenixace/TOMG-Bench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A benchmark for text-based open molecule generation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Generate Research Idea with Dynamic Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruochen Li, Liqiang Jing, Chi Han, Jiawei Zhou, Xinya Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancements in large language models (LLMs) have demonstrated
their potential to accelerate scientific discovery, particularly in automating
the process of research ideation. LLM-based systems have shown promise in
generating hypotheses and research ideas. However, current approaches
predominantly rely on prompting-based pre-trained models, limiting their
ability to optimize generated content effectively. Moreover, they also lack the
capability to deal with the complex interdependence and inherent restrictions
among novelty, feasibility, and effectiveness, which remains challenging due to
the inherent trade-offs among these dimensions, such as the
innovation-feasibility conflict. To address these limitations, we for the first
time propose fine-tuning LLMs to be better idea proposers and introduce a novel
framework that employs a two-stage approach combining Supervised Fine-Tuning
(SFT) and controllable Reinforcement Learning (RL). In the SFT stage, the model
learns foundational patterns from pairs of research papers and follow-up ideas.
In the RL stage, multi-dimensional reward modeling, guided by fine-grained
feedback, evaluates and optimizes the generated ideas across key metrics.
Dimensional controllers enable dynamic adjustment of generation, while a
sentence-level decoder ensures context-aware emphasis during inference. Our
framework provides a balanced approach to research ideation, achieving
high-quality outcomes by dynamically navigating the trade-offs among novelty,
feasibility, and effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How good is <span class="highlight-title">GPT</span> at writing political speeches for the White House? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacques Savoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using large language models (LLMs), computers are able to generate a written
text in response to a us er request. As this pervasive technology can be
applied in numerous contexts, this study analyses the written style of one LLM
called GPT by comparing its generated speeches with those of the recent US
presidents. To achieve this objective, the State of the Union (SOTU) addresses
written by Reagan to Biden are contrasted to those produced by both GPT-3.5 and
GPT-4.o versions. Compared to US presidents, GPT tends to overuse the lemma
"we" and produce shorter messages with, on average, longer sentences. Moreover,
GPT opts for an optimistic tone, opting more often for political (e.g.,
president, Congress), symbolic (e.g., freedom), and abstract terms (e.g.,
freedom). Even when imposing an author's style to GPT, the resulting speech
remains distinct from addresses written by the target author. Finally, the two
GPT versions present distinct characteristics, but both appear overall
dissimilar to true presidential messages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HarmonicEval: Multi-modal, Multi-task, Multi-criteria Automatic
  Evaluation Using a Vision Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masanari Ohi, Masahiro Kaneko, Naoaki Okazaki, Nakamasa Inoue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) have shown impressive abilities in text and
image understanding. However, existing metrics for evaluating the text
generated by VLMs focus exclusively on overall quality, leading to two
limitations: 1) it is challenging to identify which aspects of the text need
improvement from the overall score; 2) metrics may overlook specific evaluation
criteria when predicting an overall score. To address these limitations, we
propose HarmonicEval, a reference-free evaluation metric that aggregates
criterion-wise scores to produce the overall score in a bottom-up manner.
Furthermore, we construct the Multi-task Multi-criteria Human Evaluation (MMHE)
dataset, which comprises 18,000 expert human judgments across four
vision-language tasks. Our experiments demonstrate that HarmonicEval achieves
higher correlations with human judgments than conventional metrics while
providing numerical scores for each criterion.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KARRIEREWEGE: A Large Scale Career Path Prediction <span class="highlight-title">Dataset</span> <span class="chip">COLING</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elena Senger, Yuri Campbell, Rob van der Goot, Barbara Plank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate career path prediction can support many stakeholders, like job
seekers, recruiters, HR, and project managers. However, publicly available data
and tools for career path prediction are scarce. In this work, we introduce
KARRIEREWEGE, a comprehensive, publicly available dataset containing over 500k
career paths, significantly surpassing the size of previously available
datasets. We link the dataset to the ESCO taxonomy to offer a valuable resource
for predicting career trajectories. To tackle the problem of free-text inputs
typically found in resumes, we enhance it by synthesizing job titles and
descriptions resulting in KARRIEREWEGE+. This allows for accurate predictions
from unstructured data, closely aligning with real-world application
challenges. We benchmark existing state-of-the-art (SOTA) models on our dataset
and a prior benchmark and observe improved performance and robustness,
particularly for free-text use cases, due to the synthesized data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at COLING Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LDP: Generalizing to Multilingual Visual Information Extraction by
  Language Decoupled <span class="highlight-title">Pretrain</span>ing <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huawen Shen, Gengluo Li, Jinwen Zhong, Yu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Information Extraction (VIE) plays a crucial role in the comprehension
of semi-structured documents, and several pre-trained models have been
developed to enhance performance. However, most of these works are monolingual
(usually English). Due to the extremely unbalanced quantity and quality of
pre-training corpora between English and other languages, few works can extend
to non-English scenarios. In this paper, we conduct systematic experiments to
show that vision and layout modality hold invariance among images with
different languages. If decoupling language bias from document images, a
vision-layout-based model can achieve impressive cross-lingual generalization.
Accordingly, we present a simple but effective multilingual training paradigm
LDP (Language Decoupled Pre-training) for better utilization of monolingual
pre-training data. Our proposed model LDM (Language Decoupled Model) is first
pre-trained on the language-independent data, where the language knowledge is
decoupled by a diffusion model, and then the LDM is fine-tuned on the
downstream languages. Extensive experiments show that the LDM outperformed all
SOTA multilingual pre-trained models, and also maintains competitiveness on
downstream monolingual/English benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Guilt: Legal Judgment Prediction with Trichotomous Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14588v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14588v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kepu Zhang, Haoyue Yang, Xu Tang, Weijie Yu, Jun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In legal practice, judges apply the trichotomous dogmatics of criminal law,
sequentially assessing the elements of the offense, unlawfulness, and
culpability to determine whether an individual's conduct constitutes a crime.
Although current legal large language models (LLMs) show promising accuracy in
judgment prediction, they lack trichotomous reasoning capabilities due to the
absence of an appropriate benchmark dataset, preventing them from predicting
innocent outcomes. As a result, every input is automatically assigned a charge,
limiting their practical utility in legal contexts. To bridge this gap, we
introduce LJPIV, the first benchmark dataset for Legal Judgment Prediction with
Innocent Verdicts. Adhering to the trichotomous dogmatics, we extend three
widely-used legal datasets through LLM-based augmentation and manual
verification. Our experiments with state-of-the-art legal LLMs and novel
strategies that integrate trichotomous reasoning into zero-shot prompting and
fine-tuning reveal: (1) current legal LLMs have significant room for
improvement, with even the best models achieving an F1 score of less than 0.3
on LJPIV; and (2) our strategies notably enhance both in-domain and
cross-domain judgment prediction accuracy, especially for cases resulting in an
innocent verdict.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simulation-Free Hierarchical Latent Policy Planning for Proactive
  Dialogues <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao He, Lizi Liao, Yixin Cao, Yuanxing Liu, Yiheng Sun, Zerui Chen, Ming Liu, Bing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in proactive dialogues have garnered significant
attention, particularly for more complex objectives (e.g. emotion support and
persuasion). Unlike traditional task-oriented dialogues, proactive dialogues
demand advanced policy planning and adaptability, requiring rich scenarios and
comprehensive policy repositories to develop such systems. However, existing
approaches tend to rely on Large Language Models (LLMs) for user simulation and
online learning, leading to biases that diverge from realistic scenarios and
result in suboptimal efficiency. Moreover, these methods depend on manually
defined, context-independent, coarse-grained policies, which not only incur
high expert costs but also raise concerns regarding their completeness. In our
work, we highlight the potential for automatically discovering policies
directly from raw, real-world dialogue records. To this end, we introduce a
novel dialogue policy planning framework, LDPP. It fully automates the process
from mining policies in dialogue records to learning policy planning.
Specifically, we employ a variant of the Variational Autoencoder to discover
fine-grained policies represented as latent vectors. After automatically
annotating the data with these latent policy labels, we propose an Offline
Hierarchical Reinforcement Learning (RL) algorithm in the latent space to
develop effective policy planning capabilities. Our experiments demonstrate
that LDPP outperforms existing methods on two proactive scenarios, even
surpassing ChatGPT with only a 1.8-billion-parameter LLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 5 fgiures, AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CORD: Balancing COnsistency and Rank Distillation for Robust
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngwon Lee, Seung-won Hwang, Daniel Campos, Filip Graliński, Zhewei Yao, Yuxiong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the adoption of retrieval-augmented generation (RAG), large language
models (LLMs) are expected to ground their generation to the retrieved
contexts. Yet, this is hindered by position bias of LLMs, failing to evenly
attend to all contexts. Previous work has addressed this by synthesizing
contexts with perturbed positions of gold segment, creating a
position-diversified train set. We extend this intuition to propose consistency
regularization with augmentation and distillation. First, we augment each
training instance with its position perturbation to encourage consistent
predictions, regardless of ordering. We also distill behaviors of this pair,
although it can be counterproductive in certain RAG scenarios where the given
order from the retriever is crucial for generation quality. We thus propose
CORD, balancing COnsistency and Rank Distillation. CORD adaptively samples
noise-controlled perturbations from an interpolation space, ensuring both
consistency and respect for the rank prior. Empirical results show this balance
enables CORD to outperform consistently in diverse RAG benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sliding Windows Are Not the End: Exploring Full Ranking with
  Long-Context Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhan Liu, Xinyu Ma, Yutao Zhu, Ziliang Zhao, Shuaiqiang Wang, Dawei Yin, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown exciting performance in listwise
passage ranking. Due to the limited input length, existing methods often adopt
the sliding window strategy. Such a strategy, though effective, is inefficient
as it involves repetitive and serialized processing, which usually re-evaluates
relevant passages multiple times. As a result, it incurs redundant API costs,
which are proportional to the number of inference tokens. The development of
long-context LLMs enables the full ranking of all passages within a single
inference, avoiding redundant API costs. In this paper, we conduct a
comprehensive study of long-context LLMs for ranking tasks in terms of
efficiency and effectiveness. Surprisingly, our experiments reveal that full
ranking with long-context LLMs can deliver superior performance in the
supervised fine-tuning setting with a huge efficiency improvement. Furthermore,
we identify two limitations of fine-tuning the full ranking model based on
existing methods: (1) sliding window strategy fails to produce a full ranking
list as a training label, and (2) the language modeling loss cannot emphasize
top-ranked passage IDs in the label. To alleviate these issues, we propose a
new complete listwise label construction approach and a novel importance-aware
learning objective for full ranking. Experiments show the superior performance
of our method over baselines. Our codes are available at
\url{https://github.com/8421BCD/fullrank}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CitaLaw: Enhancing LLM with Citations in Legal Domain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kepu Zhang, Weijie Yu, Sunhao Dai, Jun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose CitaLaw, the first benchmark designed to evaluate
LLMs' ability to produce legally sound responses with appropriate citations.
CitaLaw features a diverse set of legal questions for both laypersons and
practitioners, paired with a comprehensive corpus of law articles and precedent
cases as a reference pool. This framework enables LLM-based systems to retrieve
supporting citations from the reference corpus and align these citations with
the corresponding sentences in their responses. Moreover, we introduce
syllogism-inspired evaluation methods to assess the legal alignment between
retrieved references and LLM-generated responses, as well as their consistency
with user questions. Extensive experiments on 2 open-domain and 7
legal-specific LLMs demonstrate that integrating legal references substantially
enhances response quality. Furthermore, our proposed syllogism-based evaluation
method exhibits strong agreement with human judgments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ClusterTalk: Corpus Exploration Framework using Multi-Dimensional
  Exploratory Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14533v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14533v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashish Chouhan, Saifeldin Mandour, Michael Gertz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploratory search of large text corpora is essential in domains like
biomedical research, where large amounts of research literature are
continuously generated. This paper presents ClusterTalk (The demo video and
source code are available at: https://github.com/achouhan93/ClusterTalk), a
framework for corpus exploration using multi-dimensional exploratory search.
Our system integrates document clustering with faceted search, allowing users
to interactively refine their exploration and ask corpus and document-level
queries. Compared to traditional one-dimensional search approaches like keyword
search or clustering, this system improves the discoverability of information
by encouraging a deeper interaction with the corpus. We demonstrate the
functionality of the ClusterTalk framework based on four million PubMed
abstracts for the four-year time frame.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Level Optimal Transport for Universal Cross-Tokenizer Knowledge
  Distillation on Language Models <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Cui, Mo Zhu, Yulei Qin, Liang Xie, Wengang Zhou, Houqiang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation (KD) has become a prevalent technique for compressing
large language models (LLMs). Existing KD methods are constrained by the need
for identical tokenizers (i.e., vocabularies) between teacher and student
models, limiting their versatility in handling LLMs of different architecture
families. In this paper, we introduce the Multi-Level Optimal Transport
(MultiLevelOT), a novel approach that advances the optimal transport for
universal cross-tokenizer knowledge distillation. Our method aligns the logit
distributions of the teacher and the student at both token and sequence levels
using diverse cost matrices, eliminating the need for dimensional or
token-by-token correspondence. At the token level, MultiLevelOT integrates both
global and local information by jointly optimizing all tokens within a sequence
to enhance robustness. At the sequence level, we efficiently capture complex
distribution structures of logits via the Sinkhorn distance, which approximates
the Wasserstein distance for divergence measures. Extensive experiments on
tasks such as extractive QA, generative QA, and summarization demonstrate that
the MultiLevelOT outperforms state-of-the-art cross-tokenizer KD methods under
various settings. Our approach is robust to different student and teacher
models across model families, architectures, and parameter sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cal-DPO: Calibrated Direct Preference Optimization for Language Model
  Alignment <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teng Xiao, Yige Yuan, Huaisheng Zhu, Mingxiao Li, Vasant G Honavar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of aligning large language models (LLMs) with human
preference data. Contrastive preference optimization has shown promising
results in aligning LLMs with available preference data by optimizing the
implicit reward associated with the policy. However, the contrastive objective
focuses mainly on the relative values of implicit rewards associated with two
responses while ignoring their actual values, resulting in suboptimal alignment
with human preferences. To address this limitation, we propose calibrated
direct preference optimization (Cal-DPO), a simple yet effective algorithm. We
show that substantial improvement in alignment with the given preferences can
be achieved simply by calibrating the implicit reward to ensure that the
learned implicit rewards are comparable in scale to the ground-truth rewards.
We demonstrate the theoretical advantages of Cal-DPO over existing approaches.
The results of our experiments on a variety of standard benchmarks show that
Cal-DPO remarkably improves off-the-shelf methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PA-RAG: RAG Alignment via Multi-Perspective Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14510v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14510v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Wu, Hengyi Cai, Lingyong Yan, Hao Sun, Xiang Li, Shuaiqiang Wang, Dawei Yin, Ming Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of Retrieval-augmented generation (RAG) has alleviated the
issues of outdated and hallucinatory content in the generation of large
language models (LLMs), yet it still reveals numerous limitations. When a
general-purpose LLM serves as the RAG generator, it often suffers from
inadequate response informativeness, response robustness, and citation quality.
Past approaches to tackle these limitations, either by incorporating additional
steps beyond generating responses or optimizing the generator through
supervised fine-tuning (SFT), still failed to align with the RAG requirement
thoroughly. Consequently, optimizing the RAG generator from multiple preference
perspectives while maintaining its end-to-end LLM form remains a challenge. To
bridge this gap, we propose Multiple Perspective Preference Alignment for
Retrieval-Augmented Generation (PA-RAG), a method for optimizing the generator
of RAG systems to align with RAG requirements comprehensively. Specifically, we
construct high-quality instruction fine-tuning data and multi-perspective
preference data by sampling varied quality responses from the generator across
different prompt documents quality scenarios. Subsequently, we optimize the
generator using SFT and Direct Preference Optimization (DPO). Extensive
experiments conducted on four question-answer datasets across three LLMs
demonstrate that PA-RAG can significantly enhance the performance of RAG
generators. Our code and datasets are available at
https://github.com/wujwyi/PA-RAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Large Language Models Defend Inferentialist Semantics?: On the
  Logical Expressivism and Anti-Representationalism of LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14501v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14501v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzuki Arai, Sho Tsugawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The philosophy of language, which has historically been developed through an
anthropocentric lens, is now being forced to move towards post-anthropocentrism
due to the advent of large language models (LLMs) like ChatGPT (OpenAI), Claude
(Anthropic), which are considered to possess linguistic abilities comparable to
those of humans. Traditionally, LLMs have been explained through distributional
semantics as their foundational semantics. However, recent research is
exploring alternative foundational semantics beyond distributional semantics.
This paper proposes Robert Brandom's inferentialist semantics as an suitable
foundational semantics for LLMs, specifically focusing on the issue of
linguistic representationalism within this post-anthropocentric trend. Here, we
show that the anti-representationalism and logical expressivism of inferential
semantics, as well as quasi-compositionality, are useful in interpreting the
characteristics and behaviors of LLMs. Further, we propose a \emph{consensus
theory of truths} for LLMs. This paper argues that the characteristics of LLMs
challenge mainstream assumptions in philosophy of language, such as semantic
externalism and compositionality. We believe the argument in this paper leads
to a re-evaluation of anti\hyphen{}representationalist views of language,
potentially leading to new developments in the philosophy of language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GraphEQA: Using 3D Semantic Scene Graphs for Real-time Embodied Question
  Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saumya Saxena, Blake Buchanan, Chris Paxton, Bingqing Chen, Narunas Vaskevicius, Luigi Palmieri, Jonathan Francis, Oliver Kroemer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Embodied Question Answering (EQA), agents must explore and develop a
semantic understanding of an unseen environment in order to answer a situated
question with confidence. This remains a challenging problem in robotics, due
to the difficulties in obtaining useful semantic representations, updating
these representations online, and leveraging prior world knowledge for
efficient exploration and planning. Aiming to address these limitations, we
propose GraphEQA, a novel approach that utilizes real-time 3D metric-semantic
scene graphs (3DSGs) and task relevant images as multi-modal memory for
grounding Vision-Language Models (VLMs) to perform EQA tasks in unseen
environments. We employ a hierarchical planning approach that exploits the
hierarchical nature of 3DSGs for structured planning and semantic-guided
exploration. Through experiments in simulation on the HM-EQA dataset and in the
real world in home and office environments, we demonstrate that our method
outperforms key baselines by completing EQA tasks with higher success rates and
fewer planning steps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://saumyasaxena.github.io/grapheqa</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MegaPairs: Massive Data Synthesis For Universal Multimodal Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Zhou, Zheng Liu, Ze Liu, Shitao Xiao, Yueze Wang, Bo Zhao, Chen Jason Zhang, Defu Lian, Yongping Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the rapidly growing demand for multimodal retrieval, progress in this
field remains severely constrained by a lack of training data. In this paper,
we introduce MegaPairs, a novel data synthesis method that leverages vision
language models (VLMs) and open-domain images, together with a massive
synthetic dataset generated from this method. Our empirical analysis shows that
MegaPairs generates high-quality data, enabling the multimodal retriever to
significantly outperform the baseline model trained on 70$\times$ more data
from existing datasets. Moreover, since MegaPairs solely relies on general
image corpora and open-source VLMs, it can be easily scaled up, enabling
continuous improvements in retrieval performance. In this stage, we produced
more than 26 million training instances and trained several models of varying
sizes using this data. These new models achieve state-of-the-art zero-shot
performance across 4 popular composed image retrieval (CIR) benchmarks and the
highest overall performance on the 36 datasets provided by MMEB. They also
demonstrate notable performance improvements with additional downstream
fine-tuning. Our produced dataset, well-trained models, and data synthesis
pipeline will be made publicly available to facilitate the future development
of this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why We Build Local Large Language Models: An Observational Analysis from
  35 Japanese and Multilingual LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Koshiro Saito, Sakae Mizuki, Masanari Ohi, Taishi Nakamura, Taihei Shiotani, Koki Maeda, Youmi Ma, Kakeru Hattori, Kazuki Fujii, Takumi Okamoto, Shigeki Ishida, Hiroya Takamura, Rio Yokota, Naoaki Okazaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Why do we build local large language models (LLMs)? What should a local LLM
learn from the target language? Which abilities can be transferred from other
languages? Do language-specific scaling laws exist? To explore these research
questions, we evaluated 35 Japanese, English, and multilingual LLMs on 19
evaluation benchmarks for Japanese and English, taking Japanese as a local
language. Adopting an observational approach, we analyzed correlations of
benchmark scores, and conducted principal component analysis (PCA) on the
scores to derive \textit{ability factors} of local LLMs. We found that training
on English text can improve the scores of academic subjects in Japanese
(JMMLU). In addition, it is unnecessary to specifically train on Japanese text
to enhance abilities for solving Japanese code generation, arithmetic
reasoning, commonsense, and reading comprehension tasks. In contrast, training
on Japanese text could improve question-answering tasks about Japanese
knowledge and English-Japanese translation, which indicates that abilities for
solving these two tasks can be regarded as \textit{Japanese abilities} for
LLMs. Furthermore, we confirmed that the Japanese abilities scale with the
computational budget for Japanese text.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agent-SafetyBench: Evaluating the Safety of LLM Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14470v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14470v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhexin Zhang, Shiyao Cui, Yida Lu, Jingzhuo Zhou, Junxiao Yang, Hongning Wang, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) are increasingly deployed as agents, their
integration into interactive environments and tool use introduce new safety
challenges beyond those associated with the models themselves. However, the
absence of comprehensive benchmarks for evaluating agent safety presents a
significant barrier to effective assessment and further improvement. In this
paper, we introduce Agent-SafetyBench, a comprehensive benchmark designed to
evaluate the safety of LLM agents. Agent-SafetyBench encompasses 349
interaction environments and 2,000 test cases, evaluating 8 categories of
safety risks and covering 10 common failure modes frequently encountered in
unsafe interactions. Our evaluation of 16 popular LLM agents reveals a
concerning result: none of the agents achieves a safety score above 60%. This
highlights significant safety challenges in LLM agents and underscores the
considerable need for improvement. Through quantitative analysis, we identify
critical failure modes and summarize two fundamental safety detects in current
LLM agents: lack of robustness and lack of risk awareness. Furthermore, our
findings suggest that reliance on defense prompts alone is insufficient to
address these safety issues, emphasizing the need for more advanced and robust
strategies. We release Agent-SafetyBench at
\url{https://github.com/thu-coai/Agent-SafetyBench} to facilitate further
research and innovation in agent safety evaluation and improvement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Human Annotation to LLMs: SILICON Annotation Workflow for
  Management Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14461v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14461v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Cheng, Raveesh Mayya, João Sedoc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unstructured text data annotation and analysis are fundamental to management
research, often relying on human annotators through crowdsourcing platforms.
While Large Language Models (LLMs) promise to provide a cost-effective and
efficient alternative to human annotation, there lacks a systematic workflow
that evaluate when LLMs are suitable or how to proceed with LLM-based text
annotation in a reproducible manner. This paper addresses this methodological
gap by introducing the ``SILICON" (\textbf{S}ystematic \textbf{I}nference with
\textbf{L}LMs for \textbf{I}nformation \textbf{C}lassificati\textbf{o}n and
\textbf{N}otation) workflow. The workflow integrates established principles of
human annotation with systematic prompt optimization and model selection,
addressing challenges such as developing robust annotation guidelines,
establishing high-quality human baselines, optimizing prompts, and ensuring
reproducibility across LLMs. We validate the SILICON workflow through seven
case studies covering common management research tasks, including business
proposal evaluation, dialog intent and breakdown analysis, review attribute
detection. Our findings highlight the importance of validating annotation
guideline agreement, the superiority of expert-developed human baselines over
crowdsourced ones, the iterative nature of prompt optimization, and the
necessity of testing multiple LLMs. Notably, we propose a regression-based
methodology to empirically compare LLM outputs across prompts and models. Our
workflow advances management research by establishing reproducible processes
for LLM-based annotation that maintain scientific rigor. We provide practical
guidance for researchers to effectively navigate the evolving landscape of
generative AI tools effectively while maintaining transparency and
reproducibility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Longer <span class="highlight-title">Prompt</span>s Always Better? <span class="highlight-title">Prompt</span> Selection in Large Language
  Models for Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genki Kusano, Kosuke Akimoto, Kunihiro Takeoka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In large language models (LLM)-based recommendation systems (LLM-RSs),
accurately predicting user preferences by leveraging the general knowledge of
LLMs is possible without requiring extensive training data. By converting
recommendation tasks into natural language inputs called prompts, LLM-RSs can
efficiently solve issues that have been difficult to address due to data
scarcity but are crucial in applications such as cold-start and cross-domain
problems. However, when applying this in practice, selecting the prompt that
matches tasks and data is essential. Although numerous prompts have been
proposed in LLM-RSs and representing the target user in prompts significantly
impacts recommendation accuracy, there are still no clear guidelines for
selecting specific prompts.
  In this paper, we categorize and analyze prompts from previous research to
establish practical prompt selection guidelines. Through 450 experiments with
90 prompts and five real-world datasets, we examined the relationship between
prompts and dataset characteristics in recommendation accuracy. We found that
no single prompt consistently outperforms others; thus, selecting prompts on
the basis of dataset characteristics is crucial. Here, we propose a prompt
selection method that achieves higher accuracy with minimal validation data.
Because increasing the number of prompts to explore raises costs, we also
introduce a cost-efficient strategy using high-performance and cost-efficient
LLMs, significantly reducing exploration costs while maintaining high
prediction accuracy. Our work offers valuable insights into the prompt
selection, advancing accurate and efficient LLM-RSs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ORBIT: Cost-Effective <span class="highlight-title">Dataset</span> Curation for Large Language Model Domain
  Adaptation with an Astronomy Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14436v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14436v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Modesitt, Ke Yang, Spencer Hulsey, Chengxiang Zhai, Volodymyr Kindratenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in language modeling demonstrate the need for high-quality
domain-specific training data, especially for tasks that require specialized
knowledge. General-purpose models, while versatile, often lack the depth needed
for expert-level tasks because of limited domain-specific information. Domain
adaptation training can enhance these models, but it demands substantial,
high-quality data. To address this, we propose ORBIT, a cost-efficient
methodology for curating massive, high-quality domain-specific datasets from
noisy web sources, tailored for training specialist large language models.
Using astronomy as a primary case study, we refined the 1.3T-token FineWeb-Edu
dataset into a high-quality, 10B-token subset focused on astronomy. Fine-tuning
\textsc{LLaMA-3-8B} on a 1B-token astronomy subset improved performance on the
MMLU astronomy benchmark from 69\% to 76\% and achieved top results on
AstroBench, an astronomy-specific benchmark. Moreover, our model (Orbit-LLaMA)
outperformed \textsc{LLaMA-3-8B-base}, with GPT-4o evaluations preferring it in
73\% of cases across 1000 astronomy-specific questions. Additionally, we
validated ORBIT's generalizability by applying it to law and medicine,
achieving a significant improvement of data quality compared to an unfiltered
baseline. We open-source the ORBIT methodology, including the curated datasets,
the codebase, and the resulting model at
\href{https://github.com/ModeEric/ORBIT-Llama}{https://github.com/ModeEric/ORBIT-Llama}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ All-in-One Tuning and Structural Pruning for Domain-Specific LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Lu, Zhepeng Wang, Ruexue Bao, Mengbing Wang, Fangyi Li, Yawen Wu, Weiwen Jiang, Jie Xu, Yanzhi Wang, Shangqian Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing pruning techniques for large language models (LLMs) targeting
domain-specific applications typically follow a two-stage process: pruning the
pretrained general-purpose LLMs and then fine-tuning the pruned LLMs on
specific domains. However, the pruning decisions, derived from the pretrained
weights, remain unchanged during fine-tuning, even if the weights have been
updated. Therefore, such a combination of the pruning decisions and the
finetuned weights may be suboptimal, leading to non-negligible performance
degradation. To address these limitations, we propose ATP: All-in-One Tuning
and Structural Pruning, a unified one-stage structural pruning and fine-tuning
approach that dynamically identifies the current optimal substructure
throughout the fine-tuning phase via a trainable pruning decision generator.
Moreover, given the limited available data for domain-specific applications,
Low-Rank Adaptation (LoRA) becomes a common technique to fine-tune the LLMs. In
ATP, we introduce LoRA-aware forward and sparsity regularization to ensure that
the substructures corresponding to the learned pruning decisions can be
directly removed after the ATP process. ATP outperforms the state-of-the-art
two-stage pruning methods on tasks in the legal and healthcare domains. More
specifically, ATP recovers up to 88% and 91% performance of the dense model
when pruning 40% parameters of LLaMA2-7B and LLaMA3-8B models, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CodeLutra: Boosting LLM Code Generation via Preference-Guided Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05199v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05199v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leitian Tao, Xiang Chen, Tong Yu, Tung Mai, Ryan Rossi, Yixuan Li, Saayan Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have revolutionized code generation but require
significant resources and often over-generalize, limiting their task-specific
efficiency. Fine-tuning smaller, open-source LLMs provides a cost-effective
alternative. However, standard supervised approaches rely only on correct
examples, missing valuable insights from failures. We introduce CodeLutra, a
framework that leverages both correct and incorrect code attempts. Instead of
using only correct solutions, CodeLutra applies iterative preference-based
refinement, comparing successful and failed outputs to better approximate
desired results. This approach narrows the performance gap with
state-of-the-art larger models without requiring massive datasets or auxiliary
models. For instance, on a challenging data science coding task, using only 500
samples improved Llama-3-8B's accuracy from 28.2% to 48.6%, approaching GPT-4's
level. By learning from both successes and mistakes, CodeLutra provides a
scalable and efficient path to high-quality code generation, making smaller
open-source models more competitive with leading closed-source alternatives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ URIEL+: Enhancing Linguistic Inclusion and Usability in a Typological
  and Multilingual Knowledge Base <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18472v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18472v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Khan, Mason Shipton, David Anugraha, Kaiyao Duan, Phuong H. Hoang, Eric Khiu, A. Seza Doğruöz, En-Shiun Annie Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  URIEL is a knowledge base offering geographical, phylogenetic, and
typological vector representations for 7970 languages. It includes distance
measures between these vectors for 4005 languages, which are accessible via the
lang2vec tool. Despite being frequently cited, URIEL is limited in terms of
linguistic inclusion and overall usability. To tackle these challenges, we
introduce URIEL+, an enhanced version of URIEL and lang2vec that addresses
these limitations. In addition to expanding typological feature coverage for
2898 languages, URIEL+ improves the user experience with robust, customizable
distance calculations to better suit the needs of users. These upgrades also
offer competitive performance on downstream tasks and provide distances that
better align with linguistic distance studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04619v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04619v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Qin, Naomi Saphra, David Alvarez-Melis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs), like other neural networks, often favor shortcut
heuristics based on surface-level patterns. Although LMs behave like n-gram
models early in training, they must eventually learn hierarchical syntactic
representations to correctly apply grammatical rules out-of-distribution (OOD).
In this work, we use case studies of English grammar to explore how complex,
diverse training data drives models to generalize OOD. We construct a framework
that unifies our understanding of random variation with training dynamics, rule
selection with memorization, and data diversity with complexity. We show that
these factors are nuanced, and that intermediate levels of diversity and
complexity lead to inconsistent behavior across random seeds and to unstable
training dynamics. Our findings emphasize the critical role of training data in
shaping generalization patterns and illuminate how competing model strategies
lead to inconsistent generalization outcomes across random seeds. Code is
available at https://github.com/sunnytqin/concept_comp.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Typhoon 2: A Family of Open Text and Multimodal Thai Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13702v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13702v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kunat Pipatanakul, Potsawee Manakul, Natapong Nitarach, Warit Sirichotedumrong, Surapon Nonesung, Teetouch Jaknamon, Parinthapat Pengpun, Pittawat Taveekitworachai, Adisai Na-Thalang, Sittipong Sripaisarnmongkol, Krisanapong Jirayoot, Kasima Tharnpipitchai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Typhoon 2, a series of text and multimodal large
language models optimized for the Thai language. The series includes models for
text, vision, and audio. Typhoon2-Text builds on state-of-the-art open models,
such as Llama 3 and Qwen2, and we perform continual pre-training on a mixture
of English and Thai data. We employ post-training techniques to enhance Thai
language performance while preserving the base models' original capabilities.
We release text models across a range of sizes, from 1 to 70 billion
parameters, available in both base and instruction-tuned variants. To guardrail
text generation, we release Typhoon2-Safety, a classifier enhanced for Thai
cultures and language. Typhoon2-Vision improves Thai document understanding
while retaining general visual capabilities, such as image captioning.
Typhoon2-Audio introduces an end-to-end speech-to-speech model architecture
capable of processing audio, speech, and text inputs and generating both text
and speech outputs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>technical report, 55 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with
  LLM Token Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14512v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14512v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duo Wang, Yuan Zuo, Fengzhi Li, Junjie Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot graph machine learning, especially with graph neural networks
(GNNs), has garnered significant interest due to the challenge of scarce
labeled data. While methods like self-supervised learning and graph prompt
learning have been extensively explored, they often rely on fine-tuning with
task-specific labels, limiting their effectiveness in zero-shot scenarios.
Inspired by the zero-shot capabilities of instruction-fine-tuned large language
models (LLMs), we introduce a novel framework named Token Embedding-Aligned
Graph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and
cross-task zero-shot learners for graph machine learning. Concretely, we
pretrain a GNN, aligning its representations with token embeddings of an LLM.
We then train a linear projector that transforms the GNN's representations into
a fixed number of graph token embeddings without tuning the LLM. A unified
instruction is designed for various graph tasks at different levels, such as
node classification (node-level) and link prediction (edge-level). These design
choices collectively enhance our method's effectiveness in zero-shot learning,
setting it apart from existing methods. Experiments show that our graph token
embeddings help the LLM predictor achieve state-of-the-art performance on
unseen datasets and tasks compared to other methods using LLMs as predictors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying Query-Relevant Neurons in Large Language Models for
  Long-Form Texts <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10868v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10868v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lihu Chen, Adam Dejl, Francesca Toni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) possess vast amounts of knowledge within their
parameters, prompting research into methods for locating and editing this
knowledge. Previous work has largely focused on locating entity-related (often
single-token) facts in smaller models. However, several key questions remain
unanswered: (1) How can we effectively locate query-relevant neurons in
decoder-only LLMs, such as Llama and Mistral? (2) How can we address the
challenge of long-form (or free-form) text generation? (3) Are there localized
knowledge regions in LLMs? In this study, we introduce Query-Relevant Neuron
Cluster Attribution (QRNCA), a novel architecture-agnostic framework capable of
identifying query-relevant neurons in LLMs. QRNCA allows for the examination of
long-form answers beyond triplet facts by employing the proxy task of
multi-choice question answering. To evaluate the effectiveness of our detected
neurons, we build two multi-choice QA datasets spanning diverse domains and
languages. Empirical evaluations demonstrate that our method outperforms
baseline methods significantly. Further, analysis of neuron distributions
reveals the presence of visible localized regions, particularly within
different domains. Finally, we show potential applications of our detected
neurons in knowledge editing and neuron-based prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025 Main Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPICA: Retrieving Scenarios for Pluralistic In-Context Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10912v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10912v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Ze Chen, K. J. Kevin Feng, Chan Young Park, Amy X. Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When different groups' values differ, one approach to model alignment is to
steer models at inference time towards each group's preferences. However,
techniques like in-context learning only consider similarity when drawing
few-shot examples and not cross-group differences in values. We propose SPICA,
a framework that accounts for group-level differences during in-context example
retrieval. SPICA introduces three designs: scenario banks, group-informed
retrieval metrics, and in-context alignment prompts. From an evaluation of
SPICA on an alignment task collecting inputs from four demographic groups ($n =
544$), our metrics retrieve in-context examples that more closely match
observed preferences, with the best prompt configuration using multiple
contrastive responses to demonstrate examples. In an end-to-end evaluation ($n
= 120$), we observe that SPICA is higher rated than similarity-based retrieval,
with groups seeing up to a +0.16 point improvement on a 5 point scale.
Additionally, gains from SPICA were more uniform, with all groups benefiting
from alignment rather than only some. Finally, we find that while a
group-agnostic approach can align to aggregated values, it is not most suited
for divergent groups.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Tagging with Large Language Model based Multi-Agent System <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.08406v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.08406v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Li, Tianlong Xu, Ethan Chang, Qingsong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge tagging for questions is vital in modern intelligent educational
applications, including learning progress diagnosis, practice question
recommendations, and course content organization. Traditionally, these
annotations have been performed by pedagogical experts, as the task demands not
only a deep semantic understanding of question stems and knowledge definitions
but also a strong ability to link problem-solving logic with relevant knowledge
concepts. With the advent of advanced natural language processing (NLP)
algorithms, such as pre-trained language models and large language models
(LLMs), pioneering studies have explored automating the knowledge tagging
process using various machine learning models. In this paper, we investigate
the use of a multi-agent system to address the limitations of previous
algorithms, particularly in handling complex cases involving intricate
knowledge definitions and strict numerical constraints. By demonstrating its
superior performance on the publicly available math question knowledge tagging
dataset, MathKnowCT, we highlight the significant potential of an LLM-based
multi-agent system in overcoming the challenges that previous methods have
encountered. Finally, through an in-depth discussion of the implications of
automating knowledge tagging, we underscore the promising results of deploying
LLM-based algorithms in educational contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025 (AAAI/IAAI 2025 Innovative Application Award)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond <span class="highlight-title">Dataset</span> Creation: Critical View of Annotation Variation and Bias
  Probing of a <span class="highlight-title">Dataset</span> for Online Radical Content Detection <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11745v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11745v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arij Riabi, Virginie Mouilleron, Menel Mahamdi, Wissam Antoun, Djamé Seddah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of radical content on online platforms poses significant
risks, including inciting violence and spreading extremist ideologies. Despite
ongoing research, existing datasets and models often fail to address the
complexities of multilingual and diverse data. To bridge this gap, we introduce
a publicly available multilingual dataset annotated with radicalization levels,
calls for action, and named entities in English, French, and Arabic. This
dataset is pseudonymized to protect individual privacy while preserving
contextual information. Beyond presenting our freely available dataset, we
analyze the annotation process, highlighting biases and disagreements among
annotators and their implications for model performance. Additionally, we use
synthetic data to investigate the influence of socio-demographic traits on
annotation patterns and model predictions. Our work offers a comprehensive
examination of the challenges and opportunities in building robust datasets for
radical content detection, emphasizing the importance of fairness and
transparency in model development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLM-SEM: A Sentiment-Based Student Engagement Metric Using LLMS for
  E-Learning Platforms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13765v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13765v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Hamdi, Ahmed Abdelmoneim Mazrou, Mohamed Shaltout
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current methods for analyzing student engagement in e-learning platforms,
including automated systems, often struggle with challenges such as handling
fuzzy sentiment in text comments and relying on limited metadata. Traditional
approaches, such as surveys and questionnaires, also face issues like small
sample sizes and scalability. In this paper, we introduce LLM-SEM (Language
Model-Based Student Engagement Metric), a novel approach that leverages video
metadata and sentiment analysis of student comments to measure engagement. By
utilizing recent Large Language Models (LLMs), we generate high-quality
sentiment predictions to mitigate text fuzziness and normalize key features
such as views and likes. Our holistic method combines comprehensive metadata
with sentiment polarity scores to gauge engagement at both the course and
lesson levels. Extensive experiments were conducted to evaluate various LLM
models, demonstrating the effectiveness of LLM-SEM in providing a scalable and
accurate measure of student engagement. We fine-tuned TXLM-RoBERTa using
human-annotated sentiment datasets to enhance prediction accuracy and utilized
LLama 3B, and Gemma 9B from Ollama.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ G-VEval: A Versatile Metric for Evaluating Image and Video Captions
  Using <span class="highlight-title">GPT</span>-4o 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13647v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13647v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tony Cheng Tong, Sirui He, Zhiwen Shao, Dit-Yan Yeung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluation metric of visual captioning is important yet not thoroughly
explored. Traditional metrics like BLEU, METEOR, CIDEr, and ROUGE often miss
semantic depth, while trained metrics such as CLIP-Score, PAC-S, and Polos are
limited in zero-shot scenarios. Advanced Language Model-based metrics also
struggle with aligning to nuanced human preferences. To address these issues,
we introduce G-VEval, a novel metric inspired by G-Eval and powered by the new
GPT-4o. G-VEval uses chain-of-thought reasoning in large multimodal models and
supports three modes: reference-free, reference-only, and combined,
accommodating both video and image inputs. We also propose MSVD-Eval, a new
dataset for video captioning evaluation, to establish a more transparent and
consistent framework for both human experts and evaluation metrics. It is
designed to address the lack of clear criteria in existing datasets by
introducing distinct dimensions of Accuracy, Completeness, Conciseness, and
Relevance (ACCR). Extensive results show that G-VEval outperforms existing
methods in correlation with human annotations, as measured by Kendall tau-b and
Kendall tau-c. This provides a flexible solution for diverse captioning tasks
and suggests a straightforward yet effective approach for large language models
to understand video content, paving the way for advancements in automated
captioning. Codes are available at https://github.com/ztangaj/gveval
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ To Word Senses and Beyond: Inducing Concepts with Contextualized
  Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.20054v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.20054v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bastien Liétard, Pascal Denis, Mikaella Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Polysemy and synonymy are two crucial interrelated facets of lexical
ambiguity. While both phenomena are widely documented in lexical resources and
have been studied extensively in NLP, leading to dedicated systems, they are
often being considered independently in practical problems. While many tasks
dealing with polysemy (e.g. Word Sense Disambiguiation or Induction) highlight
the role of word's senses, the study of synonymy is rooted in the study of
concepts, i.e. meanings shared across the lexicon. In this paper, we introduce
Concept Induction, the unsupervised task of learning a soft clustering among
words that defines a set of concepts directly from data. This task generalizes
Word Sense Induction. We propose a bi-level approach to Concept Induction that
leverages both a local lemma-centric view and a global cross-lexicon view to
induce concepts. We evaluate the obtained clustering on SemCor's annotated data
and obtain good performance (BCubed F1 above 0.60). We find that the local and
the global levels are mutually beneficial to induce concepts and also senses in
our setting. Finally, we create static embeddings representing our induced
concepts and use them on the Word-in-Context task, obtaining competitive
performance with the State-of-the-Art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in EMNLP 2024 main conference proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking Large Language Models for Math Reasoning Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10839v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10839v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kathrin Seßler, Yao Rong, Emek Gözlüklü, Enkelejda Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of Large Language Models (LLMs) in mathematical reasoning has become
a cornerstone of related research, demonstrating the intelligence of these
models and enabling potential practical applications through their advanced
performance, such as in educational settings. Despite the variety of datasets
and in-context learning algorithms designed to improve the ability of LLMs to
automate mathematical problem solving, the lack of comprehensive benchmarking
across different datasets makes it complicated to select an appropriate model
for specific tasks. In this project, we present a benchmark that fairly
compares seven state-of-the-art in-context learning algorithms for mathematical
problem solving across five widely used mathematical datasets on four powerful
foundation models. Furthermore, we explore the trade-off between efficiency and
performance, highlighting the practical applications of LLMs for mathematical
reasoning. Our results indicate that larger foundation models like GPT-4o and
LLaMA 3-70B can solve mathematical reasoning independently from the concrete
prompting strategy, while for smaller models the in-context learning approach
significantly influences the performance. Moreover, the optimal prompt depends
on the chosen foundation model. We open-source our benchmark code to support
the integration of additional models in future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ProsodyFM: Unsupervised Phrasing and Intonation Control for Intelligible
  Speech Synthesis <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangheng He, Junjie Chen, Zixing Zhang, Björn W. Schuller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prosody contains rich information beyond the literal meaning of words, which
is crucial for the intelligibility of speech. Current models still fall short
in phrasing and intonation; they not only miss or misplace breaks when
synthesizing long sentences with complex structures but also produce unnatural
intonation. We propose ProsodyFM, a prosody-aware text-to-speech synthesis
(TTS) model with a flow-matching (FM) backbone that aims to enhance the
phrasing and intonation aspects of prosody. ProsodyFM introduces two key
components: a Phrase Break Encoder to capture initial phrase break locations,
followed by a Duration Predictor for the flexible adjustment of break
durations; and a Terminal Intonation Encoder which learns a bank of intonation
shape tokens combined with a novel Pitch Processor for more robust modeling of
human-perceived intonation change. ProsodyFM is trained with no explicit
prosodic labels and yet can uncover a broad spectrum of break durations and
intonation patterns. Experimental results demonstrate that ProsodyFM can
effectively improve the phrasing and intonation aspects of prosody, thereby
enhancing the overall intelligibility compared to four state-of-the-art (SOTA)
models. Out-of-distribution experiments show that this prosody improvement can
further bring ProsodyFM superior generalizability for unseen complex sentences
and speakers. Our case study intuitively illustrates the powerful and
fine-grained controllability of ProsodyFM over phrasing and intonation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic
  Analysis of Annotators and Targets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07991v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07991v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tommaso Giorgi, Lorenzo Cima, Tiziano Fagni, Marco Avvenuti, Stefano Cresci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of online platforms exacerbated the spread of hate speech, demanding
scalable and effective detection. However, the accuracy of hate speech
detection systems heavily relies on human-labeled data, which is inherently
susceptible to biases. While previous work has examined the issue, the
interplay between the characteristics of the annotator and those of the target
of the hate are still unexplored. We fill this gap by leveraging an extensive
dataset with rich socio-demographic information of both annotators and targets,
uncovering how human biases manifest in relation to the target's attributes.
Our analysis surfaces the presence of widespread biases, which we
quantitatively describe and characterize based on their intensity and
prevalence, revealing marked differences. Furthermore, we compare human biases
with those exhibited by persona-based LLMs. Our findings indicate that while
persona-based LLMs do exhibit biases, these differ significantly from those of
human annotators. Overall, our work offers new and nuanced results on human
biases in hate speech annotations, as well as fresh insights into the design of
AI-driven hate speech detection systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Bench to Bedside: A <span class="highlight-title">Review</span> of Clinical Trials in Drug Discovery and
  Development 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09378v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09378v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyang Wang, Ming Liu, Benji Peng, Xinyuan Song, Charles Zhang, Xintian Sun, Qian Niu, Junyu Liu, Silin Chen, Keyu Chen, Ming Li, Pohsun Feng, Ziqian Bi, Yunze Wang, Yichao Zhang, Cheng Fei, Lawrence KQ Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical trials are an indispensable part of the drug development process,
bridging the gap between basic research and clinical application. During the
development of new drugs, clinical trials are used not only to evaluate the
safety and efficacy of the drug but also to explore its dosage, treatment
regimens, and potential side effects. This review discusses the various stages
of clinical trials, including Phase I (safety assessment), Phase II
(preliminary efficacy evaluation), Phase III (large-scale validation), and
Phase IV (post-marketing surveillance), highlighting the characteristics of
each phase and their interrelationships. Additionally, the paper addresses the
major challenges encountered in clinical trials, such as ethical issues,
subject recruitment difficulties, diversity and representativeness concerns,
and proposes strategies for overcoming these challenges. With the advancement
of technology, innovative technologies such as artificial intelligence, big
data, and digitalization are gradually transforming clinical trial design and
implementation, improving trial efficiency and data quality. The article also
looks forward to the future of clinical trials, particularly the impact of
emerging therapies such as gene therapy and immunotherapy on trial design, as
well as the importance of regulatory reforms and global collaboration. In
conclusion, the core role of clinical trials in drug development will continue
to drive the progress of innovative drug development and clinical treatment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ANAH-v2: Scaling Analytical Hallucination Annotation of Large Language
  Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04693v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04693v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhe Gu, Ziwei Ji, Wenwei Zhang, Chengqi Lyu, Dahua Lin, Kai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit hallucinations in long-form
question-answering tasks across various domains and wide applications. Current
hallucination detection and mitigation datasets are limited in domains and
sizes, which struggle to scale due to prohibitive labor costs and insufficient
reliability of existing hallucination annotators. To facilitate the scalable
oversight of LLM hallucinations, this paper introduces an iterative
self-training framework that simultaneously and progressively scales up the
hallucination annotation dataset and improves the accuracy of the hallucination
annotator. Based on the Expectation Maximization (EM) algorithm, in each
iteration, the framework first applies a hallucination annotation pipeline to
annotate a scaled dataset and then trains a more accurate hallucination
annotator on the dataset. This new hallucination annotator is adopted in the
hallucination annotation pipeline used for the next iteration. Extensive
experimental results demonstrate that the finally obtained hallucination
annotator with only 7B parameters surpasses the performance of GPT-4 and
obtains new state-of-the-art hallucination detection results on HaluEval and
HalluQA by zero-shot inference. Such an annotator can not only evaluate the
hallucination levels of various LLMs on the large-scale dataset but also help
to mitigate the hallucination of LLMs generations, with the Natural Language
Inference (NLI) metric increasing from 25% to 37% on HaluEval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024. Dataset, code, and model are released at
  https://github.com/open-compass/ANAH</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BayLing 2: A Multilingual Large Language Model with Efficient Language
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16300v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16300v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaolei Zhang, Kehao Zhang, Qingkai Fang, Shoutao Guo, Yan Zhou, Xiaodong Liu, Yang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), with their powerful generative capabilities and
vast knowledge, empower various tasks in everyday life. However, these
abilities are primarily concentrated in high-resource languages, leaving
low-resource languages with weaker generative capabilities and relatively
limited knowledge. Enhancing the multilingual capabilities of LLMs is therefore
crucial for serving over 100 linguistic communities worldwide. An intuitive
approach to enhance the multilingual capabilities would be to construct
instruction data for various languages, but constructing instruction data for
over 100 languages is prohibitively costly. In this paper, we introduce BayLing
2, which efficiently transfers generative capabilities and knowledge from
high-resource languages to low-resource languages through language alignment.
To achieve this, we constructed a dataset of 3.2 million instructions,
comprising high-resource language instructions (Chinese and English) and
cross-lingual instructions for 100+ languages and performed instruction tuning
based on the dataset to facilitate the capability transfer between languages.
Using Llama as the foundation model, we developed BayLing-2-7B, BayLing-2-13B,
and BayLing-2-8B, and conducted a comprehensive evaluation of BayLing. For
multilingual translation across 100+ languages, BayLing shows superior
performance compared to open-source models of similar scale. For multilingual
knowledge and understanding benchmarks, BayLing achieves significant
improvements across over 20 low-resource languages, demonstrating its
capability of effective knowledge transfer from high-resource to low-resource
languages. Furthermore, results on English benchmarks indicate that BayLing
maintains high performance in highresource languages while enhancing the
performance in low-resource languages. Demo, homepage, code and models of
BayLing are available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>BayLing 2's online demo: http://nlp.ict.ac.cn/bayling/demo. BayLing
  2's code and models: https://github.com/ictnlp/BayLing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agent-OM: Leveraging LLM Agents for Ontology Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00326v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00326v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangcheng Qiang, Weiqing Wang, Kerry Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ontology matching (OM) enables semantic interoperability between different
ontologies and resolves their conceptual heterogeneity by aligning related
entities. OM systems currently have two prevailing design paradigms:
conventional knowledge-based expert systems and newer machine learning-based
predictive systems. While large language models (LLMs) and LLM agents have
revolutionised data engineering and have been applied creatively in many
domains, their potential for OM remains underexplored. This study introduces a
novel agent-powered LLM-based design paradigm for OM systems. With
consideration of several specific challenges in leveraging LLM agents for OM,
we propose a generic framework, namely Agent-OM (Agent for Ontology Matching),
consisting of two Siamese agents for retrieval and matching, with a set of
simple OM tools. Our framework is implemented in a proof-of-concept system.
Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks
over state-of-the-art OM systems show that our system can achieve results very
close to the long-standing best performance on simple OM tasks and can
significantly improve the performance on complex and few-shot OM tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 13 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An $\mathbf{L^*}$ Algorithm for Deterministic Weighted Regular Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06228v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06228v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clemente Pasti, Talu Karagöz, Anej Svete, Franz Nowak, Reda Boumasmoud, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting finite state automata (FSAs) from black-box models offers a
powerful approach to gaining interpretable insights into complex model
behaviors. To support this pursuit, we present a weighted variant of Angluin's
(1987) $\mathbf{L^*}$ algorithm for learning FSAs. We stay faithful to the
original algorithm, devising a way to exactly learn deterministic weighted FSAs
whose weights support division. Furthermore, we formulate the learning process
in a manner that highlights the connection with FSA minimization, showing how
$\mathbf{L^*}$ directly learns a minimal automaton for the target language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MaFeRw: Query Rewriting with Multi-Aspect Feedbacks for
  Retrieval-Augmented Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.17072v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.17072v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujing Wang, Hainan Zhang, Liang Pang, Binghui Guo, Hongwei Zheng, Zhiming Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a real-world RAG system, the current query often involves spoken ellipses
and ambiguous references from dialogue contexts, necessitating query rewriting
to better describe user's information needs. However, traditional context-based
rewriting has minimal enhancement on downstream generation tasks due to the
lengthy process from query rewriting to response generation. Some researchers
try to utilize reinforcement learning with generation feedback to assist the
rewriter, but these sparse rewards provide little guidance in most cases,
leading to unstable training and generation results. We find that user's needs
are also reflected in the gold document, retrieved documents and ground truth.
Therefore, by feeding back these multi-aspect dense rewards to query rewriting,
more stable and satisfactory responses can be achieved. In this paper, we
propose a novel query rewriting method MaFeRw, which improves RAG performance
by integrating multi-aspect feedback from both the retrieval process and
generated results. Specifically, we first use manual data to train a T5 model
for the rewriter initialization. Next, we design three metrics as reinforcement
learning feedback: the similarity between the rewritten query and the gold
document, the ranking metrics, and ROUGE between the generation and the ground
truth. Inspired by RLAIF, we train three kinds of reward models for the above
metrics to achieve more efficient training. Finally, we combine the scores of
these reward models as feedback, and use PPO algorithm to explore the optimal
query rewriting strategy. Experimental results on two conversational RAG
datasets demonstrate that MaFeRw achieves superior generation metrics and more
stable training compared to baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs instead of Human Judges? A Large Scale Empirical Study across 20
  NLP Evaluation Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18403v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18403v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anna Bavaresco, Raffaella Bernardi, Leonardo Bertolazzi, Desmond Elliott, Raquel Fernández, Albert Gatt, Esam Ghaleb, Mario Giulianelli, Michael Hanna, Alexander Koller, André F. T. Martins, Philipp Mondorf, Vera Neplenbroek, Sandro Pezzelle, Barbara Plank, David Schlangen, Alessandro Suglia, Aditya K Surikuchi, Ece Takmaz, Alberto Testoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is an increasing trend towards evaluating NLP models with LLMs instead
of human judgments, raising questions about the validity of these evaluations,
as well as their reproducibility in the case of proprietary models. We provide
JUDGE-BENCH, an extensible collection of 20 NLP datasets with human annotations
covering a broad range of evaluated properties and types of data, and
comprehensively evaluate 11 current LLMs, covering both open-weight and
proprietary models, for their ability to replicate the annotations. Our
evaluations show substantial variance across models and datasets. Models are
reliable evaluators on some tasks, but overall display substantial variability
depending on the property being evaluated, the expertise level of the human
judges, and whether the language is human or model-generated. We conclude that
LLMs should be carefully validated against human judgments before being used as
evaluators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TrimLLM: Progressive Layer Dropping for Domain-Specific LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11242v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11242v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lanxiang Hu, Tajana Rosing, Hao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Specializing large language models (LLMs) for local deployment in
domain-specific use cases is necessary for strong performance while meeting
latency and privacy constraints. However, conventional task-specific adaptation
approaches do not show simultaneous memory saving and inference speedup at
deployment time. Practical compression techniques like quantization and pruning
require dedicated hardware or kernel support to achieve measured inference
speedup. We develop TrimLLM based on the layer-wise specialization phenomenon
we empirically observed and verified on contemporary LLMs. TrimLLM reduces the
depth of LLMs via progressive layer dropping. We show it retains LLMs' capacity
in specific domains and achieves inference speedup irrespective of hardware and
deep learning frameworks. We evaluated TrimLLM on LLMs of various sizes for
inference; models adapted on medical, legal, and financial datasets all
demonstrate $2.1-5.7\times$ inference speedup on consumer GPUs and up to
$3.1\times$ speedup on A100 when compared to state-of-the-art model compression
algorithms, with no loss in accuracy at 50$\sim$60\% model compression ratio.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text
  Rewriting <span class="chip">AAAI'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07675v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07675v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Yang, Bardh Prenkaj, Gjergji Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the widespread use of LLMs due to their superior performance in
various tasks, their high computational costs often lead potential users to opt
for the pretraining-finetuning pipeline. However, biases prevalent in manually
constructed datasets can introduce spurious correlations between tokens and
labels, creating so-called shortcuts and hindering the generalizability of
fine-tuned models. Existing debiasing methods often rely on prior knowledge of
specific dataset biases, which is challenging to acquire a priori. We propose
RAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised,
and data-focused debiasing approach based on text rewriting for shortcut
mitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text
segments by replacing them with heuristically selected alternatives in a
shortcut space defined by token statistics and positional information. This
process aims to align surface-level text features more closely with diverse
label distributions, thereby promoting the learning of genuine linguistic
patterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the
FEVER and 6.5% on MNLI and SNLI datasets according to the F1 score.
Additionally, RAZOR effectively mitigates specific known biases, reducing
bias-related terms by x2 without requiring prior bias information, a result
that is on par with SoTA models that leverage prior information. Our work
prioritizes data manipulation over architectural modifications, emphasizing the
pivotal role of data quality in enhancing model performance and fairness. This
research contributes to developing more robust evaluation benchmarks for
debiasing methods by incorporating metrics for bias reduction and overall model
efficacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Shuo and Bardh contributed equally. Accepted to AAAI'25, Paper #17117</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Every Token Counts: Optimal Segmentation for Low-Resource Language
  Models <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06926v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06926v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bharath Raj S, Garvit Suri, Vikrant Dewangan, Raghav Sonavane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional greedy tokenization methods have been a critical step in Natural
Language Processing (NLP), influencing how text is converted into tokens and
directly impacting model performance. While subword tokenizers like Byte-Pair
Encoding (BPE) are widely used, questions remain about their optimality across
model scales and languages. In this work, we demonstrate through extensive
experiments that an optimal BPE configuration significantly reduces token count
compared to greedy segmentation, yielding improvements in token-saving
percentages and performance benefits, particularly for smaller models. We
evaluate tokenization performance across various intrinsic and extrinsic tasks,
including generation and classification. Our findings suggest that
compression-optimized tokenization strategies could provide substantial
advantages for multilingual and low-resource language applications,
highlighting a promising direction for further research and inclusive NLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LoResLM @ COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep CLAS: Deep Contextual Listen, Attend and Spell 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.17603v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.17603v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengzhi Wang, Shifu Xiong, Genshun Wan, Hang Chen, Jianqing Gao, Lirong Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contextual-LAS (CLAS) has been shown effective in improving Automatic Speech
Recognition (ASR) of rare words. It relies on phrase-level contextual modeling
and attention-based relevance scoring without explicit contextual constraint
which lead to insufficient use of contextual information. In this work, we
propose deep CLAS to use contextual information better. We introduce bias loss
forcing model to focus on contextual information. The query of bias attention
is also enriched to improve the accuracy of the bias attention score. To get
fine-grained contextual information, we replace phrase-level encoding with
character-level encoding and encode contextual information with conformer
rather than LSTM. Moreover, we directly use the bias attention score to correct
the output probability distribution of the model. Experiments using the public
AISHELL-1 and AISHELL-NER. On AISHELL-1, compared to CLAS baselines, deep CLAS
obtains a 65.78% relative recall and a 53.49% relative F1-score increase in the
named entity recognition scene.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to JUSTC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards an optimised evaluation of teachers' discourse: The case of
  engaging messages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14011v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14011v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Falcon, Jaime Leon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating teachers' skills is crucial for enhancing education quality and
student outcomes. Teacher discourse, significantly influencing student
performance, is a key component. However, coding this discourse can be
laborious. This study addresses this issue by introducing a new methodology for
optimising the assessment of teacher discourse. The research consisted of two
studies, both within the framework of engaging messages used by secondary
education teachers. The first study involved training two large language models
on real-world examples from audio-recorded lessons over two academic years to
identify and classify the engaging messages from the lessons' transcripts. This
resulted in sensitivities of 84.31% and 91.11%, and specificities of 97.69% and
86.36% in identification and classification, respectively. The second study
applied these models to transcripts of audio-recorded lessons from a third
academic year to examine the frequency and distribution of message types by
educational level and moment of the academic year. Results showed teachers
predominantly use messages emphasising engagement benefits, linked to improved
outcomes, while one-third highlighted non-engagement disadvantages, associated
with increased anxiety. The use of engaging messages declined in Grade 12 and
towards the academic year's end. These findings suggest potential interventions
to optimise engaging message use, enhancing teaching quality and student
outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low-resource Machine Translation: what for? who for? An observational
  study on a dedicated Tetun language translation service 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphael Merx, Adérito José Guterres Correia, Hanna Suominen, Ekaterina Vylomova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-resource machine translation (MT) presents a diversity of community needs
and application challenges that remain poorly understood. To complement surveys
and focus groups, which tend to rely on small samples of respondents, we
propose an observational study on actual usage patterns of a specialized MT
service for the Tetun language, which is the lingua franca in Timor-Leste. Our
analysis of 100,000 translation requests reveals patterns that challenge
assumptions based on existing corpora. We find that users, many of them
students on mobile devices, typically translate text from a high-resource
language into Tetun across diverse domains including science, healthcare, and
daily life. This contrasts sharply with available Tetun corpora, which are
dominated by news articles covering government and social issues. Our results
suggest that MT systems for minority languages like Tetun should prioritize
accuracy on domains relevant to educational contexts, in the high-resource to
low-resource direction. More broadly, this study demonstrates how observational
analysis can inform low-resource language technology development, by grounding
research in practical community needs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Piece of Table: A Divide-and-Conquer Approach for Selecting Sub-Tables
  in Table Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07629v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07629v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonjin Lee, Kyumin Kim, Sungjae Lee, Jihun Lee, Kwang In Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Applying language models (LMs) to tables is challenging due to the inherent
structural differences between two-dimensional tables and one-dimensional text
for which the LMs were originally designed. Furthermore, when applying
linearized tables to LMs, the maximum token lengths often imposed in
self-attention calculations make it difficult to comprehensively understand the
context spread across large tables. To address these challenges, we present
PieTa (Piece of Table), a new framework for sub-table-based question answering
(QA). PieTa operates through an iterative process of dividing tables into
smaller windows, using LMs to select relevant cells within each window, and
merging these cells into a sub-table. This multi-resolution approach captures
dependencies across multiple rows and columns while avoiding the limitations
caused by long context inputs. Instantiated as a simple iterative sub-table
union algorithm, PieTa demonstrates improved performance over previous
sub-table-based QA approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Alignment-Enhanced Decoding:Defending via Token-Level Adaptive Refining
  of Probability Distributions <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07663v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07663v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Liu, Zhenhong Zhou, Longzhu He, Yi Liu, Wei Zhang, Sen Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are susceptible to jailbreak attacks, which can result
in the generation of harmful content. While prior defenses mitigate these risks
by perturbing or inspecting inputs, they ignore competing objectives, the
underlying cause of alignment failures. In this paper, we propose
Alignment-Enhanced Decoding (AED), a novel defense that employs adaptive
decoding to address the root causes of jailbreak issues. We first define the
Competitive Index to quantify alignment failures and utilize feedback from
self-evaluation to compute post-alignment logits. Then, AED adaptively combines
AED and post-alignment logits with the original logits to obtain harmless and
helpful distributions. Consequently, our method enhances safety alignment while
maintaining helpfulness. We conduct experiments across five models and four
common jailbreaks, with the results validating the effectiveness of our
approach. Code is available at https://github.com/GIGABaozi/AED.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by EMNLP 2024, 15 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for
  Fast, Memory Efficient, and Long Context Finetuning and Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13663v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13663v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Warner, Antoine Chaffin, Benjamin Clavié, Orion Weller, Oskar Hallström, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, Iacopo Poli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Encoder-only transformer models such as BERT offer a great performance-size
tradeoff for retrieval and classification tasks with respect to larger
decoder-only models. Despite being the workhorse of numerous production
pipelines, there have been limited Pareto improvements to BERT since its
release. In this paper, we introduce ModernBERT, bringing modern model
optimizations to encoder-only models and representing a major Pareto
improvement over older encoders. Trained on 2 trillion tokens with a native
8192 sequence length, ModernBERT models exhibit state-of-the-art results on a
large pool of evaluations encompassing diverse classification tasks and both
single and multi-vector retrieval on different domains (including code). In
addition to strong downstream performance, ModernBERT is also the most speed
and memory efficient encoder and is designed for inference on common GPUs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Retrieval Augmented Language Model with Self-Reasoning <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19813v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19813v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuan Xia, Jingbo Zhou, Zhenhui Shi, Jun Chen, Haifeng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Retrieval-Augmented Language Model (RALM) has shown remarkable
performance on knowledge-intensive tasks by incorporating external knowledge
during inference, which mitigates the factual hallucinations inherited in large
language models (LLMs). Despite these advancements, challenges persist in the
implementation of RALMs, particularly concerning their reliability and
traceability. To be specific, the irrelevant document retrieval may result in
unhelpful response generation or even deteriorate the performance of LLMs,
while the lack of proper citations in generated outputs complicates efforts to
verify the trustworthiness of the models. To this end, we propose a novel
self-reasoning framework aimed at improving the reliability and traceability of
RALMs, whose core idea is to leverage reasoning trajectories generated by the
LLM itself. The framework involves constructing self-reason trajectories with
three processes: a relevance-aware process, an evidence-aware selective
process, and a trajectory analysis process. We have evaluated our framework
across four public datasets (two short-form QA datasets, one long-form QA
dataset, and one fact verification dataset) to demonstrate the superiority of
our method, which can outperform existing state-of-the-art models and can
achieve comparable performance with GPT-4, while only using 2,000 training
samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Progressive Multi-granular Alignments for Grounded Reasoning in Large
  Vision-Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08125v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08125v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quang-Hung Le, Long Hoang Dang, Ngan Le, Truyen Tran, Thao Minh Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Large Vision-Language Models (LVLMs) excel at matching concepts
across multi-modal inputs but struggle with compositional concepts and
high-level relationships between entities. This paper introduces Progressive
multi-granular Vision-Language alignments (PromViL), a novel framework to
enhance LVLMs' ability in performing grounded compositional visual reasoning
tasks. Our approach constructs a hierarchical structure of multi-modal
alignments, ranging from simple to complex concepts. By progressively aligning
textual descriptions with corresponding visual regions, our model learns to
leverage contextual information from lower levels to inform higher-level
reasoning. To facilitate this learning process, we introduce a data generation
process that creates a novel dataset derived from Visual Genome, providing a
wide range of nested compositional vision-language pairs. Experimental results
demonstrate that our PromViL framework significantly outperforms baselines on
various visual grounding and compositional question answering tasks. The code
is available at: https://github.com/lqh52/PromViL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unleashing the Unseen: Harnessing Benign <span class="highlight-title">Dataset</span>s for Jailbreaking Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00451v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00451v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Zhao, Zhe Li, Yige Li, Jun Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant ongoing efforts in safety alignment, large language
models (LLMs) such as GPT-4 and LLaMA 3 remain vulnerable to jailbreak attacks
that can induce harmful behaviors, including through the use of adversarial
suffixes. Building on prior research, we hypothesize that these adversarial
suffixes are not mere bugs but may represent features that can dominate the
LLM's behavior. To evaluate this hypothesis, we conduct several experiments.
First, we demonstrate that benign features can be effectively made to function
as adversarial suffixes, i.e., we develop a feature extraction method to
extract sample-agnostic features from benign dataset in the form of suffixes
and show that these suffixes may effectively compromise safety alignment.
Second, we show that adversarial suffixes generated from jailbreak attacks may
contain meaningful features, i.e., appending the same suffix to different
prompts results in responses exhibiting specific characteristics. Third, we
show that such benign-yet-safety-compromising features can be easily introduced
through fine-tuning using only benign datasets. As a result, we are able to
completely eliminate GPT's safety alignment in a blackbox setting through
finetuning with only benign data. Our code and data is available at
\url{https://github.com/suffix-maybe-feature/adver-suffix-maybe-features}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VisualRWKV: Exploring Recurrent Neural Networks for Visual Language
  Models <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13362v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13362v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowen Hou, Peigen Zeng, Fei Ma, Fei Richard Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Language Models (VLMs) have rapidly progressed with the recent success
of large language models. However, there have been few attempts to incorporate
efficient linear Recurrent Neural Networks (RNNs) architectures into VLMs. In
this study, we introduce VisualRWKV, the first application of a linear RNN
model to multimodal learning tasks, leveraging the pre-trained RWKV language
model. We propose a data-dependent recurrence and sandwich prompts to enhance
our modeling capabilities, along with a 2D image scanning mechanism to enrich
the processing of visual sequences. Extensive experiments demonstrate that
VisualRWKV achieves competitive performance compared to Transformer-based
models like LLaVA-1.5 on various benchmarks. Compared to LLaVA-1.5, VisualRWKV
has a speed advantage of 3.98 times and can save 54% of GPU memory when
reaching an inference length of 24K tokens. To facilitate further research and
analysis, we have made the checkpoints and the associated code publicly
accessible at the following GitHub repository: see
https://github.com/howard-hou/VisualRWKV.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at COLING 2025 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fairness in Large Language Models: A Taxonomic <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01349v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01349v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhibo Chu, Zichong Wang, Wenbin Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable success across
various domains. However, despite their promising performance in numerous
real-world applications, most of these algorithms lack fairness considerations.
Consequently, they may lead to discriminatory outcomes against certain
communities, particularly marginalized populations, prompting extensive study
in fair LLMs. On the other hand, fairness in LLMs, in contrast to fairness in
traditional machine learning, entails exclusive backgrounds, taxonomies, and
fulfillment techniques. To this end, this survey presents a comprehensive
overview of recent advances in the existing literature concerning fair LLMs.
Specifically, a brief introduction to LLMs is provided, followed by an analysis
of factors contributing to bias in LLMs. Additionally, the concept of fairness
in LLMs is discussed categorically, summarizing metrics for evaluating bias in
LLMs and existing algorithms for promoting fairness. Furthermore, resources for
evaluating bias in LLMs, including toolkits and datasets, are summarized.
Finally, existing research challenges and open questions are discussed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Doubly-Universal Adversarial Perturbations: Deceiving Vision-Language
  Models Across Both Images and Text with a Single Perturbation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08108v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08108v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hee-Seon Kim, Minbeom Kim, Changick Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (VLMs) have demonstrated remarkable performance
across multimodal tasks by integrating vision encoders with large language
models (LLMs). However, these models remain vulnerable to adversarial attacks.
Among such attacks, Universal Adversarial Perturbations (UAPs) are especially
powerful, as a single optimized perturbation can mislead the model across
various input images. In this work, we introduce a novel UAP specifically
designed for VLMs: the Doubly-Universal Adversarial Perturbation (Doubly-UAP),
capable of universally deceiving VLMs across both image and text inputs. To
successfully disrupt the vision encoder's fundamental process, we analyze the
core components of the attention mechanism. After identifying value vectors in
the middle-to-late layers as the most vulnerable, we optimize Doubly-UAP in a
label-free manner with a frozen model. Despite being developed as a black-box
to the LLM, Doubly-UAP achieves high attack success rates on VLMs, consistently
outperforming baseline methods across vision-language tasks. Extensive ablation
studies and analyses further demonstrate the robustness of Doubly-UAP and
provide insights into how it influences internal attention mechanisms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Generated Critiques Boost Reward Modeling for Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16646v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16646v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Yu, Zhengxing Chen, Aston Zhang, Liang Tan, Chenguang Zhu, Richard Yuanzhe Pang, Yundi Qian, Xuewei Wang, Suchin Gururangan, Chao Zhang, Melanie Kambadur, Dhruv Mahajan, Rui Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward modeling is crucial for aligning large language models (LLMs) with
human preferences, especially in reinforcement learning from human feedback
(RLHF). However, current reward models mainly produce scalar scores and
struggle to incorporate critiques in a natural language format. We hypothesize
that predicting both critiques and the scalar reward would improve reward
modeling ability. Motivated by this, we propose Critic-RM, a framework that
improves reward models using self-generated critiques without extra
supervision. Critic-RM employs a two-stage process: generating and filtering
high-quality critiques, followed by joint fine-tuning on reward prediction and
critique generation. Experiments across benchmarks show that Critic-RM improves
reward modeling accuracy by 3.7%-7.3% compared to standard reward models and
LLM judges, demonstrating strong performance and data efficiency. Additional
studies further validate the effectiveness of generated critiques in rectifying
flawed reasoning steps with 2.5%-3.2% gains in improving reasoning accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge<span class="highlight-title">Prompt</span>s: Exploring the Abilities of Large Language Models to
  Solve Proportional Analogies via Knowledge-Enhanced <span class="highlight-title">Prompt</span>ing <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00869v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00869v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thilini Wijesiriwardene, Ruwan Wickramarachchi, Sreeram Vennam, Vinija Jain, Aman Chadha, Amitava Das, Ponnurangam Kumaraguru, Amit Sheth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Making analogies is fundamental to cognition. Proportional analogies, which
consist of four terms, are often used to assess linguistic and cognitive
abilities. For instance, completing analogies like "Oxygen is to Gas as <blank>
is to <blank>" requires identifying the semantic relationship (e.g., "type of")
between the first pair of terms ("Oxygen" and "Gas") and finding a second pair
that shares the same relationship (e.g., "Aluminum" and "Metal"). In this work,
we introduce a 15K Multiple-Choice Question Answering (MCQA) dataset for
proportional analogy completion and evaluate the performance of contemporary
Large Language Models (LLMs) in various knowledge-enhanced prompt settings.
Specifically, we augment prompts with three types of knowledge: exemplar,
structured, and targeted. Our results show that despite extensive training
data, solving proportional analogies remains challenging for current LLMs, with
the best model achieving an accuracy of 55%. Notably, we find that providing
targeted knowledge can better assist models in completing proportional
analogies compared to providing exemplars or collections of structured
knowledge. Our code and data are available at:
https://github.com/Thiliniiw/KnowledgePrompts/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UOR: Universal Backdoor Attacks on <span class="highlight-title">Pre-train</span>ed Language Models <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.09574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.09574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Du, Peixuan Li, Boqun Li, Haodong Zhao, Gongshen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backdoors implanted in pre-trained language models (PLMs) can be transferred
to various downstream tasks, which exposes a severe security threat. However,
most existing backdoor attacks against PLMs are un-targeted and task-specific.
Few targeted and task-agnostic methods use manually pre-defined triggers and
output representations, which prevent the attacks from being more effective and
general. In this paper, we first summarize the requirements that a more
threatening backdoor attack against PLMs should satisfy, and then propose a new
backdoor attack method called UOR, which breaks the bottleneck of the previous
approach by turning manual selection into automatic optimization. Specifically,
we define poisoned supervised contrastive learning which can automatically
learn the more uniform and universal output representations of triggers for
various PLMs. Moreover, we use gradient search to select appropriate trigger
words which can be adaptive to different PLMs and vocabularies. Experiments
show that our method can achieve better attack performance on various text
classification tasks compared to manual methods. Further, we tested our method
on PLMs with different architectures, different usage paradigms, and more
difficult tasks, which demonstrated the universality of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL-Findings 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DavIR: Data Selection via Implicit Reward for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.13008v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.13008v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotian Zhou, Tingkai Liu, Qianli Ma, Yufeng Zhang, Jianbo Yuan, Pengfei Liu, Yang You, Hongxia Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce DavIR, a model-based data selection method for post-training
Large Language Models. DavIR generalizes Reducible Holdout Loss to core-set
selection problem of causal language modeling, and quantifies the learnability
of a given datum with respect to a pre-trained LLM based on relative reduction
in loss during fine-tuning, a metric we show to be closely related to the
implicit reward model described in Direct Preference Optimization (DPO). We
show that 6% of Alpaca dataset selected with DavIR can steer both the LLaMA and
Gemma model family to produce superior performance compared to the same models
trained on the full 52K dataset. We also show that Alpaca dataset compressed
with DavIR can be combined with GSM8K dataset to effectively balance
open-domain freeform QA and mathematical reasoning capabilities. Finally, we
apply the DavIR objective to DPO and develop a normalized DavIR-DPO objective
which improves alignment performance of Zephyr-7B-SFT model by 8% (relative) on
AlpacaEval, compared against training on vanilla DPO objective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress? <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21792v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21792v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Ren, Steven Basart, Adam Khoja, Alice Gatti, Long Phan, Xuwang Yin, Mantas Mazeika, Alexander Pan, Gabriel Mukobi, Ryan H. Kim, Stephen Fitz, Dan Hendrycks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As artificial intelligence systems grow more powerful, there has been
increasing interest in "AI safety" research to address emerging and future
risks. However, the field of AI safety remains poorly defined and
inconsistently measured, leading to confusion about how researchers can
contribute. This lack of clarity is compounded by the unclear relationship
between AI safety benchmarks and upstream general capabilities (e.g., general
knowledge and reasoning). To address these issues, we conduct a comprehensive
meta-analysis of AI safety benchmarks, empirically analyzing their correlation
with general capabilities across dozens of models and providing a survey of
existing directions in AI safety. Our findings reveal that many safety
benchmarks highly correlate with both upstream model capabilities and training
compute, potentially enabling "safetywashing" -- where capability improvements
are misrepresented as safety advancements. Based on these findings, we propose
an empirical foundation for developing more meaningful safety metrics and
define AI safety in a machine learning research context as a set of clearly
delineated research goals that are empirically separable from generic
capabilities advancements. In doing so, we aim to provide a more rigorous
framework for AI safety research, advancing the science of safety evaluations
and clarifying the path towards measurable progress.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SafeAligner: Safety Alignment against Jailbreak Attacks via Response
  Disparity Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.18118v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.18118v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caishuang Huang, Wanxu Zhao, Rui Zheng, Huijie Lv, Shihan Dou, Sixian Li, Xiao Wang, Enyu Zhou, Junjie Ye, Yuming Yang, Tao Gui, Qi Zhang, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the development of large language models (LLMs) rapidly advances, securing
these models effectively without compromising their utility has become a
pivotal area of research. However, current defense strategies against jailbreak
attacks (i.e., efforts to bypass security protocols) often suffer from limited
adaptability, restricted general capability, and high cost. To address these
challenges, we introduce SafeAligner, a methodology implemented at the decoding
stage to fortify defenses against jailbreak attacks. We begin by developing two
specialized models: the Sentinel Model, which is trained to foster safety, and
the Intruder Model, designed to generate riskier responses. SafeAligner
leverages the disparity in security levels between the responses from these
models to differentiate between harmful and beneficial tokens, effectively
guiding the safety alignment by altering the output token distribution of the
target model. Extensive experiments show that SafeAligner can increase the
likelihood of beneficial tokens, while reducing the occurrence of harmful ones,
thereby ensuring secure alignment with minimal loss to generality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agent Planning with World Knowledge Model <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14205v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14205v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent endeavors towards directly using large language models (LLMs) as agent
models to execute interactive planning tasks have shown commendable results.
Despite their achievements, however, they still struggle with brainless
trial-and-error in global planning and generating hallucinatory actions in
local planning due to their poor understanding of the ``real'' physical world.
Imitating humans' mental world knowledge model which provides global prior
knowledge before the task and maintains local dynamic knowledge during the
task, in this paper, we introduce parametric World Knowledge Model (WKM) to
facilitate agent planning. Concretely, we steer the agent model to
self-synthesize knowledge from both expert and sampled trajectories. Then we
develop WKM, providing prior task knowledge to guide the global planning and
dynamic state knowledge to assist the local planning. Experimental results on
three complex real-world simulated datasets with three state-of-the-art
open-source LLMs, Mistral-7B, Gemma-7B, and Llama-3-8B, demonstrate that our
method can achieve superior performance compared to various strong baselines.
Besides, we analyze to illustrate that our WKM can effectively alleviate the
blind trial-and-error and hallucinatory action issues, providing strong support
for the agent's understanding of the world. Other interesting findings include:
1) our instance-level task knowledge can generalize better to unseen tasks, 2)
weak WKM can guide strong agent model planning, and 3) unified WKM training has
promising potential for further development. The code is available at
https://github.com/zjunlp/WKM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of
  Large Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14768v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14768v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) need knowledge updates to meet the ever-growing
world facts and correct the hallucinated responses, facilitating the methods of
lifelong model editing. Where the updated knowledge resides in memories is a
fundamental question for model editing. In this paper, we find that editing
either long-term memory (direct model parameters) or working memory
(non-parametric knowledge of neural network activations/representations by
retrieval) will result in an impossible triangle -- reliability,
generalization, and locality can not be realized together in the lifelong
editing settings. For long-term memory, directly editing the parameters will
cause conflicts with irrelevant pretrained knowledge or previous edits (poor
reliability and locality). For working memory, retrieval-based activations can
hardly make the model understand the edits and generalize (poor
generalization). Therefore, we propose WISE to bridge the gap between memories.
In WISE, we design a dual parametric memory scheme, which consists of the main
memory for the pretrained knowledge and a side memory for the edited knowledge.
We only edit the knowledge in the side memory and train a router to decide
which memory to go through when given a query. For continual editing, we devise
a knowledge-sharding mechanism where different sets of edits reside in distinct
subspaces of parameters, and are subsequently merged into a shared memory
without conflicts. Extensive experiments show that WISE can outperform previous
model editing methods and overcome the impossible triangle under lifelong model
editing of question answering, hallucination, and out-of-distribution settings
across trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is
available at https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DialSim: A Real-Time Simulator for Evaluating Long-Term Multi-Party
  Dialogue Understanding of Conversational Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13144v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13144v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiho Kim, Woosog Chay, Hyeonji Hwang, Daeun Kyung, Hyunseung Chung, Eunbyeol Cho, Yohan Jo, Edward Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) have significantly
enhanced the capabilities of conversational agents, making them applicable to
various fields (e.g., education). Despite their progress, the evaluation of the
agents often overlooks the complexities of real-world conversations, such as
real-time interactions, multi-party dialogues, and extended contextual
dependencies. To bridge this gap, we introduce DialSim, a real-time dialogue
simulator. In this simulator, an agent is assigned the role of a character from
popular TV shows, requiring it to respond to spontaneous questions using past
dialogue information and to distinguish between known and unknown information.
Key features of DialSim include assessing the agent's ability to respond within
a reasonable time limit, handling long-term multi-party dialogues, and
evaluating performance under randomized questioning with LongDialQA, a novel,
high-quality question-answering dataset. Our experiments using DialSim reveal
the strengths and weaknesses of the latest conversational agents, offering
valuable insights for future advancements in conversational AI. DialSim is
available at https://dialsim.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Circuits in <span class="highlight-title">Pretrain</span>ed <span class="highlight-title">Transformer</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17969v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17969v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable capabilities of modern large language models are rooted in
their vast repositories of knowledge encoded within their parameters, enabling
them to perceive the world and engage in reasoning. The inner workings of how
these models store knowledge have long been a subject of intense interest and
investigation among researchers. To date, most studies have concentrated on
isolated components within these models, such as the Multilayer Perceptrons and
attention head. In this paper, we delve into the computation graph of the
language model to uncover the knowledge circuits that are instrumental in
articulating specific knowledge. The experiments, conducted with GPT2 and
TinyLLAMA, have allowed us to observe how certain information heads, relation
heads, and Multilayer Perceptrons collaboratively encode knowledge within the
model. Moreover, we evaluate the impact of current knowledge editing techniques
on these knowledge circuits, providing deeper insights into the functioning and
constraints of these editing methodologies. Finally, we utilize knowledge
circuits to analyze and interpret language model behaviors such as
hallucinations and in-context learning. We believe the knowledge circuits hold
potential for advancing our understanding of Transformers and guiding the
improved design of knowledge editing. Code and data are available in
https://github.com/zjunlp/KnowledgeCircuits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024, 26 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">21</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nano-ESG: Extracting Corporate Sustainability Information from News
  Articles <span class="chip">ECIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Billert, Stefan Conrad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Determining the sustainability impact of companies is a highly complex
subject which has garnered more and more attention over the past few years.
Today, investors largely rely on sustainability-ratings from established
rating-providers in order to analyze how responsibly a company acts. However,
those ratings have recently been criticized for being hard to understand and
nearly impossible to reproduce.
  An independent way to find out about the sustainability practices of
companies lies in the rich landscape of news article data. In this paper, we
explore a different approach to identify key opportunities and challenges of
companies in the sustainability domain. We present a novel dataset of more than
840,000 news articles which were gathered for major German companies between
January 2023 and September 2024. By applying a mixture of Natural Language
Processing techniques, we first identify relevant articles, before summarizing
them and extracting their sustainability-related sentiment and aspect using
Large Language Models (LLMs). Furthermore, we conduct an evaluation of the
obtained data and determine that the LLM-produced answers are accurate. We
release both datasets at https://github.com/Bailefan/Nano-ESG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published at ECIR 2025. Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DisCo: Graph-Based Disentangled Contrastive Learning for Cold-Start
  Cross-Domain Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hourun Li, Yifan Wang, Zhiping Xiao, Jia Yang, Changling Zhou, Ming Zhang, Wei Ju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are widely used in various real-world applications, but
they often encounter the persistent challenge of the user cold-start problem.
Cross-domain recommendation (CDR), which leverages user interactions from one
domain to improve prediction performance in another, has emerged as a promising
solution. However, users with similar preferences in the source domain may
exhibit different interests in the target domain. Therefore, directly
transferring embeddings may introduce irrelevant source-domain collaborative
information. In this paper, we propose a novel graph-based disentangled
contrastive learning framework to capture fine-grained user intent and filter
out irrelevant collaborative information, thereby avoiding negative transfer.
Specifically, for each domain, we use a multi-channel graph encoder to capture
diverse user intents. We then construct the affinity graph in the embedding
space and perform multi-step random walks to capture high-order user similarity
relationships. Treating one domain as the target, we propose a disentangled
intent-wise contrastive learning approach, guided by user similarity, to refine
the bridging of user intents across domains. Extensive experiments on four
benchmark CDR datasets demonstrate that DisCo consistently outperforms existing
state-of-the-art baselines, thereby validating the effectiveness of both DisCo
and its components.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spectrum-based Modality Representation Fusion Graph Convolutional
  Network for Multimodal Recommendation <span class="chip">WSDM</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongqing Kenneth Ong, Andy W. H. Khong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating multi-modal features as side information has recently become a
trend in recommender systems. To elucidate user-item preferences, recent
studies focus on fusing modalities via concatenation, element-wise sum, or
attention mechanisms. Despite having notable success, existing approaches do
not account for the modality-specific noise encapsulated within each modality.
As a result, direct fusion of modalities will lead to the amplification of
cross-modality noise. Moreover, the variation of noise that is unique within
each modality results in noise alleviation and fusion being more challenging.
In this work, we propose a new Spectrum-based Modality Representation (SMORE)
fusion graph recommender that aims to capture both uni-modal and fusion
preferences while simultaneously suppressing modality noise. Specifically,
SMORE projects the multi-modal features into the frequency domain and leverages
the spectral space for fusion. To reduce dynamic contamination that is unique
to each modality, we introduce a filter to attenuate and suppress the modality
noise adaptively while capturing the universal modality patterns effectively.
Furthermore, we explore the item latent structures by designing a new
multi-modal graph learning module to capture associative semantic correlations
and universal fusion patterns among similar items. Finally, we formulate a new
modality-aware preference module, which infuses behavioral features and
balances the uni- and multi-modal features for precise preference modeling.
This empowers SMORE with the ability to infer both user modality-specific and
fusion preferences more accurately. Experiments on three real-world datasets
show the efficacy of our proposed model. The source code for this work has been
made publicly available at https://github.com/kennethorq/SMORE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Web Search and Data Mining (WSDM) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECLIPSE: Contrastive Dimension Importance Estimation with
  Pseudo-Irrelevance Feedback for Dense Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14967v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14967v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giulio D'Erasmo, Giovanni Trappolini, Nicola Tonellotto, Fabrizio Silvestri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Information Retrieval have leveraged high-dimensional
embedding spaces to improve the retrieval of relevant documents. Moreover, the
Manifold Clustering Hypothesis suggests that despite these high-dimensional
representations, documents relevant to a query reside on a lower-dimensional,
query-dependent manifold. While this hypothesis has inspired new retrieval
methods, existing approaches still face challenges in effectively separating
non-relevant information from relevant signals. We propose a novel methodology
that addresses these limitations by leveraging information from both relevant
and non-relevant documents. Our method, ECLIPSE, computes a centroid based on
irrelevant documents as a reference to estimate noisy dimensions present in
relevant ones, enhancing retrieval performance. Extensive experiments on three
in-domain and one out-of-domain benchmarks demonstrate an average improvement
of up to 19.50% (resp. 22.35%) in mAP(AP) and 11.42% (resp. 13.10%) in nDCG@10
w.r.t. the DIME-based baseline (resp. the baseline using all dimensions). Our
results pave the way for more robust, pseudo-irrelevance-based retrieval
systems in future IR research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Progressive Multimodal Reasoning via Active Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14835v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14835v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanting Dong, Chenghao Zhang, Mengjie Deng, Yutao Zhu, Zhicheng Dou, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-step multimodal reasoning tasks pose significant challenges for
multimodal large language models (MLLMs), and finding effective ways to enhance
their performance in such scenarios remains an unresolved issue. In this paper,
we propose AR-MCTS, a universal framework designed to progressively improve the
reasoning capabilities of MLLMs through Active Retrieval (AR) and Monte Carlo
Tree Search (MCTS). Our approach begins with the development of a unified
retrieval module that retrieves key supporting insights for solving complex
reasoning problems from a hybrid-modal retrieval corpus. To bridge the gap in
automated multimodal reasoning verification, we employ the MCTS algorithm
combined with an active retrieval mechanism, which enables the automatic
generation of step-wise annotations. This strategy dynamically retrieves key
insights for each reasoning step, moving beyond traditional beam search
sampling to improve the diversity and reliability of the reasoning space.
Additionally, we introduce a process reward model that aligns progressively to
support the automatic verification of multimodal reasoning tasks. Experimental
results across three complex multimodal reasoning benchmarks confirm the
effectiveness of the AR-MCTS framework in enhancing the performance of various
multimodal models. Further analysis demonstrates that AR-MCTS can optimize
sampling diversity and accuracy, yielding reliable multimodal reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sliding Windows Are Not the End: Exploring Full Ranking with
  Long-Context Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhan Liu, Xinyu Ma, Yutao Zhu, Ziliang Zhao, Shuaiqiang Wang, Dawei Yin, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown exciting performance in listwise
passage ranking. Due to the limited input length, existing methods often adopt
the sliding window strategy. Such a strategy, though effective, is inefficient
as it involves repetitive and serialized processing, which usually re-evaluates
relevant passages multiple times. As a result, it incurs redundant API costs,
which are proportional to the number of inference tokens. The development of
long-context LLMs enables the full ranking of all passages within a single
inference, avoiding redundant API costs. In this paper, we conduct a
comprehensive study of long-context LLMs for ranking tasks in terms of
efficiency and effectiveness. Surprisingly, our experiments reveal that full
ranking with long-context LLMs can deliver superior performance in the
supervised fine-tuning setting with a huge efficiency improvement. Furthermore,
we identify two limitations of fine-tuning the full ranking model based on
existing methods: (1) sliding window strategy fails to produce a full ranking
list as a training label, and (2) the language modeling loss cannot emphasize
top-ranked passage IDs in the label. To alleviate these issues, we propose a
new complete listwise label construction approach and a novel importance-aware
learning objective for full ranking. Experiments show the superior performance
of our method over baselines. Our codes are available at
\url{https://github.com/8421BCD/fullrank}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient <span class="highlight-title">Self-Supervised</span> Video Hashing with Selective State Spaces <span class="chip">AAAI'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinpeng Wang, Niu Lian, Jun Li, Yuting Wang, Yan Feng, Bin Chen, Yongbing Zhang, Shu-Tao Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised video hashing (SSVH) is a practical task in video indexing
and retrieval. Although Transformers are predominant in SSVH for their
impressive temporal modeling capabilities, they often suffer from computational
and memory inefficiencies. Drawing inspiration from Mamba, an advanced
state-space model, we explore its potential in SSVH to achieve a better balance
between efficacy and efficiency. We introduce S5VH, a Mamba-based video hashing
model with an improved self-supervised learning paradigm. Specifically, we
design bidirectional Mamba layers for both the encoder and decoder, which are
effective and efficient in capturing temporal relationships thanks to the
data-dependent selective scanning mechanism with linear complexity. In our
learning strategy, we transform global semantics in the feature space into
semantically consistent and discriminative hash centers, followed by a center
alignment loss as a global learning signal. Our self-local-global (SLG)
paradigm significantly improves learning efficiency, leading to faster and
better convergence. Extensive experiments demonstrate S5VH's improvements over
state-of-the-art methods, superior transferability, and scalable advantages in
inference efficiency. Code is available at
https://github.com/gimpong/AAAI25-S5VH.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI'25. 9 pages, 5 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Moving Beyond LDA: A Comparison of Unsupervised Topic Modelling
  Techniques for Qualitative Data Analysis of Online Communities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14486v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14486v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amandeep Kaur, James R. Wallace
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media constitutes a rich and influential source of information for
qualitative researchers. Although computational techniques like topic modelling
assist with managing the volume and diversity of social media content,
qualitative researcher's lack of programming expertise creates a significant
barrier to their adoption. In this paper we explore how BERTopic, an advanced
Large Language Model (LLM)-based topic modelling technique, can support
qualitative data analysis of social media. We conducted interviews and hands-on
evaluations in which qualitative researchers compared topics from three
modelling techniques: LDA, NMF, and BERTopic. BERTopic was favoured by 8 of 12
participants for its ability to provide detailed, coherent clusters for deeper
understanding and actionable insights. Participants also prioritised topic
relevance, logical organisation, and the capacity to reveal unexpected
relationships within the data. Our findings underscore the potential of
LLM-based techniques for supporting qualitative analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HEC-GCN: Hypergraph Enhanced Cascading Graph Convolution Network for
  Multi-Behavior Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yabo Yin, Xiaofei Zhu, Wenshan Wang, Yihao Zhang, Pengfei Wang, Yixing Fan, Jiafeng Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-behavior recommendation (MBR) has garnered growing attention recently
due to its ability to mitigate the sparsity issue by inferring user preferences
from various auxiliary behaviors to improve predictions for the target
behavior. Although existing research on MBR has yielded impressive results,
they still face two major limitations. First, previous methods mainly focus on
modeling fine-grained interaction information between users and items under
each behavior, which may suffer from sparsity issue. Second, existing models
usually concentrate on exploiting dependencies between two consecutive
behaviors, leaving intra- and inter-behavior consistency largely unexplored. To
the end, we propose a novel approach named Hypergraph Enhanced Cascading Graph
Convolution Network for multi-behavior recommendation (HEC-GCN). To be
specific, we first explore both fine- and coarse-grained correlations among
users or items of each behavior by simultaneously modeling the
behavior-specific interaction graph and its corresponding hypergraph in a
cascaded manner. Then, we propose a behavior consistency-guided alignment
strategy that ensures consistent representations between the interaction graph
and its associated hypergraph for each behavior, while also maintaining
representation consistency across different behaviors. Extensive experiments
and analyses on three public benchmark datasets demonstrate that our proposed
approach is consistently superior to previous state-of-the-art methods due to
its capability to effectively attenuate the sparsity issue as well as preserve
both intra- and inter-behavior consistencies. The code is available at
https://github.com/marqu22/HEC-GCN.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VISA: Retrieval Augmented Generation with Visual Source Attribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueguang Ma, Shengyao Zhuang, Bevan Koopman, Guido Zuccon, Wenhu Chen, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generation with source attribution is important for enhancing the
verifiability of retrieval-augmented generation (RAG) systems. However,
existing approaches in RAG primarily link generated content to document-level
references, making it challenging for users to locate evidence among multiple
content-rich retrieved documents. To address this challenge, we propose
Retrieval-Augmented Generation with Visual Source Attribution (VISA), a novel
approach that combines answer generation with visual source attribution.
Leveraging large vision-language models (VLMs), VISA identifies the evidence
and highlights the exact regions that support the generated answers with
bounding boxes in the retrieved document screenshots. To evaluate its
effectiveness, we curated two datasets: Wiki-VISA, based on crawled Wikipedia
webpage screenshots, and Paper-VISA, derived from PubLayNet and tailored to the
medical domain. Experimental results demonstrate the effectiveness of VISA for
visual source attribution on documents' original look, as well as highlighting
the challenges for improvement. Code, data, and model checkpoints will be
released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Longer <span class="highlight-title">Prompt</span>s Always Better? <span class="highlight-title">Prompt</span> Selection in Large Language
  Models for Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Genki Kusano, Kosuke Akimoto, Kunihiro Takeoka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In large language models (LLM)-based recommendation systems (LLM-RSs),
accurately predicting user preferences by leveraging the general knowledge of
LLMs is possible without requiring extensive training data. By converting
recommendation tasks into natural language inputs called prompts, LLM-RSs can
efficiently solve issues that have been difficult to address due to data
scarcity but are crucial in applications such as cold-start and cross-domain
problems. However, when applying this in practice, selecting the prompt that
matches tasks and data is essential. Although numerous prompts have been
proposed in LLM-RSs and representing the target user in prompts significantly
impacts recommendation accuracy, there are still no clear guidelines for
selecting specific prompts.
  In this paper, we categorize and analyze prompts from previous research to
establish practical prompt selection guidelines. Through 450 experiments with
90 prompts and five real-world datasets, we examined the relationship between
prompts and dataset characteristics in recommendation accuracy. We found that
no single prompt consistently outperforms others; thus, selecting prompts on
the basis of dataset characteristics is crucial. Here, we propose a prompt
selection method that achieves higher accuracy with minimal validation data.
Because increasing the number of prompts to explore raises costs, we also
introduce a cost-efficient strategy using high-performance and cost-efficient
LLMs, significantly reducing exploration costs while maintaining high
prediction accuracy. Our work offers valuable insights into the prompt
selection, advancing accurate and efficient LLM-RSs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Metric Compatible Training for Online Backfilling in Large-Scale
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.03767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.03767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonguk Seo, Mustafa Gokhan Uzunbas, Bohyung Han, Sara Cao, Ser-Nam Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backfilling is the process of re-extracting all gallery embeddings from
upgraded models in image retrieval systems. It inevitably requires a
prohibitively large amount of computational cost and even entails the downtime
of the service. Although backward-compatible learning sidesteps this challenge
by tackling query-side representations, this leads to suboptimal solutions in
principle because gallery embeddings cannot benefit from model upgrades. We
address this dilemma by introducing an online backfilling algorithm, which
enables us to achieve a progressive performance improvement during the
backfilling process while not sacrificing the final performance of new model
after the completion of backfilling. To this end, we first propose a simple
distance rank merge technique for online backfilling. Then, we incorporate a
reverse transformation module for more effective and efficient merging, which
is further enhanced by adopting a metric-compatible contrastive learning
approach. These two components help to make the distances of old and new models
compatible, resulting in desirable merge results during backfilling with no
extra computational overhead. Extensive experiments show the effectiveness of
our framework on four standard benchmarks in various settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Agent-OM: Leveraging LLM Agents for Ontology Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.00326v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.00326v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangcheng Qiang, Weiqing Wang, Kerry Taylor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ontology matching (OM) enables semantic interoperability between different
ontologies and resolves their conceptual heterogeneity by aligning related
entities. OM systems currently have two prevailing design paradigms:
conventional knowledge-based expert systems and newer machine learning-based
predictive systems. While large language models (LLMs) and LLM agents have
revolutionised data engineering and have been applied creatively in many
domains, their potential for OM remains underexplored. This study introduces a
novel agent-powered LLM-based design paradigm for OM systems. With
consideration of several specific challenges in leveraging LLM agents for OM,
we propose a generic framework, namely Agent-OM (Agent for Ontology Matching),
consisting of two Siamese agents for retrieval and matching, with a set of
simple OM tools. Our framework is implemented in a proof-of-concept system.
Evaluations of three Ontology Alignment Evaluation Initiative (OAEI) tracks
over state-of-the-art OM systems show that our system can achieve results very
close to the long-standing best performance on simple OM tasks and can
significantly improve the performance on complex and few-shot OM tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 13 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DNS-Rec: Data-aware Neural Architecture Search for Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.00390v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.00390v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Zhang, Maolin Wang, Yao Zhao, Chenyi Zhuang, Jinjie Gu, Ruocheng Guo, Xiangyu Zhao, Zijian Zhang, Hongzhi Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of data proliferation, efficiently sifting through vast
information to extract meaningful insights has become increasingly crucial.
This paper addresses the computational overhead and resource inefficiency
prevalent in existing Sequential Recommender Systems (SRSs). We introduce an
innovative approach combining pruning methods with advanced model designs.
Furthermore, we delve into resource-constrained Neural Architecture Search
(NAS), an emerging technique in recommender systems, to optimize models in
terms of FLOPs, latency, and energy consumption while maintaining or enhancing
accuracy. Our principal contribution is the development of a Data-aware Neural
Architecture Search for Recommender System (DNS-Rec). DNS-Rec is specifically
designed to tailor compact network architectures for attention-based SRS
models, thereby ensuring accuracy retention. It incorporates data-aware gates
to enhance the performance of the recommendation network by learning
information from historical user-item interactions. Moreover, DNS-Rec employs a
dynamic resource constraint strategy, stabilizing the search process and
yielding more suitable architectural solutions. We demonstrate the
effectiveness of our approach through rigorous experiments conducted on three
benchmark datasets, which highlight the superiority of DNS-Rec in SRSs. Our
findings set a new standard for future research in efficient and accurate
recommendation systems, marking a significant step forward in this rapidly
evolving field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probability Distribution Learning and Its Application in Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05666v9">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05666v9.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binchuan Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel theoretical learning framework, termed
probability distribution learning (PD learning). Departing from the traditional
statistical learning framework, PD learning focuses on learning the underlying
probability distribution, which is modeled as a random variable within the
probability simplex. In this framework, the optimization objective is the
learning error, which quantifies the posterior expected discrepancy between the
model's predicted distribution and the underlying true distribution, given
available sample data and prior knowledge. To optimize the learning error, this
paper proposes the necessary conditions for loss functions, models, and
optimization algorithms, ensuring that these conditions are met in real-world
machine learning scenarios. Based on these conditions, the non-convex
optimization mechanism corresponding to model training can be theoretically
resolved. Moreover, this paper provides model-dependent and model-independent
bounds on learning error, offering new insights into the model's fitting and
generalization capabilities. Furthermore, the paper applies the PD learning
framework to elucidate the mechanisms by which various techniques, including
random parameter initialization, over-parameterization, and dropout, influence
deep model training. Finally, the paper substantiates the key conclusions of
the proposed framework through experimental results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2105.04026 by other
  authors. arXiv admin note: text overlap with arXiv:2105.04026 by other
  authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lightning IR: Straightforward Fine-tuning and Inference of
  <span class="highlight-title">Transformer</span>-based Language Models for Information Retrieval <span class="chip">WSDM'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.04677v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.04677v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ferdinand Schlatt, Maik Fröbe, Matthias Hagen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A wide range of transformer-based language models have been proposed for
information retrieval tasks. However, including transformer-based models in
retrieval pipelines is often complex and requires substantial engineering
effort. In this paper, we introduce Lightning IR, an easy-to-use PyTorch
Lightning-based framework for applying transformer-based language models in
retrieval scenarios. Lightning IR provides a modular and extensible
architecture that supports all stages of a retrieval pipeline: from fine-tuning
and indexing to searching and re-ranking. Designed to be scalable and
reproducible, Lightning IR is available as open-source:
https://github.com/webis-de/lightning-ir.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a demo at WSDM'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distribution-Consistency-Guided Multi-modal Hashing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11216v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11216v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin-Yu Liu, Xian-Ling Mao, Tian-Yi Che, Rong-Cheng Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal hashing methods have gained popularity due to their fast speed
and low storage requirements. Among them, the supervised methods demonstrate
better performance by utilizing labels as supervisory signals compared with
unsupervised methods. Currently, for almost all supervised multi-modal hashing
methods, there is a hidden assumption that training sets have no noisy labels.
However, labels are often annotated incorrectly due to manual labeling in
real-world scenarios, which will greatly harm the retrieval performance. To
address this issue, we first discover a significant distribution consistency
pattern through experiments, i.e., the 1-0 distribution of the presence or
absence of each category in the label is consistent with the high-low
distribution of similarity scores of the hash codes relative to category
centers. Then, inspired by this pattern, we propose a novel
Distribution-Consistency-Guided Multi-modal Hashing (DCGMH), which aims to
filter and reconstruct noisy labels to enhance retrieval performance.
Specifically, the proposed method first randomly initializes several category
centers, which are used to compute the high-low distribution of similarity
scores; Noisy and clean labels are then separately filtered out via the
discovered distribution consistency pattern to mitigate the impact of noisy
labels; Subsequently, a correction strategy, which is indirectly designed via
the distribution consistency pattern, is applied to the filtered noisy labels,
correcting high-confidence ones while treating low-confidence ones as unlabeled
for unsupervised learning, thereby further enhancing the model's performance.
Extensive experiments on three widely used datasets demonstrate the superiority
of the proposed method compared to state-of-the-art baselines in multi-modal
retrieval tasks. The code is available at
https://github.com/LiuJinyu1229/DCGMH.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DLCRec: A Novel Approach for Managing Diversity in LLM-Based Recommender
  Systems <span class="chip">WSDM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.12470v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.12470v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaju Chen, Chongming Gao, Shuai Yuan, Shuchang Liu, Qingpeng Cai, Peng Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of Large Language Models (LLMs) into recommender systems has
led to substantial performance improvements. However, this often comes at the
cost of diminished recommendation diversity, which can negatively impact user
satisfaction. To address this issue, controllable recommendation has emerged as
a promising approach, allowing users to specify their preferences and receive
recommendations that meet their diverse needs. Despite its potential, existing
controllable recommender systems frequently rely on simplistic mechanisms, such
as a single prompt, to regulate diversity-an approach that falls short of
capturing the full complexity of user preferences. In response to these
limitations, we propose DLCRec, a novel framework designed to enable
fine-grained control over diversity in LLM-based recommendations. Unlike
traditional methods, DLCRec adopts a fine-grained task decomposition strategy,
breaking down the recommendation process into three sequential sub-tasks: genre
prediction, genre filling, and item prediction. These sub-tasks are trained
independently and inferred sequentially according to user-defined control
numbers, ensuring more precise control over diversity. Furthermore, the
scarcity and uneven distribution of diversity-related user behavior data pose
significant challenges for fine-tuning. To overcome these obstacles, we
introduce two data augmentation techniques that enhance the model's robustness
to noisy and out-of-distribution data. These techniques expose the model to a
broader range of patterns, improving its adaptability in generating
recommendations with varying levels of diversity. Our extensive empirical
evaluation demonstrates that DLCRec not only provides precise control over
diversity but also outperforms state-of-the-art baselines across multiple
recommendation scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WSDM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SCONE: A Novel Stochastic Sampling to Generate Contrastive Views and
  Hard Negative Samples for Recommendation <span class="chip">WSDM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.00287v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.00287v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaejeong Lee, Jeongwhan Choi, Hyowon Wi, Sung-Bae Cho, Noseong Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-based collaborative filtering (CF) has emerged as a promising approach
in recommender systems. Despite its achievements, graph-based CF models face
challenges due to data sparsity and negative sampling. In this paper, we
propose a novel Stochastic sampling for i) COntrastive views and ii) hard
NEgative samples (SCONE) to overcome these issues. SCONE generates dynamic
augmented views and diverse hard negative samples via a unified stochastic
sampling approach based on score-based generative models. Our extensive
experiments on 6 benchmark datasets show that SCONE consistently outperforms
state-of-the-art baselines. SCONE shows efficacy in addressing user sparsity
and item popularity issues, while enhancing performance for both cold-start
users and long-tail items. Furthermore, our approach improves the diversity of
the recommendation and the uniformity of the representations. The code is
available at https://github.com/jeongwhanchoi/SCONE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WSDM 2025. Chaejeong Lee and Jeongwhan Choi are co-first
  authors with equal contributions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WISE: Rethinking the Knowledge Memory for Lifelong Model Editing of
  Large Language Models <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14768v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14768v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Wang, Zexi Li, Ningyu Zhang, Ziwen Xu, Yunzhi Yao, Yong Jiang, Pengjun Xie, Fei Huang, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) need knowledge updates to meet the ever-growing
world facts and correct the hallucinated responses, facilitating the methods of
lifelong model editing. Where the updated knowledge resides in memories is a
fundamental question for model editing. In this paper, we find that editing
either long-term memory (direct model parameters) or working memory
(non-parametric knowledge of neural network activations/representations by
retrieval) will result in an impossible triangle -- reliability,
generalization, and locality can not be realized together in the lifelong
editing settings. For long-term memory, directly editing the parameters will
cause conflicts with irrelevant pretrained knowledge or previous edits (poor
reliability and locality). For working memory, retrieval-based activations can
hardly make the model understand the edits and generalize (poor
generalization). Therefore, we propose WISE to bridge the gap between memories.
In WISE, we design a dual parametric memory scheme, which consists of the main
memory for the pretrained knowledge and a side memory for the edited knowledge.
We only edit the knowledge in the side memory and train a router to decide
which memory to go through when given a query. For continual editing, we devise
a knowledge-sharding mechanism where different sets of edits reside in distinct
subspaces of parameters, and are subsequently merged into a shared memory
without conflicts. Extensive experiments show that WISE can outperform previous
model editing methods and overcome the impossible triangle under lifelong model
editing of question answering, hallucination, and out-of-distribution settings
across trending LLM architectures, e.g., GPT, LLaMA, and Mistral. Code is
available at https://github.com/zjunlp/EasyEdit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Circuits in <span class="highlight-title">Pretrain</span>ed <span class="highlight-title">Transformer</span>s <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17969v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17969v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunzhi Yao, Ningyu Zhang, Zekun Xi, Mengru Wang, Ziwen Xu, Shumin Deng, Huajun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable capabilities of modern large language models are rooted in
their vast repositories of knowledge encoded within their parameters, enabling
them to perceive the world and engage in reasoning. The inner workings of how
these models store knowledge have long been a subject of intense interest and
investigation among researchers. To date, most studies have concentrated on
isolated components within these models, such as the Multilayer Perceptrons and
attention head. In this paper, we delve into the computation graph of the
language model to uncover the knowledge circuits that are instrumental in
articulating specific knowledge. The experiments, conducted with GPT2 and
TinyLLAMA, have allowed us to observe how certain information heads, relation
heads, and Multilayer Perceptrons collaboratively encode knowledge within the
model. Moreover, we evaluate the impact of current knowledge editing techniques
on these knowledge circuits, providing deeper insights into the functioning and
constraints of these editing methodologies. Finally, we utilize knowledge
circuits to analyze and interpret language model behaviors such as
hallucinations and in-context learning. We believe the knowledge circuits hold
potential for advancing our understanding of Transformers and guiding the
improved design of knowledge editing. Code and data are available in
https://github.com/zjunlp/KnowledgeCircuits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024, 26 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling 4D Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        João Carreira, Dilara Gokay, Michael King, Chuhan Zhang, Ignacio Rocco, Aravindh Mahendran, Thomas Albert Keck, Joseph Heyward, Skanda Koppula, Etienne Pot, Goker Erdogan, Yana Hasson, Yi Yang, Klaus Greff, Guillaume Le Moing, Sjoerd van Steenkiste, Daniel Zoran, Drew A. Hudson, Pedro Vélez, Luisa Polanía, Luke Friedman, Chris Duvarney, Ross Goroshin, Kelsey Allen, Jacob Walker, Rishabh Kabra, Eric Aboussouan, Jennifer Sun, Thomas Kipf, Carl Doersch, Viorica Pătrăucean, Dima Damen, Pauline Luc, Mehdi S. M. Sajjadi, Andrew Zisserman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling has not yet been convincingly demonstrated for pure self-supervised
learning from video. However, prior work has focused evaluations on
semantic-related tasks $\unicode{x2013}$ action classification, ImageNet
classification, etc. In this paper we focus on evaluating self-supervised
learning on non-semantic vision tasks that are more spatial (3D) and temporal
(+1D = 4D), such as camera pose estimation, point and object tracking, and
depth estimation. We show that by learning from very large video datasets,
masked auto-encoding (MAE) with transformer video models actually scales,
consistently improving performance on these 4D tasks, as model size increases
from 20M all the way to the largest by far reported self-supervised video model
$\unicode{x2013}$ 22B parameters. Rigorous apples-to-apples comparison with
many recent image and video models demonstrates the benefits of scaling 4D
representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PRIMA: Multi-Image Vision-Language Models for Reasoning Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15209v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15209v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muntasir Wahed, Kiet A. Nguyen, Adheesh Sunil Juvekar, Xinzhuo Li, Xiaona Zhou, Vedant Shah, Tianjiao Yu, Pinar Yanardag, Ismini Lourentzou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant advancements in Large Vision-Language Models (LVLMs),
existing pixel-grounding models operate on single-image settings, limiting
their ability to perform detailed, fine-grained comparisons across multiple
images. Conversely, current multi-image understanding models lack pixel-level
grounding. Our work addresses this gap by introducing the task of multi-image
pixel-grounded reasoning segmentation, and PRIMA, a novel LVLM that integrates
pixel-level grounding with robust multi-image reasoning capabilities to produce
contextually rich, pixel-grounded explanations. Central to PRIMA is an
efficient vision module that queries fine-grained visual representations across
multiple images, reducing TFLOPs by $25.3\%$. To support training and
evaluation, we curate $M^4Seg$, a new reasoning segmentation benchmark
consisting of $\sim$224K question-answer pairs that require fine-grained visual
understanding across multiple images. Experimental results demonstrate PRIMA
outperforms state-of-the-art baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://plan-lab.github.io/prima</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenEMMA: Open-Source Multimodal Model for End-to-End Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Xing, Chengyuan Qian, Yuping Wang, Hongyuan Hua, Kexin Tian, Yang Zhou, Zhengzhong Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the advent of Multimodal Large Language Models (MLLMs), they have made
a significant impact across a wide range of real-world applications,
particularly in Autonomous Driving (AD). Their ability to process complex
visual data and reason about intricate driving scenarios has paved the way for
a new paradigm in end-to-end AD systems. However, the progress of developing
end-to-end models for AD has been slow, as existing fine-tuning methods demand
substantial resources, including extensive computational power, large-scale
datasets, and significant funding. Drawing inspiration from recent advancements
in inference computing, we propose OpenEMMA, an open-source end-to-end
framework based on MLLMs. By incorporating the Chain-of-Thought reasoning
process, OpenEMMA achieves significant improvements compared to the baseline
when leveraging a diverse range of MLLMs. Furthermore, OpenEMMA demonstrates
effectiveness, generalizability, and robustness across a variety of challenging
driving scenarios, offering a more efficient and effective approach to
autonomous driving. We release all the codes in
https://github.com/taco-group/OpenEMMA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoTrust: Benchmarking Trustworthiness in Large Vision Language Models
  for Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15206v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15206v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Xing, Hongyuan Hua, Xiangbo Gao, Shenzhe Zhu, Renjie Li, Kexin Tian, Xiaopeng Li, Heng Huang, Tianbao Yang, Zhangyang Wang, Yang Zhou, Huaxiu Yao, Zhengzhong Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large vision language models (VLMs) tailored for
autonomous driving (AD) have shown strong scene understanding and reasoning
capabilities, making them undeniable candidates for end-to-end driving systems.
However, limited work exists on studying the trustworthiness of DriveVLMs -- a
critical factor that directly impacts public transportation safety. In this
paper, we introduce AutoTrust, a comprehensive trustworthiness benchmark for
large vision-language models in autonomous driving (DriveVLMs), considering
diverse perspectives -- including trustfulness, safety, robustness, privacy,
and fairness. We constructed the largest visual question-answering dataset for
investigating trustworthiness issues in driving scenarios, comprising over 10k
unique scenes and 18k queries. We evaluated six publicly available VLMs,
spanning from generalist to specialist, from open-source to commercial models.
Our exhaustive evaluations have unveiled previously undiscovered
vulnerabilities of DriveVLMs to trustworthiness threats. Specifically, we found
that the general VLMs like LLaVA-v1.6 and GPT-4o-mini surprisingly outperform
specialized models fine-tuned for driving in terms of overall trustworthiness.
DriveVLMs like DriveLM-Agent are particularly vulnerable to disclosing
sensitive information. Additionally, both generalist and specialist VLMs remain
susceptible to adversarial attacks and struggle to ensure unbiased
decision-making across diverse environments and populations. Our findings call
for immediate and decisive action to address the trustworthiness of DriveVLMs
-- an issue of critical importance to public safety and the welfare of all
citizens relying on autonomous transportation systems. Our benchmark is
publicly available at \url{https://github.com/taco-group/AutoTrust}, and the
leaderboard is released at \url{https://taco-group.github.io/AutoTrust/}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>55 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LiDAR-RT: Gaussian-based Ray Tracing for Dynamic LiDAR Re-simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxu Zhou, Lvchang Fu, Sida Peng, Yunzhi Yan, Zhanhua Zhang, Yong Chen, Jiazhi Xia, Xiaowei Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper targets the challenge of real-time LiDAR re-simulation in dynamic
driving scenarios. Recent approaches utilize neural radiance fields combined
with the physical modeling of LiDAR sensors to achieve high-fidelity
re-simulation results. Unfortunately, these methods face limitations due to
high computational demands in large-scale scenes and cannot perform real-time
LiDAR rendering. To overcome these constraints, we propose LiDAR-RT, a novel
framework that supports real-time, physically accurate LiDAR re-simulation for
driving scenes. Our primary contribution is the development of an efficient and
effective rendering pipeline, which integrates Gaussian primitives and
hardware-accelerated ray tracing technology. Specifically, we model the
physical properties of LiDAR sensors using Gaussian primitives with learnable
parameters and incorporate scene graphs to handle scene dynamics. Building upon
this scene representation, our framework first constructs a bounding volume
hierarchy (BVH), then casts rays for each pixel and generates novel LiDAR views
through a differentiable rendering algorithm. Importantly, our framework
supports realistic rendering with flexible scene editing operations and various
sensor configurations. Extensive experiments across multiple public benchmarks
demonstrate that our method outperforms state-of-the-art methods in terms of
rendering quality and efficiency. Our project page is at
https://zju3dv.github.io/lidar-rt.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://zju3dv.github.io/lidar-rt</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Preventing Local Pitfalls in Vector Quantization via Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Borui Zhang, Wenzhao Zheng, Jie Zhou, Jiwen Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vector-quantized networks (VQNs) have exhibited remarkable performance across
various tasks, yet they are prone to training instability, which complicates
the training process due to the necessity for techniques such as subtle
initialization and model distillation. In this study, we identify the local
minima issue as the primary cause of this instability. To address this, we
integrate an optimal transport method in place of the nearest neighbor search
to achieve a more globally informed assignment. We introduce OptVQ, a novel
vector quantization method that employs the Sinkhorn algorithm to optimize the
optimal transport problem, thereby enhancing the stability and efficiency of
the training process. To mitigate the influence of diverse data distributions
on the Sinkhorn algorithm, we implement a straightforward yet effective
normalization strategy. Our comprehensive experiments on image reconstruction
tasks demonstrate that OptVQ achieves 100% codebook utilization and surpasses
current state-of-the-art VQNs in reconstruction quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/zbr17/OptVQ</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AV-Link: Temporally-Aligned Diffusion Features for Cross-Modal
  Audio-Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15191v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15191v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moayed Haji-Ali, Willi Menapace, Aliaksandr Siarohin, Ivan Skorokhodov, Alper Canberk, Kwot Sin Lee, Vicente Ordonez, Sergey Tulyakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose AV-Link, a unified framework for Video-to-Audio and Audio-to-Video
generation that leverages the activations of frozen video and audio diffusion
models for temporally-aligned cross-modal conditioning. The key to our
framework is a Fusion Block that enables bidirectional information exchange
between our backbone video and audio diffusion models through a
temporally-aligned self attention operation. Unlike prior work that uses
feature extractors pretrained for other tasks for the conditioning signal,
AV-Link can directly leverage features obtained by the complementary modality
in a single framework i.e. video features to generate audio, or audio features
to generate video. We extensively evaluate our design choices and demonstrate
the ability of our method to achieve synchronized and high-quality audiovisual
content, showcasing its potential for applications in immersive media
generation. Project Page: snap-research.github.io/AVLink/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: snap-research.github.io/AVLink/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LlamaFusion: Adapting <span class="highlight-title">Pretrain</span>ed Language Models for Multimodal
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijia Shi, Xiaochuang Han, Chunting Zhou, Weixin Liang, Xi Victoria Lin, Luke Zettlemoyer, Lili Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LlamaFusion, a framework for empowering pretrained text-only large
language models (LLMs) with multimodal generative capabilities, enabling them
to understand and generate both text and images in arbitrary sequences.
LlamaFusion leverages existing Llama-3's weights for processing texts
autoregressively while introducing additional and parallel transformer modules
for processing images with diffusion. During training, the data from each
modality is routed to its dedicated modules: modality-specific feedforward
layers, query-key-value projections, and normalization layers process each
modality independently, while the shared self-attention layers allow
interactions across text and image features. By freezing the text-specific
modules and only training the image-specific modules, LlamaFusion preserves the
language capabilities of text-only LLMs while developing strong visual
understanding and generation abilities. Compared to methods that pretrain
multimodal generative models from scratch, our experiments demonstrate that,
LlamaFusion improves image understanding by 20% and image generation by 3.6%
using only 50% of the FLOPs while maintaining Llama-3's language capabilities.
We also demonstrate that this framework can adapt existing vision-language
models with multimodal generation ability. Overall, this framework not only
leverages existing computational investments in text-only LLMs but also enables
the parallel development of language and vision capabilities, presenting a
promising direction for efficient multimodal model development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data for Mathematical Copilots: Better Ways of Presenting Proofs for
  Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Frieder, Jonas Bayer, Katherine M. Collins, Julius Berner, Jacob Loader, András Juhász, Fabian Ruehle, Sean Welleck, Gabriel Poesia, Ryan-Rhys Griffiths, Adrian Weller, Anirudh Goyal, Thomas Lukasiewicz, Timothy Gowers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The suite of datasets commonly used to train and evaluate the mathematical
capabilities of AI-based mathematical copilots (primarily large language
models) exhibit several shortcomings. These limitations include a restricted
scope of mathematical complexity, typically not exceeding lower
undergraduate-level mathematics, binary rating protocols and other issues,
which makes comprehensive proof-based evaluation suites difficult. We
systematically explore these limitations and contend that enhancing the
capabilities of large language models, or any forthcoming advancements in
AI-based mathematical assistants (copilots or "thought partners"), necessitates
a paradigm shift in the design of mathematical datasets and the evaluation
criteria of mathematical ability: It is necessary to move away from
result-based datasets (theorem statement to theorem proof) and convert the rich
facets of mathematical research practice to data LLMs can train on. Examples of
these are mathematical workflows (sequences of atomic, potentially
subfield-dependent tasks that are often performed when creating new
mathematics), which are an important part of the proof-discovery process.
Additionally, we advocate for mathematical dataset developers to consider the
concept of "motivated proof", introduced by G. P\'olya in 1949, which can serve
as a blueprint for datasets that offer a better proof learning signal,
alleviating some of the mentioned limitations. Lastly, we introduce math
datasheets for datasets, extending the general, dataset-agnostic variants of
datasheets: We provide a questionnaire designed specifically for math datasets
that we urge dataset creators to include with their datasets. This will make
creators aware of potential limitations of their datasets while at the same
time making it easy for readers to assess it from the point of view of training
and evaluating mathematical copilots.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STRAP: Robot Sub-Trajectory Retrieval for Augmented Policy Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15182v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15182v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marius Memmel, Jacob Berg, Bingqing Chen, Abhishek Gupta, Jonathan Francis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot learning is witnessing a significant increase in the size, diversity,
and complexity of pre-collected datasets, mirroring trends in domains such as
natural language processing and computer vision. Many robot learning methods
treat such datasets as multi-task expert data and learn a multi-task,
generalist policy by training broadly across them. Notably, while these
generalist policies can improve the average performance across many tasks, the
performance of generalist policies on any one task is often suboptimal due to
negative transfer between partitions of the data, compared to task-specific
specialist policies. In this work, we argue for the paradigm of training
policies during deployment given the scenarios they encounter: rather than
deploying pre-trained policies to unseen problems in a zero-shot manner, we
non-parametrically retrieve and train models directly on relevant data at test
time. Furthermore, we show that many robotics tasks share considerable amounts
of low-level behaviors and that retrieval at the "sub"-trajectory granularity
enables significantly improved data utilization, generalization, and robustness
in adapting policies to novel problems. In contrast, existing full-trajectory
retrieval methods tend to underutilize the data and miss out on shared
cross-task content. This work proposes STRAP, a technique for leveraging
pre-trained vision foundation models and dynamic time warping to retrieve
sub-sequences of trajectories from large training corpora in a robust fashion.
STRAP outperforms both prior retrieval algorithms and multi-task learning
methods in simulated and real experiments, showing the ability to scale to much
larger offline datasets in the real world as well as the ability to learn
robust control policies with just a handful of real-world demonstrations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website at https://weirdlabuw.github.io/strap/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HPC-Coder-V2: Studying Code LLMs Across Low-Resource Parallel Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aman Chaturvedi, Daniel Nichols, Siddharth Singh, Abhinav Bhatele
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM) based coding tools have been tremendously
successful as software development assistants, yet they are often designed for
general purpose programming tasks and perform poorly for more specialized
domains such as high performance computing. Creating specialized models and
tools for these domains is crucial towards gaining the benefits of LLMs in
areas such as HPC. While previous work has explored HPC-specific models, LLMs
still struggle to generate parallel code and it is not at all clear what
hurdles are still holding back these LLMs and what must be done to overcome
them. In this work, we conduct an in-depth study along the many axes of
fine-tuning a specialized HPC LLM in order to better understand the challenges.
Based on our findings we fine-tune and evaluate a specialized HPC LLM that is
shown to be the best performing open-source code LLM for parallel code
generation to date.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Uncertainty Estimation in Natural Language Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Aichberger, Kajetan Schweighofer, Sepp Hochreiter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are increasingly employed in real-world
applications, driving the need to evaluate the trustworthiness of their
generated text. To this end, reliable uncertainty estimation is essential.
Since current LLMs generate text autoregressively through a stochastic process,
the same prompt can lead to varying outputs. Consequently, leading uncertainty
estimation methods generate and analyze multiple output sequences to determine
the LLM's uncertainty. However, generating output sequences is computationally
expensive, making these methods impractical at scale. In this work, we inspect
the theoretical foundations of the leading methods and explore new directions
to enhance their computational efficiency. Building on the framework of proper
scoring rules, we find that the negative log-likelihood of the most likely
output sequence constitutes a theoretically grounded uncertainty measure. To
approximate this alternative measure, we propose G-NLL, which has the advantage
of being obtained using only a single output sequence generated by greedy
decoding. This makes uncertainty estimation more efficient and straightforward,
while preserving theoretical rigor. Empirical results demonstrate that G-NLL
achieves state-of-the-art performance across various LLMs and tasks. Our work
lays the foundation for efficient and reliable uncertainty estimation in
natural language generation, challenging the necessity of more computationally
involved methods currently leading the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Operationalising Rawlsian Ethics for Fairness in Norm-Learning Agents <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jessica Woodgate, Paul Marshall, Nirav Ajmeri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social norms are standards of behaviour common in a society. However, when
agents make decisions without considering how others are impacted, norms can
emerge that lead to the subjugation of certain agents. We present RAWL-E, a
method to create ethical norm-learning agents. RAWL-E agents operationalise
maximin, a fairness principle from Rawlsian ethics, in their decision-making
processes to promote ethical norms by balancing societal well-being with
individual goals. We evaluate RAWL-E agents in simulated harvesting scenarios.
We find that norms emerging in RAWL-E agent societies enhance social welfare,
fairness, and robustness, and yield higher minimum experience compared to those
that emerge in agent societies that do not implement Rawlsian ethics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures, 8 tables (and supplementary material with
  reproducibility and additional results), accepted at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Color Channel Independence for Improved Unsupervised Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bastian Jäckl, Yannick Metz, Udo Schlegel, Daniel A. Keim, Maximilian T. Fischer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-centric architectures can learn to extract distinct object
representations from visual scenes, enabling downstream applications on the
object level. Similarly to autoencoder-based image models, object-centric
approaches have been trained on the unsupervised reconstruction loss of images
encoded by RGB color spaces. In our work, we challenge the common assumption
that RGB images are the optimal color space for unsupervised learning in
computer vision. We discuss conceptually and empirically that other color
spaces, such as HSV, bear essential characteristics for object-centric
representation learning, like robustness to lighting conditions. We further
show that models improve when requiring them to predict additional color
channels. Specifically, we propose to transform the predicted targets to the
RGB-S space, which extends RGB with HSV's saturation component and leads to
markedly better reconstruction and disentanglement for five common evaluation
datasets. The use of composite color spaces can be implemented with basically
no computational overhead, is agnostic of the models' architecture, and is
universally applicable across a wide range of visual computing tasks and
training types. The findings of our approach encourage additional
investigations in computer vision tasks beyond object-centric learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages incl. references, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Jet: A Modern <span class="highlight-title">Transformer</span>-Based Normalizing Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Kolesnikov, André Susano Pinto, Michael Tschannen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past, normalizing generative flows have emerged as a promising class
of generative models for natural images. This type of model has many modeling
advantages: the ability to efficiently compute log-likelihood of the input
data, fast generation and simple overall structure. Normalizing flows remained
a topic of active research but later fell out of favor, as visual quality of
the samples was not competitive with other model classes, such as GANs,
VQ-VAE-based approaches or diffusion models. In this paper we revisit the
design of the coupling-based normalizing flow models by carefully ablating
prior design choices and using computational blocks based on the Vision
Transformer architecture, not convolutional neural networks. As a result, we
achieve state-of-the-art quantitative and qualitative performance with a much
simpler architecture. While the overall visual quality is still behind the
current state-of-the-art models, we argue that strong normalizing flow models
can help advancing research frontier by serving as building components of more
powerful generative models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Pruning for Large Language Models with Structural Importance
  Awareness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotian Zheng, Jinke Ren, Yushan Sun, Ruichen Zhang, Wenbo Zhang, Zhen Li, Dusit Niyato, Shuguang Cui, Yatong Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advancements in large language models (LLMs) have significantly
improved language understanding and generation capabilities. However, it is
difficult to deploy LLMs on resource-constrained edge devices due to their high
computational and storage resource demands. To address this issue, we propose a
novel LLM model pruning method, namely structurally-aware adaptive pruning
(SAAP), to significantly reduce the computational and memory costs while
maintaining model performance. We first define an adaptive importance fusion
metric to evaluate the importance of all coupled structures in LLMs by
considering their homoscedastic uncertainty. Then, we rank the importance of
all modules to determine the specific layers that should be pruned to meet
particular performance requirements. Furthermore, we develop a new group
fine-tuning strategy to improve the inference efficiency of LLMs. Finally, we
evaluate the proposed SAAP method on multiple LLMs across two common tasks,
i.e., zero-shot classification and text generation. Experimental results show
that our SAAP method outperforms several state-of-the-art baseline methods,
achieving 2.17%, 2.37%, and 2.39% accuracy gains on LLaMA-7B, Vicuna-7B, and
LLaMA-13B. Additionally, SAAP improves the token generation speed by 5%,
showcasing its practical advantages in resource-constrained scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Outcome-Refining Process Supervision for Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15118v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15118v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuohao Yu, Weizheng Gu, Yidong Wang, Zhengran Zeng, Jindong Wang, Wei Ye, Shikun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models have demonstrated remarkable capabilities in code
generation, yet they often struggle with complex programming tasks that require
deep algorithmic reasoning. While process supervision through learned reward
models shows promise in guiding reasoning steps, it requires expensive training
data and suffers from unreliable evaluation. We propose Outcome-Refining
Process Supervision, a novel paradigm that treats outcome refinement itself as
the process to be supervised. Our framework leverages concrete execution
signals to ground the supervision of reasoning steps, while using
tree-structured exploration to maintain multiple solution trajectories
simultaneously. Experiments demonstrate that our approach enables even smaller
models to achieve high success accuracy and performance metrics on competitive
programming tasks, creates more reliable verification than traditional reward
models without requiring training PRMs. Our approach achieves significant
improvements across 5 models and 3 datasets: an average of 26.9% increase in
correctness and 42.2% in efficiency. The results suggest that providing
structured reasoning space with concrete verification signals is crucial for
solving complex programming tasks. We open-source all our code and data at:
https://github.com/zhuohaoyu/ORPS
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 5 figures, Code: https://github.com/zhuohaoyu/ORPS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tests for model misspecification in simulation-based inference: from
  local distortions to global model checks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noemi Anau Montel, James Alvey, Christoph Weniger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model misspecification analysis strategies, such as anomaly detection, model
validation, and model comparison are a key component of scientific model
development. Over the last few years, there has been a rapid rise in the use of
simulation-based inference (SBI) techniques for Bayesian parameter estimation,
applied to increasingly complex forward models. To move towards fully
simulation-based analysis pipelines, however, there is an urgent need for a
comprehensive simulation-based framework for model misspecification analysis.
In this work, we provide a solid and flexible foundation for a wide range of
model discrepancy analysis tasks, using distortion-driven model
misspecification tests. From a theoretical perspective, we introduce the
statistical framework built around performing many hypothesis tests for
distortions of the simulation model. We also make explicit analytic connections
to classical techniques: anomaly detection, model validation, and
goodness-of-fit residual analysis. Furthermore, we introduce an efficient
self-calibrating training algorithm that is useful for practitioners. We
demonstrate the performance of the framework in multiple scenarios, making the
connection to classical results where they are valid. Finally, we show how to
conduct such a distortion-driven model misspecification test for real
gravitational wave data, specifically on the event GW150914.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures. Code available on github (NoemiAM/mist) at
  https://github.com/NoemiAM/mist</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Full <span class="highlight-title">Transformer</span>-based Framework for Automatic Pain Estimation using
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15095v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15095v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefanos Gkikas, Manolis Tsiknakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automatic estimation of pain is essential in designing an optimal pain
management system offering reliable assessment and reducing the suffering of
patients. In this study, we present a novel full transformer-based framework
consisting of a Transformer in Transformer (TNT) model and a Transformer
leveraging cross-attention and self-attention blocks. Elaborating on videos
from the BioVid database, we demonstrate state-of-the-art performances, showing
the efficacy, efficiency, and generalization capability across all the primary
pain estimation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Disentangled Equivariant Representation for Explicitly
  Controllable 3D Molecule Generation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Liu, Youzhi Luo, Tianxiao Li, James Caverlee, Martin Renqiang Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the conditional generation of 3D drug-like molecules with
\textit{explicit control} over molecular properties such as drug-like
properties (e.g., Quantitative Estimate of Druglikeness or Synthetic
Accessibility score) and effectively binding to specific protein sites. To
tackle this problem, we propose an E(3)-equivariant Wasserstein autoencoder and
factorize the latent space of our generative model into two disentangled
aspects: molecular properties and the remaining structural context of 3D
molecules. Our model ensures explicit control over these molecular attributes
while maintaining equivariance of coordinate representation and invariance of
data likelihood. Furthermore, we introduce a novel alignment-based coordinate
loss to adapt equivariant networks for auto-regressive de-novo 3D molecule
generation from scratch. Extensive experiments validate our model's
effectiveness on property-guided and context-guided molecule generation, both
for de-novo 3D molecule design and structure-based drug discovery against
protein targets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AceMath: Advancing Frontier Math Reasoning with Post-Training and Reward
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, Wei Ping
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce AceMath, a suite of frontier math models that
excel in solving complex math problems, along with highly effective reward
models capable of evaluating generated solutions and reliably identifying the
correct ones. To develop the instruction-tuned math models, we propose a
supervised fine-tuning (SFT) process that first achieves competitive
performance across general domains, followed by targeted fine-tuning for the
math domain using a carefully curated set of prompts and synthetically
generated responses. The resulting model, AceMath-72B-Instruct greatly
outperforms Qwen2.5-Math-72B-Instruct, GPT-4o and Claude-3.5 Sonnet. To develop
math-specialized reward model, we first construct AceMath-RewardBench, a
comprehensive and robust benchmark for evaluating math reward models across
diverse problems and difficulty levels. After that, we present a systematic
approach to build our math reward models. The resulting model, AceMath-72B-RM,
consistently outperforms state-of-the-art reward models. Furthermore, when
combining AceMath-72B-Instruct with AceMath-72B-RM, we achieve the highest
average rm@8 score across the math reasoning benchmarks. We will release model
weights, training data, and evaluation benchmarks at:
https://research.nvidia.com/labs/adlr/acemath
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Till the Layers Collapse: Compressing a Deep Neural Network through the
  Lenses of Batch Normalization Layers <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhu Liao, Nour Hezbri, Victor Quétu, Van-Tam Nguyen, Enzo Tartaglione
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Today, deep neural networks are widely used since they can handle a variety
of complex tasks. Their generality makes them very powerful tools in modern
technology. However, deep neural networks are often overparameterized. The
usage of these large models consumes a lot of computation resources. In this
paper, we introduce a method called \textbf{T}ill the \textbf{L}ayers
\textbf{C}ollapse (TLC), which compresses deep neural networks through the
lenses of batch normalization layers. By reducing the depth of these networks,
our method decreases deep neural networks' computational requirements and
overall latency. We validate our method on popular models such as Swin-T,
MobileNet-V2, and RoBERTa, across both image classification and natural
language processing (NLP) tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DroughtSet: Understanding Drought Through Spatial-Temporal Learning <span class="chip">AAAI25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuwei Tan, Qian Zhao, Yanlan Liu, Xueru Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drought is one of the most destructive and expensive natural disasters,
severely impacting natural resources and risks by depleting water resources and
diminishing agricultural yields. Under climate change, accurately predicting
drought is critical for mitigating drought-induced risks. However, the
intricate interplay among the physical and biological drivers that regulate
droughts limits the predictability and understanding of drought, particularly
at a subseasonal to seasonal (S2S) time scale. While deep learning has been
demonstrated with potential in addressing climate forecasting challenges, its
application to drought prediction has received relatively less attention. In
this work, we propose a new dataset, DroughtSet, which integrates relevant
predictive features and three drought indices from multiple remote sensing and
reanalysis datasets across the contiguous United States (CONUS). DroughtSet
specifically provides the machine learning community with a new real-world
dataset to benchmark drought prediction models and more generally, time-series
forecasting methods. Furthermore, we propose a spatial-temporal model SPDrought
to predict and interpret S2S droughts. Our model learns from the spatial and
temporal information of physical and biological features to predict three types
of droughts simultaneously. Multiple strategies are employed to quantify the
importance of physical and biological features for drought prediction. Our
results provide insights for researchers to better understand the
predictability and sensitivity of drought to biological and physical
conditions. We aim to contribute to the climate field by proposing a new tool
to predict and understand the occurrence of droughts and provide the AI
community with a new benchmark to study deep learning applications in climate
science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiverSeg: Scalable Interactive Segmentation of Biomedical Imaging
  <span class="highlight-title">Dataset</span>s with In-Context Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hallee E. Wong, Jose Javier Gonzalez Ortiz, John Guttag, Adrian V. Dalca
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical researchers and clinicians often need to perform novel segmentation
tasks on a set of related images. Existing methods for segmenting a new dataset
are either interactive, requiring substantial human effort for each image, or
require an existing set of manually labeled images. We introduce a system,
MultiverSeg, that enables practitioners to rapidly segment an entire new
dataset without requiring access to any existing labeled data from that task or
domain. Along with the image to segment, the model takes user interactions such
as clicks, bounding boxes or scribbles as input, and predicts a segmentation.
As the user segments more images, those images and segmentations become
additional inputs to the model, providing context. As the context set of
labeled images grows, the number of interactions required to segment each new
image decreases. We demonstrate that MultiverSeg enables users to interactively
segment new datasets efficiently, by amortizing the number of interactions per
image to achieve an accurate segmentation. Compared to using a state-of-the-art
interactive segmentation method, using MultiverSeg reduced the total number of
scribble steps by 53% and clicks by 36% to achieve 90% Dice on sets of images
from unseen tasks. We release code and model weights at
https://multiverseg.csail.mit.edu
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Website: https://multiverseg.csail.mit.edu Keywords:
  interactive segmentation, in-context learning, medical image analysis,
  biomedical imaging, image annotation, visual prompting</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DCTdiff: Intriguing Properties of Image Generative Modeling in the DCT
  Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mang Ning, Mingxiao Li, Jianlin Su, Haozhe Jia, Lanmiao Liu, Martin Beneš, Albert Ali Salah, Itir Onal Ertugrul
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores image modeling from the frequency space and introduces
DCTdiff, an end-to-end diffusion generative paradigm that efficiently models
images in the discrete cosine transform (DCT) space. We investigate the design
space of DCTdiff and reveal the key design factors. Experiments on different
frameworks (UViT, DiT), generation tasks, and various diffusion samplers
demonstrate that DCTdiff outperforms pixel-based diffusion models regarding
generative quality and training efficiency. Remarkably, DCTdiff can seamlessly
scale up to high-resolution generation without using the latent diffusion
paradigm. Finally, we illustrate several intriguing properties of DCT image
modeling. For example, we provide a theoretical proof of why `image diffusion
can be seen as spectral autoregression', bridging the gap between diffusion and
autoregressive models. The effectiveness of DCTdiff and the introduced
properties suggest a promising direction for image modeling in the frequency
space. The code is at \url{https://github.com/forever208/DCTdiff}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stable-V2A: Synthesis of Synchronized Sound Effects with Temporal and
  Semantic Controls 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo Fosco Gramaccioni, Christian Marinoni, Emilian Postolache, Marco Comunità, Luca Cosmo, Joshua D. Reiss, Danilo Comminiello
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sound designers and Foley artists usually sonorize a scene, such as from a
movie or video game, by manually annotating and sonorizing each action of
interest in the video. In our case, the intent is to leave full creative
control to sound designers with a tool that allows them to bypass the more
repetitive parts of their work, thus being able to focus on the creative
aspects of sound production. We achieve this presenting Stable-V2A, a two-stage
model consisting of: an RMS-Mapper that estimates an envelope representative of
the audio characteristics associated with the input video; and Stable-Foley, a
diffusion model based on Stable Audio Open that generates audio semantically
and temporally aligned with the target video. Temporal alignment is guaranteed
by the use of the envelope as a ControlNet input, while semantic alignment is
achieved through the use of sound representations chosen by the designer as
cross-attention conditioning of the diffusion process. We train and test our
model on Greatest Hits, a dataset commonly used to evaluate V2A models. In
addition, to test our model on a case study of interest, we introduce Walking
The Maps, a dataset of videos extracted from video games depicting animated
characters walking in different locations. Samples and code available on our
demo page at https://ispamm.github.io/Stable-V2A.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Federated Learning in the Face of Covariate Shift: A Magnitude
  Pruning with Hybrid Regularization Framework for Enhanced Model Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ozgu Goksu, Nicolas Pugeault
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of highly sophisticated neural networks has allowed for fast
progress in every field of computer vision, however, applications where
annotated data is prohibited due to privacy or security concerns remain
challenging. Federated Learning (FL) offers a promising framework for
individuals aiming to collaboratively develop a shared model while preserving
data privacy. Nevertheless, our findings reveal that variations in data
distribution among clients can profoundly affect FL methodologies, primarily
due to instabilities in the aggregation process. We also propose a novel FL
framework to mitigate the adverse effects of covariate shifts among federated
clients by combining individual parameter pruning and regularization techniques
to improve the robustness of individual clients' models to aggregate. Each
client's model is optimized through magnitude-based pruning and the addition of
dropout and noise injection layers to build more resilient decision pathways in
the networks and improve the robustness of the model's parameter aggregation
step. The proposed framework is capable of extracting robust representations
even in the presence of very large covariate shifts among client data
distributions and in the federation of a small number of clients. Empirical
findings substantiate the effectiveness of our proposed methodology across
common benchmark datasets, including CIFAR10, MNIST, SVHN, and Fashion MNIST.
Furthermore, we introduce the CelebA-Gender dataset, specifically designed to
evaluate performance on a more realistic domain. The proposed method is capable
of extracting robust representations even in the presence of both high and low
covariate shifts among client data distributions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DisCo: Graph-Based Disentangled Contrastive Learning for Cold-Start
  Cross-Domain Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15005v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15005v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hourun Li, Yifan Wang, Zhiping Xiao, Jia Yang, Changling Zhou, Ming Zhang, Wei Ju
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are widely used in various real-world applications, but
they often encounter the persistent challenge of the user cold-start problem.
Cross-domain recommendation (CDR), which leverages user interactions from one
domain to improve prediction performance in another, has emerged as a promising
solution. However, users with similar preferences in the source domain may
exhibit different interests in the target domain. Therefore, directly
transferring embeddings may introduce irrelevant source-domain collaborative
information. In this paper, we propose a novel graph-based disentangled
contrastive learning framework to capture fine-grained user intent and filter
out irrelevant collaborative information, thereby avoiding negative transfer.
Specifically, for each domain, we use a multi-channel graph encoder to capture
diverse user intents. We then construct the affinity graph in the embedding
space and perform multi-step random walks to capture high-order user similarity
relationships. Treating one domain as the target, we propose a disentangled
intent-wise contrastive learning approach, guided by user similarity, to refine
the bridging of user intents across domains. Extensive experiments on four
benchmark CDR datasets demonstrate that DisCo consistently outperforms existing
state-of-the-art baselines, thereby validating the effectiveness of both DisCo
and its components.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stitch Contrast and Segment_Learning a Human Action Segmentation Model
  Using Trimmed Skeleton Videos <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14988v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14988v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitao Tian, Pierre Payeur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing skeleton-based human action classification models rely on
well-trimmed action-specific skeleton videos for both training and testing,
precluding their scalability to real-world applications where untrimmed videos
exhibiting concatenated actions are predominant. To overcome this limitation,
recently introduced skeleton action segmentation models involve un-trimmed
skeleton videos into end-to-end training. The model is optimized to provide
frame-wise predictions for any length of testing videos, simultaneously
realizing action localization and classification. Yet, achieving such an
improvement im-poses frame-wise annotated skeleton videos, which remains
time-consuming in practice. This paper features a novel framework for
skeleton-based action segmentation trained on short trimmed skeleton videos,
but that can run on longer un-trimmed videos. The approach is implemented in
three steps: Stitch, Contrast, and Segment. First, Stitch proposes a tem-poral
skeleton stitching scheme that treats trimmed skeleton videos as elementary
human motions that compose a semantic space and can be sampled to generate
multi-action stitched se-quences. Contrast learns contrastive representations
from stitched sequences with a novel discrimination pretext task that enables a
skeleton encoder to learn meaningful action-temporal contexts to improve action
segmentation. Finally, Segment relates the proposed method to action
segmentation by learning a segmentation layer while handling particular da-ta
availability. Experiments involve a trimmed source dataset and an untrimmed
target dataset in an adaptation formulation for real-world skeleton-based human
action segmentation to evaluate the effectiveness of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Injection via <span class="highlight-title">Prompt</span> Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kalle Kujanpää, Harri Valpola, Alexander Ilin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many practical applications, large language models (LLMs) need to
incorporate new knowledge not present in their pre-training data. The primary
methods for this are fine-tuning and retrieval-augmented generation (RAG).
Although RAG has emerged as the industry standard for knowledge injection,
fine-tuning has not yet achieved comparable success. In this paper, we propose
a new fine-tuning technique for learning new knowledge and show that it can
reach the performance of RAG. The proposed method is based on the
self-distillation approach, which we call prompt distillation. First, we
generate question-answer pairs about the new knowledge. Then, we fine-tune a
student model on the question-answer pairs to imitate the output distributions
of a teacher model, which additionally receives the new knowledge in its
prompt. The student model is identical to the teacher, except it is equipped
with a LoRA adapter. This training procedure facilitates distilling the new
knowledge from the teacher's prompt into the student's weights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IDOL: Instant Photorealistic 3D Human Creation from a Single Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyu Zhuang, Jiaxi Lv, Hao Wen, Qing Shuai, Ailing Zeng, Hao Zhu, Shifeng Chen, Yujiu Yang, Xun Cao, Wei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating a high-fidelity, animatable 3D full-body avatar from a single image
is a challenging task due to the diverse appearance and poses of humans and the
limited availability of high-quality training data. To achieve fast and
high-quality human reconstruction, this work rethinks the task from the
perspectives of dataset, model, and representation. First, we introduce a
large-scale HUman-centric GEnerated dataset, HuGe100K, consisting of 100K
diverse, photorealistic sets of human images. Each set contains 24-view frames
in specific human poses, generated using a pose-controllable
image-to-multi-view model. Next, leveraging the diversity in views, poses, and
appearances within HuGe100K, we develop a scalable feed-forward transformer
model to predict a 3D human Gaussian representation in a uniform space from a
given human image. This model is trained to disentangle human pose, body shape,
clothing geometry, and texture. The estimated Gaussians can be animated without
post-processing. We conduct comprehensive experiments to validate the
effectiveness of the proposed dataset and method. Our model demonstrates the
ability to efficiently reconstruct photorealistic humans at 1K resolution from
a single input image using a single GPU instantly. Additionally, it seamlessly
supports various applications, as well as shape and texture editing tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 15 figures, includes main content, supplementary materials,
  and references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Corn Ear Detection and Orientation Estimation Using Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Sprague, John Evans, Michael Mardikes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monitoring growth behavior of maize plants such as the development of ears
can give key insights into the plant's health and development. Traditionally,
the measurement of the angle of ears is performed manually, which can be
time-consuming and prone to human error. To address these challenges, this
paper presents a computer vision-based system for detecting and tracking ears
of corn in an image sequence. The proposed system could accurately detect,
track, and predict the ear's orientation, which can be useful in monitoring
their growth behavior. This can significantly save time compared to manual
measurement and enables additional areas of ear orientation research and
potential increase in efficiencies for maize production. Using an object
detector with keypoint detection, the algorithm proposed could detect 90
percent of all ears. The cardinal estimation had a mean absolute error (MAE) of
18 degrees, compared to a mean 15 degree difference between two people
measuring by hand. These results demonstrate the feasibility of using computer
vision techniques for monitoring maize growth and can lead to further research
in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages;15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Point to probabilistic gradient boosting for claim frequency and
  severity prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominik Chevalier, Marie-Pier Côté
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gradient boosting for decision tree algorithms are increasingly used in
actuarial applications as they show superior predictive performance over
traditional generalized linear models. Many improvements and sophistications to
the first gradient boosting machine algorithm exist. We present in a unified
notation, and contrast, all the existing point and probabilistic gradient
boosting for decision tree algorithms: GBM, XGBoost, DART, LightGBM, CatBoost,
EGBM, PGBM, XGBoostLSS, cyclic GBM, and NGBoost. In this comprehensive
numerical study, we compare their performance on five publicly available
datasets for claim frequency and severity, of various size and comprising
different number of (high cardinality) categorical variables. We explain how
varying exposure-to-risk can be handled with boosting in frequency models. We
compare the algorithms on the basis of computational efficiency, predictive
performance, and model adequacy. LightGBM and XGBoostLSS win in terms of
computational efficiency. The fully interpretable EGBM achieves competitive
predictive performance compared to the black box algorithms considered. We find
that there is no trade-off between model adequacy and predictive accuracy: both
are achievable simultaneously.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 4 figures, 26 tables, 7 algorithms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion priors for Bayesian 3D reconstruction from incomplete
  measurements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian L. Möbius, Michael Habeck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many inverse problems are ill-posed and need to be complemented by prior
information that restricts the class of admissible models. Bayesian approaches
encode this information as prior distributions that impose generic properties
on the model such as sparsity, non-negativity or smoothness. However, in case
of complex structured models such as images, graphs or three-dimensional (3D)
objects,generic prior distributions tend to favor models that differ largely
from those observed in the real world. Here we explore the use of diffusion
models as priors that are combined with experimental data within a Bayesian
framework. We use 3D point clouds to represent 3D objects such as household
items or biomolecular complexes formed from proteins and nucleic acids. We
train diffusion models that generate coarse-grained 3D structures at a medium
resolution and integrate these with incomplete and noisy experimental data. To
demonstrate the power of our approach, we focus on the reconstruction of
biomolecular assemblies from cryo-electron microscopy (cryo-EM) images, which
is an important inverse problem in structural biology. We find that posterior
sampling with diffusion model priors allows for 3D reconstruction from very
sparse, low-resolution and partial observations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-Powered Intracranial Hemorrhage Detection: A Co-Scale Convolutional
  Attention Model with Uncertainty-Based Fuzzy Integral Operator and Feature
  Screening 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdi Hosseini Chagahi, Md. Jalil Piran, Niloufar Delfan, Behzad Moshiri, Jaber Hatam Parikhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intracranial hemorrhage (ICH) refers to the leakage or accumulation of blood
within the skull, which occurs due to the rupture of blood vessels in or around
the brain. If this condition is not diagnosed in a timely manner and
appropriately treated, it can lead to serious complications such as decreased
consciousness, permanent neurological disabilities, or even death.The primary
aim of this study is to detect the occurrence or non-occurrence of ICH,
followed by determining the type of subdural hemorrhage (SDH). These tasks are
framed as two separate binary classification problems. By adding two layers to
the co-scale convolutional attention (CCA) classifier architecture, we
introduce a novel approach for ICH detection. In the first layer, after
extracting features from different slices of computed tomography (CT) scan
images, we combine these features and select the 50 components that capture the
highest variance in the data, considering them as informative features. We then
assess the discriminative power of these features using the bootstrap forest
algorithm, discarding those that lack sufficient discriminative ability between
different classes. This algorithm explicitly determines the contribution of
each feature to the final prediction, assisting us in developing an explainable
AI model. The features feed into a boosting neural network as a latent feature
space. In the second layer, we introduce a novel uncertainty-based fuzzy
integral operator to fuse information from different CT scan slices. This
operator, by accounting for the dependencies between consecutive slices,
significantly improves detection accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hierarchical Subspaces of Policies for Continual Offline Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anthony Kobanda, Rémy Portelas, Odalric-Ambrym Maillard, Ludovic Denoyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In dynamic domains such as autonomous robotics and video game simulations,
agents must continuously adapt to new tasks while retaining previously acquired
skills. This ongoing process, known as Continual Reinforcement Learning,
presents significant challenges, including the risk of forgetting past
knowledge and the need for scalable solutions as the number of tasks increases.
To address these issues, we introduce HIerarchical LOW-rank Subspaces of
Policies (HILOW), a novel framework designed for continual learning in offline
navigation settings. HILOW leverages hierarchical policy subspaces to enable
flexible and efficient adaptation to new tasks while preserving existing
knowledge. We demonstrate, through a careful experimental study, the
effectiveness of our method in both classical MuJoCo maze environments and
complex video game-like simulations, showcasing competitive performance and
satisfying adaptability according to classical continual learning metrics, in
particular regarding memory usage. Our work provides a promising framework for
real-world applications where continuous learning from pre-collected data is
essential.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Surrogate-assisted multi-objective design of complex multibody systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Augustina C. Amakor, Manuel B. Berkemeier, Meike Wohlleben, Walter Sextro, Sebastian Peitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The optimization of large-scale multibody systems is a numerically
challenging task, in particular when considering multiple conflicting criteria
at the same time. In this situation, we need to approximate the Pareto set of
optimal compromises, which is significantly more expensive than finding a
single optimum in single-objective optimization. To prevent large costs, the
usage of surrogate models, constructed from a small but informative number of
expensive model evaluations, is a very popular and widely studied approach. The
central challenge then is to ensure a high quality (that is, near-optimality)
of the solutions that were obtained using the surrogate model, which can be
hard to guarantee with a single pre-computed surrogate. We present a
back-and-forth approach between surrogate modeling and multi-objective
optimization to improve the quality of the obtained solutions. Using the
example of an expensive-to-evaluate multibody system, we compare different
strategies regarding multi-objective optimization, sampling and also surrogate
modeling, to identify the most promising approach in terms of computational
efficiency and solution quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2412.01566</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Entropy Regularized Task Representation Learning for Offline
  Meta-Reinforcement Learning <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza nakhaei, Aidan Scannell, Joni Pajarinen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline meta-reinforcement learning aims to equip agents with the ability to
rapidly adapt to new tasks by training on data from a set of different tasks.
Context-based approaches utilize a history of state-action-reward transitions
-- referred to as the context -- to infer representations of the current task,
and then condition the agent, i.e., the policy and value function, on the task
representations. Intuitively, the better the task representations capture the
underlying tasks, the better the agent can generalize to new tasks.
Unfortunately, context-based approaches suffer from distribution mismatch, as
the context in the offline data does not match the context at test time,
limiting their ability to generalize to the test tasks. This leads to the task
representations overfitting to the offline training data. Intuitively, the task
representations should be independent of the behavior policy used to collect
the offline data. To address this issue, we approximately minimize the mutual
information between the distribution over the task representations and behavior
policy by maximizing the entropy of behavior policy conditioned on the task
representations. We validate our approach in MuJoCo environments, showing that
compared to baselines, our task representations more faithfully represent the
underlying tasks, leading to outperforming prior methods in both
in-distribution and out-of-distribution tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 Pages, Accepted at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Answer Set Networks: Casting Answer Set Programming into Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14814v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14814v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arseny Skryagin, Daniel Ochs, Phillip Deibert, Simon Kohaut, Devendra Singh Dhami, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although Answer Set Programming (ASP) allows constraining neural-symbolic
(NeSy) systems, its employment is hindered by the prohibitive costs of
computing stable models and the CPU-bound nature of state-of-the-art solvers.
To this end, we propose Answer Set Networks (ASN), a NeSy solver. Based on
Graph Neural Networks (GNN), ASNs are a scalable approach to ASP-based Deep
Probabilistic Logic Programming (DPPL). Specifically, we show how to translate
ASPs into ASNs and demonstrate how ASNs can efficiently solve the encoded
problem by leveraging GPU's batching and parallelization capabilities. Our
experimental evaluations demonstrate that ASNs outperform state-of-the-art
CPU-bound NeSy systems on multiple tasks. Simultaneously, we make the following
two contributions based on the strengths of ASNs. Namely, we are the first to
show the finetuning of Large Language Models (LLM) with DPPLs, employing ASNs
to guide the training with logic. Further, we show the "constitutional
navigation" of drones, i.e., encoding public aviation laws in an ASN for
routing Unmanned Aerial Vehicles in uncertain environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MARIA: a Multimodal <span class="highlight-title">Transformer</span> Model for Incomplete Healthcare Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Camillo Maria Caruso, Paolo Soda, Valerio Guarrasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In healthcare, the integration of multimodal data is pivotal for developing
comprehensive diagnostic and predictive models. However, managing missing data
remains a significant challenge in real-world applications. We introduce MARIA
(Multimodal Attention Resilient to Incomplete datA), a novel transformer-based
deep learning model designed to address these challenges through an
intermediate fusion strategy. Unlike conventional approaches that depend on
imputation, MARIA utilizes a masked self-attention mechanism, which processes
only the available data without generating synthetic values. This approach
enables it to effectively handle incomplete datasets, enhancing robustness and
minimizing biases introduced by imputation methods. We evaluated MARIA against
10 state-of-the-art machine learning and deep learning models across 8
diagnostic and prognostic tasks. The results demonstrate that MARIA outperforms
existing methods in terms of performance and resilience to varying levels of
data incompleteness, underscoring its potential for critical healthcare
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stack Trace Deduplication: Faster, More Accurately, and in More
  Realistic Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Egor Shibaev, Denis Sushentsev, Yaroslav Golubev, Aleksandr Khvorov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In large-scale software systems, there are often no fully-fledged bug reports
with human-written descriptions when an error occurs. In this case, developers
rely on stack traces, i.e., series of function calls that led to the error.
Since there can be tens and hundreds of thousands of them describing the same
issue from different users, automatic deduplication into categories is
necessary to allow for processing. Recent works have proposed powerful deep
learning-based approaches for this, but they are evaluated and compared in
isolation from real-life workflows, and it is not clear whether they will
actually work well at scale.
  To overcome this gap, this work presents three main contributions: a novel
model, an industry-based dataset, and a multi-faceted evaluation. Our model
consists of two parts - (1) an embedding model with byte-pair encoding and
approximate nearest neighbor search to quickly find the most relevant stack
traces to the incoming one, and (2) a reranker that re-ranks the most fitting
stack traces, taking into account the repeated frames between them. To
complement the existing datasets collected from open-source projects, we share
with the community SlowOps - a dataset of stack traces from IntelliJ-based
products developed by JetBrains, which has an order of magnitude more stack
traces per category. Finally, we carry out an evaluation that strives to be
realistic: measuring not only the accuracy of categorization, but also the
operation time and the ability to create new categories. The evaluation shows
that our model strikes a good balance - it outperforms other models on both
open-source datasets and SlowOps, while also being faster on time than most. We
release all of our code and data, and hope that our work can pave the way to
further practice-oriented research in the area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at SANER'25. 11 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extending TWIG: Zero-Shot Predictive Hyperparameter Selection for KGEs
  based on Graph Structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeffrey Sardina, John D. Kelleher, Declan O'Sullivan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graphs (KGs) have seen increasing use across various domains --
from biomedicine and linguistics to general knowledge modelling. In order to
facilitate the analysis of knowledge graphs, Knowledge Graph Embeddings (KGEs)
have been developed to automatically analyse KGs and predict new facts based on
the information in a KG, a task called "link prediction". Many existing studies
have documented that the structure of a KG, KGE model components, and KGE
hyperparameters can significantly change how well KGEs perform and what
relationships they are able to learn. Recently, the Topologically-Weighted
Intelligence Generation (TWIG) model has been proposed as a solution to
modelling how each of these elements relate. In this work, we extend the
previous research on TWIG and evaluate its ability to simulate the output of
the KGE model ComplEx in the cross-KG setting. Our results are twofold. First,
TWIG is able to summarise KGE performance on a wide range of hyperparameter
settings and KGs being learned, suggesting that it represents a general
knowledge of how to predict KGE performance from KG structure. Second, we show
that TWIG can successfully predict hyperparameter performance on unseen KGs in
the zero-shot setting. This second observation leads us to propose that, with
additional research, optimal hyperparameter selection for KGE models could be
determined in a pre-hoc manner using TWIG-like methods, rather than by using a
full hyperparameter search.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agent-Temporal Credit Assignment for Optimal Policy Preservation in
  Sparse Multi-Agent Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Kapoor, Sushant Swamy, Kale-ab Tessera, Mayank Baranwal, Mingfei Sun, Harshad Khadilkar, Stefano V. Albrecht
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multi-agent environments, agents often struggle to learn optimal policies
due to sparse or delayed global rewards, particularly in long-horizon tasks
where it is challenging to evaluate actions at intermediate time steps. We
introduce Temporal-Agent Reward Redistribution (TAR$^2$), a novel approach
designed to address the agent-temporal credit assignment problem by
redistributing sparse rewards both temporally and across agents. TAR$^2$
decomposes sparse global rewards into time-step-specific rewards and calculates
agent-specific contributions to these rewards. We theoretically prove that
TAR$^2$ is equivalent to potential-based reward shaping, ensuring that the
optimal policy remains unchanged. Empirical results demonstrate that TAR$^2$
stabilizes and accelerates the learning process. Additionally, we show that
when TAR$^2$ is integrated with single-agent reinforcement learning algorithms,
it performs as well as or better than traditional multi-agent reinforcement
learning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ALKAFI-LLAMA3: Fine-Tuning LLMs for Precise Legal Understanding in
  Palestine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rabee Qasem, Mohannad Hendi, Banan Tantour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable potential in
diverse domains, yet their application in the legal sector, particularly in
low-resource contexts, remains limited. This study addresses the challenges of
adapting LLMs to the Palestinian legal domain, where political instability,
fragmented legal frameworks, and limited AI resources hinder effective
machine-learning applications. We present a fine-tuned model based on a
quantized version of Llama-3.2-1B-Instruct, trained on a synthetic data set
derived from Palestinian legal texts. Using smaller-scale models and
strategically generated question-answer pairs, we achieve a cost-effective,
locally sustainable solution that provides accurate and contextually relevant
legal guidance. Our experiments demonstrate promising performance on various
query types, ranging from yes/no questions and narrative explanations to
complex legal differentiations, while highlighting areas for improvement, such
as handling calculation-based inquiries and structured list formatting. This
work provides a pathway for the deployment of AI-driven legal assistance tools
tailored to the needs of resource-constrained environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Opportunities and limitations of explaining quantum machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14753v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14753v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elies Gil-Fuster, Jonas R. Naujoks, Grégoire Montavon, Thomas Wiegand, Wojciech Samek, Jens Eisert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A common trait of many machine learning models is that it is often difficult
to understand and explain what caused the model to produce the given output.
While the explainability of neural networks has been an active field of
research in the last years, comparably little is known for quantum machine
learning models. Despite a few recent works analyzing some specific aspects of
explainability, as of now there is no clear big picture perspective as to what
can be expected from quantum learning models in terms of explainability. In
this work, we address this issue by identifying promising research avenues in
this direction and lining out the expected future results. We additionally
propose two explanation methods designed specifically for quantum machine
learning models, as first of their kind to the best of our knowledge. Next to
our pre-view of the field, we compare both existing and novel methods to
explain the predictions of quantum learning models. By studying explainability
in quantum machine learning, we can contribute to the sustainable development
of the field, preventing trust issues in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16+16 pages, 3+4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning Based Recalibration of SDSS and DESI BAO Alleviates Hubble
  and Clustering Tensions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Shah, Purba Mukherjee, Soumadeep Saha, Utpal Garain, Supratik Pal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional calibration of Baryon Acoustic Oscillations (BAO) data relies on
estimation of the sound horizon at drag epoch $r_d$ from early universe
observations by assuming a cosmological model. We present a recalibration of
two independent BAO datasets, SDSS and DESI, by employing deep learning
techniques for model-independent estimation of $r_d$, and explore the impacts
on $\Lambda$CDM cosmological parameters. Significant reductions in both Hubble
($H_0$) and clustering ($S_8$) tensions are observed for both the recalibrated
datasets. Moderate shifts in some other parameters hint towards further
exploration of such data-driven approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, 2 tables. Comments are welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A parametric algorithm is optimal for non-parametric regression of
  smooth functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Maran, Marcello Restelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the regression problem for a general function $f:[-1,1]^d\to
\mathbb R$ when the learner selects the training points $\{x_i\}_{i=1}^n$ to
achieve a uniform error bound across the entire domain. In this setting, known
historically as nonparametric regression, we aim to establish a sample
complexity bound that depends solely on the function's degree of smoothness.
Assuming periodicity at the domain boundaries, we introduce PADUA, an algorithm
that, with high probability, provides performance guarantees optimal up to
constant or logarithmic factors across all problem parameters. Notably, PADUA
is the first parametric algorithm with optimal sample complexity for this
setting. Due to this feature, we prove that, differently from the
non-parametric state of the art, PADUA enjoys optimal space complexity in the
prediction phase. To validate these results, we perform numerical experiments
over functions coming from real audio data, where PADUA shows comparable
performance to state-of-the-art methods, while requiring only a fraction of the
computational time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Inference and Human--Computer Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roderick Murray-Smith, John H. Williamson, Sebastian Stein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Active Inference is a closed-loop computational theoretical basis for
understanding behaviour, based on agents with internal probabilistic generative
models that encode their beliefs about how hidden states in their environment
cause their sensations. We review Active Inference and how it could be applied
to model the human-computer interaction loop. Active Inference provides a
coherent framework for managing generative models of humans, their
environments, sensors and interface components. It informs off-line design and
supports real-time, online adaptation. It provides model-based explanations for
behaviours observed in HCI, and new tools to measure important concepts such as
agency and engagement. We discuss how Active Inference offers a new basis for a
theory of interaction in HCI, tools for design of modern, complex sensor-based
systems, and integration of artificial intelligence technologies, enabling it
to cope with diversity in human users and contexts. We discuss the practical
challenges in implementing such Active Inference-based systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Use of Deep Learning Models for Semantic Clone Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subroto Nag Pinku, Debajyoti Mondal, Chanchal K. Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting and tracking code clones can ease various software development and
maintenance tasks when changes in a code fragment should be propagated over all
its copies. Several deep learning-based clone detection models have appeared in
the literature for detecting syntactic and semantic clones, widely evaluated
with the BigCloneBench dataset. However, class imbalance and the small number
of semantic clones make BigCloneBench less ideal for interpreting model
performance. Researchers also use other datasets such as GoogleCodeJam,
OJClone, and SemanticCloneBench to understand model generalizability. To
overcome the limitations of existing datasets, the GPT-assisted semantic and
cross-language clone dataset GPTCloneBench has been released. However, how
these models compare across datasets remains unclear. In this paper, we propose
a multi-step evaluation approach for five state-of-the-art clone detection
models leveraging existing benchmark datasets, including GPTCloneBench, and
using mutation operators to study model ability. Specifically, we examine three
highly-performing single-language models (ASTNN, GMN, CodeBERT) on
BigCloneBench, SemanticCloneBench, and GPTCloneBench, testing their robustness
with mutation operations. Additionally, we compare them against cross-language
models (C4, CLCDSA) known for detecting semantic clones. While single-language
models show high F1 scores for BigCloneBench, their performance on
SemanticCloneBench varies (up to 20%). Interestingly, the cross-language model
(C4) shows superior performance (around 7%) on SemanticCloneBench over other
models and performs similarly on BigCloneBench and GPTCloneBench. On
mutation-based datasets, C4 has more robust performance (less than 1%
difference) compared to single-language models, which show high variability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 40th IEEE International Conference on Software
  Maintenance and Evolution (ICSME 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting GNN Performance via Training Sample Selection Based on
  Adversarial Robustness Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have established themselves as one of the most
powerful neural network architectures, excelling in leveraging graph topology
and node features for various tasks. However, GNNs are inherently vulnerable to
noise in their inputs. Such noise can significantly degrade their performance.
To address this challenge, we propose a novel approach that employs adversarial
robustness evaluation techniques to identify nodes in the graph that are most
susceptible to noise. By selecting and constructing a training set composed of
these particularly noise-prone nodes, we then use them to train a Graph
Convolutional Network (GCN). Our experimental results demonstrate that this
strategy leads to substantial improvements in the GCN's performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative AI for Banks: Benchmarks and Algorithms for Synthetic
  Financial Transaction Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Sven Karst, Sook-Yee Chong, Abigail A. Antenor, Enyu Lin, Mahei Manhai Li, Jan Marco Leimeister
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The banking sector faces challenges in using deep learning due to data
sensitivity and regulatory constraints, but generative AI may offer a solution.
Thus, this study identifies effective algorithms for generating synthetic
financial transaction data and evaluates five leading models - Conditional
Tabular Generative Adversarial Networks (CTGAN), DoppelGANger (DGAN),
Wasserstein GAN, Financial Diffusion (FinDiff), and Tabular Variational
AutoEncoders (TVAE) - across five criteria: fidelity, synthesis quality,
efficiency, privacy, and graph structure. While none of the algorithms is able
to replicate the real data's graph structure, each excels in specific areas:
DGAN is ideal for privacy-sensitive tasks, FinDiff and TVAE excel in data
replication and augmentation, and CTGAN achieves a balance across all five
criteria, making it suitable for general applications with moderate privacy
concerns. As a result, our findings offer valuable insights for choosing the
most suitable algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at the 34th Workshop on Information Technologies and
  Systems (WITS 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FROC: Building Fair ROC from a Trained Classifier <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avyukta Manjunatha Vummintala, Shantanu Das, Sujit Gujar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers the problem of fair probabilistic binary classification
with binary protected groups. The classifier assigns scores, and a practitioner
predicts labels using a certain cut-off threshold based on the desired
trade-off between false positives vs. false negatives. It derives these
thresholds from the ROC of the classifier. The resultant classifier may be
unfair to one of the two protected groups in the dataset. It is desirable that
no matter what threshold the practitioner uses, the classifier should be fair
to both the protected groups; that is, the $\mathcal{L}_p$ norm between FPRs
and TPRs of both the protected groups should be at most $\varepsilon$. We call
such fairness on ROCs of both the protected attributes
$\varepsilon_p$-Equalized ROC. Given a classifier not satisfying
$\varepsilon_1$-Equalized ROC, we aim to design a post-processing method to
transform the given (potentially unfair) classifier's output (score) to a
suitable randomized yet fair classifier. That is, the resultant classifier must
satisfy $\varepsilon_1$-Equalized ROC. First, we introduce a threshold query
model on the ROC curves for each protected group. The resulting classifier is
bound to face a reduction in AUC. With the proposed query model, we provide a
rigorous theoretical analysis of the minimal AUC loss to achieve
$\varepsilon_1$-Equalized ROC. To achieve this, we design a linear time
algorithm, namely \texttt{FROC}, to transform a given classifier's output to a
probabilistic classifier that satisfies $\varepsilon_1$-Equalized ROC. We prove
that under certain theoretical conditions, \texttt{FROC}\ achieves the
theoretical optimal guarantees. We also study the performance of our
\texttt{FROC}\ on multiple real-world datasets with many trained classifiers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>51 pages, The 39th Annual AAAI Conference on Artificial Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prototypical Calibrating Ambiguous Samples for Micro-Action Recognition <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Li, Dan Guo, Guoliang Chen, Chunxiao Fan, Jingyuan Xu, Zhiliang Wu, Hehe Fan, Meng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Micro-Action Recognition (MAR) has gained increasing attention due to its
crucial role as a form of non-verbal communication in social interactions, with
promising potential for applications in human communication and emotion
analysis. However, current approaches often overlook the inherent ambiguity in
micro-actions, which arises from the wide category range and subtle visual
differences between categories. This oversight hampers the accuracy of
micro-action recognition. In this paper, we propose a novel Prototypical
Calibrating Ambiguous Network (\textbf{PCAN}) to unleash and mitigate the
ambiguity of MAR. \textbf{Firstly}, we employ a hierarchical action-tree to
identify the ambiguous sample, categorizing them into distinct sets of
ambiguous samples of false negatives and false positives, considering both
body- and action-level categories. \textbf{Secondly}, we implement an ambiguous
contrastive refinement module to calibrate these ambiguous samples by
regulating the distance between ambiguous samples and their corresponding
prototypes. This calibration process aims to pull false negative
($\mathbb{FN}$) samples closer to their respective prototypes and push false
positive ($\mathbb{FP}$) samples apart from their affiliated prototypes. In
addition, we propose a new prototypical diversity amplification loss to
strengthen the model's capacity by amplifying the differences between different
prototypes. \textbf{Finally}, we propose a prototype-guided rectification to
rectify prediction by incorporating the representability of prototypes.
Extensive experiments conducted on the benchmark dataset demonstrate the
superior performance of our method compared to existing approaches. The code is
available at https://github.com/kunli-cs/PCAN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Comprehensive Forecasting Framework based on Multi-Stage Hierarchical
  Forecasting Reconciliation and Adjustment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengchao Yang, Mithun Ghosh, Anish Saha, Dong Xu, Konstantin Shmakov, Kuang-chih Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ads demand forecasting for Walmart's ad products plays a critical role in
enabling effective resource planning, allocation, and management of ads
performance. In this paper, we introduce a comprehensive demand forecasting
system that tackles hierarchical time series forecasting in business settings.
Though traditional hierarchical reconciliation methods ensure forecasting
coherence, they often trade off accuracy for coherence especially at lower
levels and fail to capture the seasonality unique to each time-series in the
hierarchy. Thus, we propose a novel framework "Multi-Stage Hierarchical
Forecasting Reconciliation and Adjustment (Multi-Stage HiFoReAd)" to address
the challenges of preserving seasonality, ensuring coherence, and improving
accuracy. Our system first utilizes diverse models, ensembled through Bayesian
Optimization (BO), achieving base forecasts. The generated base forecasts are
then passed into the Multi-Stage HiFoReAd framework. The initial stage refines
the hierarchy using Top-Down forecasts and "harmonic alignment." The second
stage aligns the higher levels' forecasts using MinTrace algorithm, following
which the last two levels undergo "harmonic alignment" and "stratified
scaling", to eventually achieve accurate and coherent forecasts across the
whole hierarchy. Our experiments on Walmart's internal Ads-demand dataset and 3
other public datasets, each with 4 hierarchical levels, demonstrate that the
average Absolute Percentage Error from the cross-validation sets improve from
3% to 40% across levels against BO-ensemble of models (LGBM, MSTL+ETS, Prophet)
as well as from 1.2% to 92.9% against State-Of-The-Art models. In addition, the
forecasts at all hierarchical levels are proved to be coherent. The proposed
framework has been deployed and leveraged by Walmart's ads, sales and
operations teams to track future demands, make informed decisions and plan
resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in 2024 IEEE International Conference on Big Data (BigData)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computing Gram Matrix for SMILES Strings using RDKFingerprint and
  Sinkhorn-Knopp Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarwan Ali, Haris Mansoor, Prakash Chourasia, Imdad Ullah Khan, Murray Patterson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In molecular structure data, SMILES (Simplified Molecular Input Line Entry
System) strings are used to analyze molecular structure design. Numerical
feature representation of SMILES strings is a challenging task. This work
proposes a kernel-based approach for encoding and analyzing molecular
structures from SMILES strings. The proposed approach involves computing a
kernel matrix using the Sinkhorn-Knopp algorithm while using kernel principal
component analysis (PCA) for dimensionality reduction. The resulting
low-dimensional embeddings are then used for classification and regression
analysis. The kernel matrix is computed by converting the SMILES strings into
molecular structures using the Morgan Fingerprint, which computes a fingerprint
for each molecule. The distance matrix is computed using the pairwise kernels
function. The Sinkhorn-Knopp algorithm is used to compute the final kernel
matrix that satisfies the constraints of a probability distribution. This is
achieved by iteratively adjusting the kernel matrix until the marginal
distributions of the rows and columns match the desired marginal distributions.
We provided a comprehensive empirical analysis of the proposed kernel method to
evaluate its goodness with greater depth. The suggested method is assessed for
drug subcategory prediction (classification task) and solubility AlogPS
``Aqueous solubility and Octanol/Water partition coefficient" (regression task)
using the benchmark SMILES string dataset. The outcomes show the proposed
method outperforms several baseline methods in terms of supervised analysis and
has potential uses in molecular design and drug discovery. Overall, the
suggested method is a promising avenue for kernel methods-based molecular
structure analysis and design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Holistic Adversarially Robust Pruning <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Zhao, Christian Wressnegger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural networks can be drastically shrunk in size by removing redundant
parameters. While crucial for the deployment on resource-constraint hardware,
oftentimes, compression comes with a severe drop in accuracy and lack of
adversarial robustness. Despite recent advances, counteracting both aspects has
only succeeded for moderate compression rates so far. We propose a novel
method, HARP, that copes with aggressive pruning significantly better than
prior work. For this, we consider the network holistically. We learn a global
compression strategy that optimizes how many parameters (compression rate) and
which parameters (scoring connections) to prune specific to each layer
individually. Our method fine-tunes an existing model with dynamic
regularization, that follows a step-wise incremental function balancing the
different objectives. It starts by favoring robustness before shifting focus on
reaching the target compression rate and only then handles the objectives
equally. The learned compression strategies allow us to maintain the
pre-trained model natural accuracy and its adversarial robustness for a
reduction by 99% of the network original size. Moreover, we observe a crucial
influence of non-uniform compression across layers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReMoE: Fully Differentiable Mixture-of-Experts with ReLU Routing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14711v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14711v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziteng Wang, Jianfei Chen, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparsely activated Mixture-of-Experts (MoE) models are widely adopted to
scale up model capacity without increasing the computation budget. However,
vanilla TopK routers are trained in a discontinuous, non-differentiable way,
limiting their performance and scalability. To address this issue, we propose
ReMoE, a fully differentiable MoE architecture that offers a simple yet
effective drop-in replacement for the conventional TopK+Softmax routing,
utilizing ReLU as the router instead. We further propose methods to regulate
the router's sparsity while balancing the load among experts. ReMoE's
continuous nature enables efficient dynamic allocation of computation across
tokens and layers, while also exhibiting domain specialization. Our experiments
demonstrate that ReMoE consistently outperforms vanilla TopK-routed MoE across
various model sizes, expert counts, and levels of granularity. Furthermore,
ReMoE exhibits superior scalability with respect to the number of experts,
surpassing traditional MoE architectures. The implementation based on
Megatron-LM is available at https://github.com/thu-ml/ReMoE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taming the Memory Beast: Strategies for Reliable ML Training on
  Kubernetes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaideep Ray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kubernetes offers a powerful orchestration platform for machine learning
training, but memory management can be challenging due to specialized needs and
resource constraints. This paper outlines how Kubernetes handles memory
requests, limits, Quality of Service classes, and eviction policies for ML
workloads, with special focus on GPU memory and ephemeral storage. Common
pitfalls such as overcommitment, memory leaks, and ephemeral volume exhaustion
are examined. We then provide best practices for stable, scalable memory
utilization to help ML practitioners prevent out-of-memory events and ensure
high-performance ML training pipelines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lorentzian Residual Neural Networks <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neil He, Menglin Yang, Rex Ying
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperbolic neural networks have emerged as a powerful tool for modeling
hierarchical data structures prevalent in real-world datasets. Notably,
residual connections, which facilitate the direct flow of information across
layers, have been instrumental in the success of deep neural networks. However,
current methods for constructing hyperbolic residual networks suffer from
limitations such as increased model complexity, numerical instability, and
errors due to multiple mappings to and from the tangent space. To address these
limitations, we introduce LResNet, a novel Lorentzian residual neural network
based on the weighted Lorentzian centroid in the Lorentz model of hyperbolic
geometry. Our method enables the efficient integration of residual connections
in Lorentz hyperbolic neural networks while preserving their hierarchical
representation capabilities. We demonstrate that our method can theoretically
derive previous methods while offering improved stability, efficiency, and
effectiveness. Extensive experiments on both graph and vision tasks showcase
the superior performance and robustness of our method compared to
state-of-the-art Euclidean and hyperbolic alternatives. Our findings highlight
the potential of \method for building more expressive neural networks in
hyperbolic embedding space as a generally applicable method to multiple
architectures, including CNNs, GNNs, and graph Transformers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures, KDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to Synthesize Text Data without Model Collapse? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuekai Zhu, Daixuan Cheng, Hengli Li, Kaiyan Zhang, Ermo Hua, Xingtai Lv, Ning Ding, Zhouhan Lin, Zilong Zheng, Bowen Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model collapse in synthetic data indicates that iterative training on
self-generated data leads to a gradual decline in performance. With the
proliferation of AI models, synthetic data will fundamentally reshape the web
data ecosystem. Future GPT-$\{n\}$ models will inevitably be trained on a blend
of synthetic and human-produced data. In this paper, we focus on two questions:
what is the impact of synthetic data on language model training, and how to
synthesize data without model collapse? We first pre-train language models
across different proportions of synthetic data, revealing a negative
correlation between the proportion of synthetic data and model performance. We
further conduct statistical analysis on synthetic data to uncover
distributional shift phenomenon and over-concentration of n-gram features.
Inspired by the above findings, we propose token editing on human-produced data
to obtain semi-synthetic data. As a proof of concept, we theoretically
demonstrate that token-level editing can prevent model collapse, as the test
error is constrained by a finite upper bound. We conduct extensive experiments
on pre-training from scratch, continual pre-training, and supervised
fine-tuning. The results validate our theoretical proof that token-level
editing improves data quality and enhances model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LoLaFL: Low-Latency Federated Learning via Forward-only Propagation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14668v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14668v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jierui Zhang, Jianhao Huang, Kaibin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) has emerged as a widely adopted paradigm for enabling
edge learning with distributed data while ensuring data privacy. However, the
traditional FL with deep neural networks trained via backpropagation can hardly
meet the low-latency learning requirements in the sixth generation (6G) mobile
networks. This challenge mainly arises from the high-dimensional model
parameters to be transmitted and the numerous rounds of communication required
for convergence due to the inherent randomness of the training process. To
address this issue, we adopt the state-of-the-art principle of maximal coding
rate reduction to learn linear discriminative features and extend the resultant
white-box neural network into FL, yielding the novel framework of Low-Latency
Federated Learning (LoLaFL) via forward-only propagation. LoLaFL enables
layer-wise transmissions and aggregation with significantly fewer communication
rounds, thereby considerably reducing latency. Additionally, we propose two
\emph{nonlinear} aggregation schemes for LoLaFL. The first scheme is based on
the proof that the optimal NN parameter aggregation in LoLaFL should be
harmonic-mean-like. The second scheme further exploits the low-rank structures
of the features and transmits the low-rank-approximated covariance matrices of
features to achieve additional latency reduction. Theoretic analysis and
experiments are conducted to evaluate the performance of LoLaFL. In comparison
with traditional FL, the two nonlinear aggregation schemes for LoLaFL can
achieve reductions in latency of over 91\% and 98\%, respectively, while
maintaining comparable accuracies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling Uncertainty: A Deep Dive into Calibration and Performance of
  Multimodal Large Language Models <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijun Chen, Wenbo Hu, Guande He, Zhijie Deng, Zheng Zhang, Richang Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) combine visual and textual data for
tasks such as image captioning and visual question answering. Proper
uncertainty calibration is crucial, yet challenging, for reliable use in areas
like healthcare and autonomous driving. This paper investigates representative
MLLMs, focusing on their calibration across various scenarios, including before
and after visual fine-tuning, as well as before and after multimodal training
of the base LLMs. We observed miscalibration in their performance, and at the
same time, no significant differences in calibration across these scenarios. We
also highlight how uncertainty differs between text and images and how their
integration affects overall uncertainty. To better understand MLLMs'
miscalibration and their ability to self-assess uncertainty, we construct the
IDK (I don't know) dataset, which is key to evaluating how they handle
unknowns. Our findings reveal that MLLMs tend to give answers rather than admit
uncertainty, but this self-assessment improves with proper prompt adjustments.
Finally, to calibrate MLLMs and enhance model reliability, we propose
techniques such as temperature scaling and iterative prompt optimization. Our
results provide insights into improving MLLMs for effective and responsible
deployment in multimodal applications. Code and IDK dataset:
\href{https://github.com/hfutml/Calibration-MLLM}{https://github.com/hfutml/Calibration-MLLM}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trainable Adaptive Activation Function Structure (TAAFS) Enhances Neural
  Network Force Field Performance with Only Dozens of Additional Parameters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enji Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  At the heart of neural network force fields (NNFFs) is the architecture of
neural networks, where the capacity to model complex interactions is typically
enhanced through widening or deepening multilayer perceptrons (MLPs) or by
increasing layers of graph neural networks (GNNs). These enhancements, while
improving the model's performance, often come at the cost of a substantial
increase in the number of parameters. By applying the Trainable Adaptive
Activation Function Structure (TAAFS), we introduce a method that selects
distinct mathematical formulations for non-linear activations, thereby
increasing the precision of NNFFs with an insignificant addition to the
parameter count. In this study, we integrate TAAFS into a variety of neural
network models, resulting in observed accuracy improvements, and further
validate these enhancements through molecular dynamics (MD) simulations using
DeepMD.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Permutation recovery of spikes in noisy high-dimensional tensor
  estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gérard Ben Arous, CĆedric Gerbelot, Vanessa Piccolo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the dynamics of gradient flow in high dimensions for the
multi-spiked tensor problem, where the goal is to estimate $r$ unknown signal
vectors (spikes) from noisy Gaussian tensor observations. Specifically, we
analyze the maximum likelihood estimation procedure, which involves optimizing
a highly nonconvex random function. We determine the sample complexity required
for gradient flow to efficiently recover all spikes, without imposing any
assumptions on the separation of the signal-to-noise ratios (SNRs). More
precisely, our results provide the sample complexity required to guarantee
recovery of the spikes up to a permutation. Our work builds on our companion
paper [Ben Arous, Gerbelot, Piccolo 2024], which studies Langevin dynamics and
determines the sample complexity and separation conditions for the SNRs
necessary for ensuring exact recovery of the spikes (where the recovered
permutation matches the identity). During the recovery process, the
correlations between the estimators and the hidden vectors increase in a
sequential manner. The order in which these correlations become significant
depends on their initial values and the corresponding SNRs, which ultimately
determines the permutation of the recovered spikes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 2 figures. arXiv admin note: substantial text overlap with
  arXiv:2408.06401</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive <span class="highlight-title">Prompt</span> Tuning: Vision Guided <span class="highlight-title">Prompt</span> Tuning with Cross-Attention
  for Fine-Grained Few-Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Brouwer, Jan Erik van Woerden, Gertjan Burghouts, Matias Valedenegro-Toro, Marco Zullich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot, fine-grained classification in computer vision poses significant
challenges due to the need to differentiate subtle class distinctions with
limited data. This paper presents a novel method that enhances the Contrastive
Language-Image Pre-Training (CLIP) model through adaptive prompt tuning, guided
by real-time visual inputs. Unlike existing techniques such as Context
Optimization (CoOp) and Visual Prompt Tuning (VPT), which are constrained by
static prompts or visual token reliance, the proposed approach leverages a
cross-attention mechanism to dynamically refine text prompts for the image at
hand. This enables an image-specific alignment of textual features with image
patches extracted from the Vision Transformer, making the model more effective
for datasets with high intra-class variance and low inter-class differences.
The method is evaluated on several datasets, including CUBirds, Oxford Flowers,
and FGVC Aircraft, showing significant performance gains over static prompt
tuning approaches. To ensure these performance gains translate into trustworthy
predictions, we integrate Monte-Carlo Dropout in our approach to improve the
reliability of the model predictions and uncertainty estimates. This
integration provides valuable insights into the model's predictive confidence,
helping to identify when predictions can be trusted and when additional
verification is necessary. This dynamic approach offers a robust solution,
advancing the state-of-the-art for few-shot fine-grained classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust PCA Based on Adaptive Weighted Least Squares and Low-Rank Matrix
  Factorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kexin Li, You-wei Wen, Xu Xiao, Mingchao Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust Principal Component Analysis (RPCA) is a fundamental technique for
decomposing data into low-rank and sparse components, which plays a critical
role for applications such as image processing and anomaly detection.
Traditional RPCA methods commonly use $\ell_1$ norm regularization to enforce
sparsity, but this approach can introduce bias and result in suboptimal
estimates, particularly in the presence of significant noise or outliers.
Non-convex regularization methods have been proposed to mitigate these
challenges, but they tend to be complex to optimize and sensitive to initial
conditions, leading to potential instability in solutions. To overcome these
challenges, in this paper, we propose a novel RPCA model that integrates
adaptive weighted least squares (AWLS) and low-rank matrix factorization
(LRMF). The model employs a {self-attention-inspired} mechanism in its weight
update process, allowing the weight matrix to dynamically adjust and emphasize
significant components during each iteration. By employing a weighted F-norm
for the sparse component, our method effectively reduces bias while simplifying
the computational process compared to traditional $\ell_1$-norm-based methods.
We use an alternating minimization algorithm, where each subproblem has an
explicit solution, thereby improving computational efficiency. Despite its
simplicity, numerical experiments demonstrate that our method outperforms
existing non-convex regularization approaches, offering superior performance
and stability, as well as enhanced accuracy and robustness in practical
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Qua$^2$SeDiMo: Quantifiable Quantization Sensitivity of Diffusion Models <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keith G. Mills, Mohammad Salameh, Ruichen Chen, Negar Hassanpour, Wei Lu, Di Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion Models (DM) have democratized AI image generation through an
iterative denoising process. Quantization is a major technique to alleviate the
inference cost and reduce the size of DM denoiser networks. However, as
denoisers evolve from variants of convolutional U-Nets toward newer Transformer
architectures, it is of growing importance to understand the quantization
sensitivity of different weight layers, operations and architecture types to
performance. In this work, we address this challenge with Qua$^2$SeDiMo, a
mixed-precision Post-Training Quantization framework that generates explainable
insights on the cost-effectiveness of various model weight quantization methods
for different denoiser operation types and block structures. We leverage these
insights to make high-quality mixed-precision quantization decisions for a
myriad of diffusion models ranging from foundational U-Nets to state-of-the-art
Transformers. As a result, Qua$^2$SeDiMo can construct 3.4-bit, 3.9-bit,
3.65-bit and 3.7-bit weight quantization on PixArt-${\alpha}$,
PixArt-${\Sigma}$, Hunyuan-DiT and SDXL, respectively. We further pair our
weight-quantization configurations with 6-bit activation quantization and
outperform existing approaches in terms of quantitative metrics and generative
image quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025; version includes supplementary material; 22 Pages, 18
  Figures, 8 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continuous latent representations for modeling precipitation with deep
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gokul Radhakrishnan, Rahul Sundar, Nishant Parashar, Antoine Blanchard, Daiwei Wang, Boyko Dodov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sparse and spatio-temporally discontinuous nature of precipitation data
presents significant challenges for simulation and statistical processing for
bias correction and downscaling. These include incorrect representation of
intermittency and extreme values (critical for hydrology applications), Gibbs
phenomenon upon regridding, and lack of fine scales details. To address these
challenges, a common approach is to transform the precipitation variable
nonlinearly into one that is more malleable. In this work, we explore how deep
learning can be used to generate a smooth, spatio-temporally continuous
variable as a proxy for simulation of precipitation data. We develop a normally
distributed field called pseudo-precipitation (PP) as an alternative for
simulating precipitation. The practical applicability of this variable is
investigated by applying it for downscaling precipitation from \(1\degree\)
(\(\sim\) 100 km) to \(0.25\degree\) (\(\sim\) 25 km).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pitfalls of topology-aware image segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14619v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14619v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander H. Berger, Laurin Lux, Alexander Weers, Martin Menten, Daniel Rueckert, Johannes C. Paetzold
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topological correctness, i.e., the preservation of structural integrity and
specific characteristics of shape, is a fundamental requirement for medical
imaging tasks, such as neuron or vessel segmentation. Despite the recent surge
in topology-aware methods addressing this challenge, their real-world
applicability is hindered by flawed benchmarking practices. In this paper, we
identify critical pitfalls in model evaluation that include inadequate
connectivity choices, overlooked topological artifacts in ground truth
annotations, and inappropriate use of evaluation metrics. Through detailed
empirical analysis, we uncover these issues' profound impact on the evaluation
and ranking of segmentation methods. Drawing from our findings, we propose a
set of actionable recommendations to establish fair and robust evaluation
standards for topology-aware medical image segmentation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at
  https://github.com/AlexanderHBerger/topo-pitfalls</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Scalable and Deep Graph Neural Networks via Noise Masking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14602v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14602v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Liang, Wentao Zhang, Zeang Sheng, Ling Yang, Quanqing Xu, Jiawei Jiang, Yunhai Tong, Bin Cu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, Graph Neural Networks (GNNs) have achieved remarkable
success in many graph mining tasks. However, scaling them to large graphs is
challenging due to the high computational and storage costs of repeated feature
propagation and non-linear transformation during training. One commonly
employed approach to address this challenge is model-simplification, which only
executes the Propagation (P) once in the pre-processing, and Combine (C) these
receptive fields in different ways and then feed them into a simple model for
better performance. Despite their high predictive performance and scalability,
these methods still face two limitations. First, existing approaches mainly
focus on exploring different C methods from the model perspective, neglecting
the crucial problem of performance degradation with increasing P depth from the
data-centric perspective, known as the over-smoothing problem. Second,
pre-processing overhead takes up most of the end-to-end processing time,
especially for large-scale graphs. To address these limitations, we present
random walk with noise masking (RMask), a plug-and-play module compatible with
the existing model-simplification works. This module enables the exploration of
deeper GNNs while preserving their scalability. Unlike the previous
model-simplification works, we focus on continuous P and found that the noise
existing inside each P is the cause of the over-smoothing issue, and use the
efficient masking mechanism to eliminate them. Experimental results on six
real-world datasets demonstrate that model-simplification works equipped with
RMask yield superior performance compared to their original version and can
make a good trade-off between accuracy and efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast inverse lithography based on a model-driven block stacking
  convolutional neural network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixiang Chen, Yang Zhao, Haoqin Li, Rui Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of lithography, Optical Proximity Correction (OPC) is a crucial
resolution enhancement technique that optimizes the transmission function of
photomasks on a pixel-based to effectively counter Optical Proximity Effects
(OPE). However, conventional pixel-based OPC methods often generate patterns
that pose manufacturing challenges, thereby leading to the increased cost in
practical scenarios. This paper presents a novel inverse lithographic approach
to OPC, employing a model-driven, block stacking deep learning framework that
expedites the generation of masks conducive to manufacturing. This method is
founded on vector lithography modelling and streamlines the training process by
eliminating the requirement for extensive labeled datasets. Furthermore,
diversity of mask patterns is enhanced by employing a wave function collapse
algorithm, which facilitates the random generation of a multitude of target
patterns, therefore significantly expanding the range of mask paradigm.
Numerical experiments have substantiated the efficacy of the proposed
end-to-end approach, highlighting its superior capability to manage mask
complexity within the context of advanced OPC lithography. This advancement is
anticipated to enhance the feasibility and economic viability of OPC technology
within actual manufacturing environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LDP: Generalizing to Multilingual Visual Information Extraction by
  Language Decoupled <span class="highlight-title">Pretrain</span>ing <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huawen Shen, Gengluo Li, Jinwen Zhong, Yu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Information Extraction (VIE) plays a crucial role in the comprehension
of semi-structured documents, and several pre-trained models have been
developed to enhance performance. However, most of these works are monolingual
(usually English). Due to the extremely unbalanced quantity and quality of
pre-training corpora between English and other languages, few works can extend
to non-English scenarios. In this paper, we conduct systematic experiments to
show that vision and layout modality hold invariance among images with
different languages. If decoupling language bias from document images, a
vision-layout-based model can achieve impressive cross-lingual generalization.
Accordingly, we present a simple but effective multilingual training paradigm
LDP (Language Decoupled Pre-training) for better utilization of monolingual
pre-training data. Our proposed model LDM (Language Decoupled Model) is first
pre-trained on the language-independent data, where the language knowledge is
decoupled by a diffusion model, and then the LDM is fine-tuned on the
downstream languages. Extensive experiments show that the LDM outperformed all
SOTA multilingual pre-trained models, and also maintains competitiveness on
downstream monolingual/English benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Sensor Object Anomaly Detection: Unifying Appearance, Geometry,
  and Internal Properties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqiao Li, Bozhong Zheng, Xiaohao Xu, Jinye Gan, Fading Lu, Xiang Li, Na Ni, Zheng Tian, Xiaonan Huang, Shenghua Gao, Yingna Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object anomaly detection is essential for industrial quality inspection, yet
traditional single-sensor methods face critical limitations. They fail to
capture the wide range of anomaly types, as single sensors are often
constrained to either external appearance, geometric structure, or internal
properties. To overcome these challenges, we introduce MulSen-AD, the first
high-resolution, multi-sensor anomaly detection dataset tailored for industrial
applications. MulSen-AD unifies data from RGB cameras, laser scanners, and
lock-in infrared thermography, effectively capturing external appearance,
geometric deformations, and internal defects. The dataset spans 15 industrial
products with diverse, real-world anomalies. We also present MulSen-AD Bench, a
benchmark designed to evaluate multi-sensor methods, and propose
MulSen-TripleAD, a decision-level fusion algorithm that integrates these three
modalities for robust, unsupervised object anomaly detection. Our experiments
demonstrate that multi-sensor fusion substantially outperforms single-sensor
approaches, achieving 96.1% AUROC in object-level detection accuracy. These
results highlight the importance of integrating multi-sensor data for
comprehensive industrial anomaly detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MixLLM: LLM Quantization with Global Mixed-precision between
  Output-features and Highly-efficient System Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Zheng, Xiaonan Song, Chuanjie Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantization has become one of the most effective methodologies to compress
LLMs into smaller size. However, the existing quantization solutions still show
limitations of either non-negligible accuracy drop or system inefficiency. In
this paper, we make a comprehensive analysis of the general quantization
principles on their effect to the triangle of accuracy, memory consumption and
system efficiency. We propose MixLLM that explores the new optimization space
of mixed-precision quantization between output features based on the insight
that different output features matter differently in the model. MixLLM
identifies the output features with high salience in the global view rather
than within each single layer, effectively assigning the larger bit-width to
output features that need it most to achieve good accuracy with low memory
consumption. We present the sweet spot of quantization configuration of
algorithm-system co-design that leads to high accuracy and system efficiency.
To address the system challenge, we design the two-step dequantization to make
use of the int8 Tensor Core easily and fast data type conversion to reduce
dequantization overhead significantly, and present the software pipeline to
overlap the memory access, dequantization and the MatMul to the best. Extensive
experiments show that with only 10% more bits, the PPL increasement can be
reduced from about 0.5 in SOTA to within 0.2 for Llama 3.1 70B, while on
average MMLU-Pro improves by 0.93 over the SOTA of three popular models. In
addition to its superior accuracy, MixLLM also achieves state-of-the-art system
efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code will be released in the future</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerated Patient-Specific Calibration via Differentiable Hemodynamics
  Simulations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diego Renner, Georgios Kissas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the goals of personalized medicine is to tailor diagnostics to
individual patients. Diagnostics are performed in practice by measuring
quantities, called biomarkers, that indicate the existence and progress of a
disease. In common cardiovascular diseases, such as hypertension, biomarkers
that are closely related to the clinical representation of a patient can be
predicted using computational models. Personalizing computational models
translates to considering patient-specific flow conditions, for example, the
compliance of blood vessels that cannot be a priori known and quantities such
as the patient geometry that can be measured using imaging. Therefore, a
patient is identified by a set of measurable and nonmeasurable parameters
needed to well-define a computational model; else, the computational model is
not personalized, meaning it is prone to large prediction errors. Therefore, to
personalize a computational model, sufficient information needs to be extracted
from the data. The current methods by which this is done are either
inefficient, due to relying on slow-converging optimization methods, or hard to
interpret, due to using `black box` deep-learning algorithms. We propose a
personalized diagnostic procedure based on a differentiable 0D-1D Navier-Stokes
reduced order model solver and fast parameter inference methods that take
advantage of gradients through the solver. By providing a faster method for
performing parameter inference and sensitivity analysis through
differentiability while maintaining the interpretability of well-understood
mathematical models and numerical methods, the best of both worlds is combined.
The performance of the proposed solver is validated against a well-established
process on different geometries, and different parameter inference processes
are successfully performed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Global Spatio-Temporal Fusion-based Traffic Prediction Algorithm with
  Anomaly Aware 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoqun Liu, Xuanpeng Li, Chen Gong, Guangyu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic prediction is an indispensable component of urban planning and
traffic management. Achieving accurate traffic prediction hinges on the ability
to capture the potential spatio-temporal relationships among road sensors.
However, the majority of existing works focus on local short-term
spatio-temporal correlations, failing to fully consider the interactions of
different sensors in the long-term state. In addition, these works do not
analyze the influences of anomalous factors, or have insufficient ability to
extract personalized features of anomalous factors, which make them
ineffectively capture their spatio-temporal influences on traffic prediction.
To address the aforementioned issues, We propose a global spatio-temporal
fusion-based traffic prediction algorithm that incorporates anomaly awareness.
Initially, based on the designed anomaly detection network, we construct an
efficient anomalous factors impacting module (AFIM), to evaluate the
spatio-temporal impact of unexpected external events on traffic prediction.
Furthermore, we propose a multi-scale spatio-temporal feature fusion module
(MTSFFL) based on the transformer architecture, to obtain all possible both
long and short term correlations among different sensors in a wide-area traffic
environment for accurate prediction of traffic flow. Finally, experiments are
implemented based on real-scenario public transportation datasets (PEMS04 and
PEMS08) to demonstrate that our approach can achieve state-of-the-art
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AIArena: A Blockchain-Based Decentralized AI Training Platform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhipeng Wang, Rui Sun, Elizabeth Lui, Tuo Zhou, Yizhe Wen, Jiahao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of AI has underscored critical challenges in its
development and implementation, largely due to centralized control by a few
major corporations. This concentration of power intensifies biases within AI
models, resulting from inadequate governance and oversight mechanisms.
Additionally, it limits public involvement and heightens concerns about the
integrity of model generation. Such monopolistic control over data and AI
outputs threatens both innovation and fair data usage, as users inadvertently
contribute data that primarily benefits these corporations. In this work, we
propose AIArena, a blockchain-based decentralized AI training platform designed
to democratize AI development and alignment through on-chain incentive
mechanisms. AIArena fosters an open and collaborative environment where
participants can contribute models and computing resources. Its on-chain
consensus mechanism ensures fair rewards for participants based on their
contributions. We instantiate and implement AIArena on the public Base
blockchain Sepolia testnet, and the evaluation results demonstrate the
feasibility of AIArena in real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GBRIP: Granular Ball Representation for Imbalanced Partial Label
  Learning <span class="chip">AAAI25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintao Huang, Yiu-ming Cheung, Chi-man Vong, Wenbin Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial label learning (PLL) is a complicated weakly supervised
multi-classification task compounded by class imbalance. Currently, existing
methods only rely on inter-class pseudo-labeling from inter-class features,
often overlooking the significant impact of the intra-class imbalanced features
combined with the inter-class. To address these limitations, we introduce
Granular Ball Representation for Imbalanced PLL (GBRIP), a novel framework for
imbalanced PLL. GBRIP utilizes coarse-grained granular ball representation and
multi-center loss to construct a granular ball-based nfeature space through
unsupervised learning, effectively capturing the feature distribution within
each class. GBRIP mitigates the impact of confusing features by systematically
refining label disambiguation and estimating imbalance distributions. The novel
multi-center loss function enhances learning by emphasizing the relationships
between samples and their respective centers within the granular balls.
Extensive experiments on standard benchmarks demonstrate that GBRIP outperforms
existing state-of-the-art methods, offering a robust solution to the challenges
of imbalanced PLL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SoK: Watermarking for AI-Generated Content 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18479v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18479v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuandong Zhao, Sam Gunn, Miranda Christ, Jaiden Fairoze, Andres Fabrega, Nicholas Carlini, Sanjam Garg, Sanghyun Hong, Milad Nasr, Florian Tramer, Somesh Jha, Lei Li, Yu-Xiang Wang, Dawn Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the outputs of generative AI (GenAI) techniques improve in quality, it
becomes increasingly challenging to distinguish them from human-created
content. Watermarking schemes are a promising approach to address the problem
of distinguishing between AI and human-generated content. These schemes embed
hidden signals within AI-generated content to enable reliable detection. While
watermarking is not a silver bullet for addressing all risks associated with
GenAI, it can play a crucial role in enhancing AI safety and trustworthiness by
combating misinformation and deception. This paper presents a comprehensive
overview of watermarking techniques for GenAI, beginning with the need for
watermarking from historical and regulatory perspectives. We formalize the
definitions and desired properties of watermarking schemes and examine the key
objectives and threat models for existing approaches. Practical evaluation
strategies are also explored, providing insights into the development of robust
watermarking techniques capable of resisting various attacks. Additionally, we
review recent representative works, highlight open challenges, and discuss
potential directions for this emerging field. By offering a thorough
understanding of watermarking in GenAI, this work aims to guide researchers in
advancing watermarking methods and applications, and support policymakers in
addressing the broader implications of GenAI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S$^{2}$FT: Efficient, Scalable and Generalizable LLM Fine-tuning by
  Structured Sparsity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06289v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06289v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Yang, Jixuan Leng, Geyang Guo, Jiawei Zhao, Ryumei Nakada, Linjun Zhang, Huaxiu Yao, Beidi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current PEFT methods for LLMs can achieve either high quality, efficient
training, or scalable serving, but not all three simultaneously. To address
this limitation, we investigate sparse fine-tuning and observe a remarkable
improvement in generalization ability. Utilizing this key insight, we propose a
family of Structured Sparse Fine-Tuning (S$^{2}$FT) methods for LLMs, which
concurrently achieve state-of-the-art fine-tuning performance, training
efficiency, and inference scalability. S$^{2}$FT accomplishes this by
"selecting sparsely and computing densely". It selects a few heads and channels
in the MHA and FFN modules for each Transformer block, respectively. Next, it
co-permutes weight matrices on both sides of the coupled structures in LLMs to
connect the selected components in each layer into a dense submatrix. Finally,
S$^{2}$FT performs in-place gradient updates on all submatrices. Through
theoretical analysis and empirical results, our method prevents forgetting
while simplifying optimization, delivers SOTA performance on both commonsense
and arithmetic reasoning with 4.6% and 1.3% average improvements compared to
LoRA, and surpasses full FT by 11.5% when generalizing to various domains after
instruction tuning. Using our partial backpropagation algorithm, S$^{2}$FT
saves training memory up to 3$\times$ and improves latency by 1.5-2.7$\times$
compared to full FT, while delivering an average 10% improvement over LoRA on
both metrics. We further demonstrate that the weight updates in S$^{2}$FT can
be decoupled into adapters, enabling effective fusion, fast switch, and
efficient parallelism for serving multiple fine-tuned models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ URIEL+: Enhancing Linguistic Inclusion and Usability in a Typological
  and Multilingual Knowledge Base <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18472v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18472v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Khan, Mason Shipton, David Anugraha, Kaiyao Duan, Phuong H. Hoang, Eric Khiu, A. Seza Doğruöz, En-Shiun Annie Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  URIEL is a knowledge base offering geographical, phylogenetic, and
typological vector representations for 7970 languages. It includes distance
measures between these vectors for 4005 languages, which are accessible via the
lang2vec tool. Despite being frequently cited, URIEL is limited in terms of
linguistic inclusion and overall usability. To tackle these challenges, we
introduce URIEL+, an enhanced version of URIEL and lang2vec that addresses
these limitations. In addition to expanding typological feature coverage for
2898 languages, URIEL+ improves the user experience with robust, customizable
distance calculations to better suit the needs of users. These upgrades also
offer competitive performance on downstream tasks and provide distances that
better align with linguistic distance studies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sometimes I am a Tree: Data Drives Unstable Hierarchical Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04619v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04619v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Qin, Naomi Saphra, David Alvarez-Melis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models (LMs), like other neural networks, often favor shortcut
heuristics based on surface-level patterns. Although LMs behave like n-gram
models early in training, they must eventually learn hierarchical syntactic
representations to correctly apply grammatical rules out-of-distribution (OOD).
In this work, we use case studies of English grammar to explore how complex,
diverse training data drives models to generalize OOD. We construct a framework
that unifies our understanding of random variation with training dynamics, rule
selection with memorization, and data diversity with complexity. We show that
these factors are nuanced, and that intermediate levels of diversity and
complexity lead to inconsistent behavior across random seeds and to unstable
training dynamics. Our findings emphasize the critical role of training data in
shaping generalization patterns and illuminate how competing model strategies
lead to inconsistent generalization outcomes across random seeds. Code is
available at https://github.com/sunnytqin/concept_comp.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Ewald summation for machine learning of long-range interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15165v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15165v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingqing Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning interatomic potentials (MLIPs) often neglect long-range
interactions, such as electrostatic and dispersion forces. In this work, we
introduce a straightforward and efficient method to account for long-range
interactions by learning a latent variable from local atomic descriptors and
applying an Ewald summation to this variable. We demonstrate that in systems
including charged and polar molecular dimers, bulk water, and water-vapor
interface, standard short-ranged MLIPs can lead to unphysical predictions even
when employing message passing. The long-range models effectively eliminate
these artifacts, with only about twice the computational cost of short-range
MLIPs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Machine Unlearning with Dimensional Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17710v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17710v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonguk Seo, Dongwan Kim, Bohyung Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine unlearning, an emerging research topic focusing on compliance with
data privacy regulations, enables trained models to remove the information
learned from specific data. While many existing methods indirectly address this
issue by intentionally injecting incorrect supervisions, they can drastically
and unpredictably alter the decision boundaries and feature spaces, leading to
training instability and undesired side effects. To fundamentally approach this
task, we first analyze the changes in latent feature spaces between original
and retrained models, and observe that the feature representations of samples
not involved in training are closely aligned with the feature manifolds of
previously seen samples in training. Based on these findings, we introduce a
novel evaluation metric for machine unlearning, coined dimensional alignment,
which measures the alignment between the eigenspaces of the forget and retain
set samples. We employ this metric as a regularizer loss to build a robust and
stable unlearning framework, which is further enhanced by integrating a
self-distillation loss and an alternating training scheme. Our framework
effectively eliminates information from the forget set and preserves knowledge
from the retain set. Lastly, we identify critical flaws in established
evaluation metrics for machine unlearning, and introduce new evaluation tools
that more accurately reflect the fundamental goals of machine unlearning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Metric Compatible Training for Online Backfilling in Large-Scale
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.03767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.03767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonguk Seo, Mustafa Gokhan Uzunbas, Bohyung Han, Sara Cao, Ser-Nam Lim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backfilling is the process of re-extracting all gallery embeddings from
upgraded models in image retrieval systems. It inevitably requires a
prohibitively large amount of computational cost and even entails the downtime
of the service. Although backward-compatible learning sidesteps this challenge
by tackling query-side representations, this leads to suboptimal solutions in
principle because gallery embeddings cannot benefit from model upgrades. We
address this dilemma by introducing an online backfilling algorithm, which
enables us to achieve a progressive performance improvement during the
backfilling process while not sacrificing the final performance of new model
after the completion of backfilling. To this end, we first propose a simple
distance rank merge technique for online backfilling. Then, we incorporate a
reverse transformation module for more effective and efficient merging, which
is further enhanced by adopting a metric-compatible contrastive learning
approach. These two components help to make the distances of old and new models
compatible, resulting in desirable merge results during backfilling with no
extra computational overhead. Extensive experiments show the effectiveness of
our framework on four standard benchmarks in various settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Latent Variable Sequence Identification for Cognitive Models with Neural
  Network Estimators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14742v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14742v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ti-Fen Pan, Jing-Jing Li, Bill Thompson, Anne Collins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting time-varying latent variables from computational cognitive models
is a key step in model-based neural analysis, which aims to understand the
neural correlates of cognitive processes. However, existing methods only allow
researchers to infer latent variables that explain subjects' behavior in a
relatively small class of cognitive models. For example, a broad class of
relevant cognitive models with analytically intractable likelihood is currently
out of reach from standard techniques, based on Maximum a Posteriori parameter
estimation. Here, we present an approach that extends neural Bayes estimation
to learn a direct mapping between experimental data and the targeted latent
variable space using recurrent neural networks and simulated datasets. We show
that our approach achieves competitive performance in inferring latent variable
sequences in both tractable and intractable models. Furthermore, the approach
is generalizable across different computational models and is adaptable for
both continuous and discrete latent spaces. We then demonstrate its
applicability in real world datasets. Our work underscores that combining
recurrent neural networks and simulation-based inference to identify latent
variable sequences can enable researchers to access a wider class of cognitive
models for model-based neural analyses, and thus test a broader set of
theories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLMs as Zero-shot Graph Learners: Alignment of GNN Representations with
  LLM Token Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14512v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14512v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Duo Wang, Yuan Zuo, Fengzhi Li, Junjie Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot graph machine learning, especially with graph neural networks
(GNNs), has garnered significant interest due to the challenge of scarce
labeled data. While methods like self-supervised learning and graph prompt
learning have been extensively explored, they often rely on fine-tuning with
task-specific labels, limiting their effectiveness in zero-shot scenarios.
Inspired by the zero-shot capabilities of instruction-fine-tuned large language
models (LLMs), we introduce a novel framework named Token Embedding-Aligned
Graph Language Model (TEA-GLM) that leverages LLMs as cross-dataset and
cross-task zero-shot learners for graph machine learning. Concretely, we
pretrain a GNN, aligning its representations with token embeddings of an LLM.
We then train a linear projector that transforms the GNN's representations into
a fixed number of graph token embeddings without tuning the LLM. A unified
instruction is designed for various graph tasks at different levels, such as
node classification (node-level) and link prediction (edge-level). These design
choices collectively enhance our method's effectiveness in zero-shot learning,
setting it apart from existing methods. Experiments show that our graph token
embeddings help the LLM predictor achieve state-of-the-art performance on
unseen datasets and tasks compared to other methods using LLMs as predictors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning from Linear Algebra: A Graph Neural Network Approach to
  Preconditioner Design for Conjugate Gradient Solvers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15557v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15557v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladislav Trifonov, Alexander Rudikov, Oleg Iliev, Yuri M. Laevsky, Ivan Oseledets, Ekaterina Muravleva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large linear systems are ubiquitous in modern computational science and
engineering. The main recipe for solving them is the use of Krylov subspace
iterative methods with well-designed preconditioners. Deep learning models can
be used as nonlinear preconditioners during the iteration of linear solvers
such as the conjugate gradient (CG) method. Neural network models require an
enormous number of parameters to approximate well in this setup. Another
approach is to take advantage of small graph neural networks (GNNs) to
construct preconditioners with predefined sparsity patterns. Recently, GNNs
have been shown to be a promising tool for designing preconditioners to reduce
the overall computational cost of iterative methods by constructing them more
efficiently than with classical linear algebra techniques. However,
preconditioners designed with these approaches cannot outperform those designed
with classical methods in terms of the number of iterations in CG. In our work,
we recall well-established preconditioners from linear algebra and use them as
a starting point for training the GNN to obtain preconditioners that reduce the
condition number of the system more significantly. Numerical experiments show
that our approach outperforms both classical and neural network-based methods
for an important class of parametric partial differential equations. We also
provide a heuristic justification for the loss function used and show that
preconditioners obtained by learning with this loss function reduce the
condition number in a more desirable way for CG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TurboSVM-FL: Boosting Federated Learning through SVM Aggregation for
  Lazy Clients <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12012v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12012v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengdi Wang, Anna Bodonhelyi, Efe Bozkir, Enkelejda Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a distributed collaborative machine learning paradigm
that has gained strong momentum in recent years. In federated learning, a
central server periodically coordinates models with clients and aggregates the
models trained locally by clients without necessitating access to local data.
Despite its potential, the implementation of federated learning continues to
encounter several challenges, predominantly the slow convergence that is
largely due to data heterogeneity. The slow convergence becomes particularly
problematic in cross-device federated learning scenarios where clients may be
strongly limited by computing power and storage space, and hence counteracting
methods that induce additional computation or memory cost on the client side
such as auxiliary objective terms and larger training iterations can be
impractical. In this paper, we propose a novel federated aggregation strategy,
TurboSVM-FL, that poses no additional computation burden on the client side and
can significantly accelerate convergence for federated classification task,
especially when clients are "lazy" and train their models solely for few epochs
for next global aggregation. TurboSVM-FL extensively utilizes support vector
machine to conduct selective aggregation and max-margin spread-out
regularization on class embeddings. We evaluate TurboSVM-FL on multiple
datasets including FEMNIST, CelebA, and Shakespeare using user-independent
validation with non-iid data distribution. Our results show that TurboSVM-FL
can significantly outperform existing popular algorithms on convergence rate
and reduce communication rounds while delivering better test metrics including
accuracy, F1 score, and MCC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the AAAI Conference on Artificial Intelligence 2024
  (AAAI'24)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating federated learning contribution allocation instability
  through randomized aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.08044v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.08044v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arno Geimer, Beltran Fiz, Radu State
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is a collaborative and privacy-preserving Machine
Learning paradigm, allowing the development of robust models without the need
to centralise sensitive data. A critical challenge in FL lies in fairly and
accurately allocating contributions from diverse participants. Inaccurate
allocation can undermine trust, lead to unfair compensation, and thus
participants may lack the incentive to join or actively contribute to the
federation.
  Various remuneration strategies have been proposed to date, including
auction-based approaches and Shapley-value based methods, the latter offering a
means to quantify the contribution of each participant. However, little to no
work has studied the stability of these contribution evaluation methods.
  In this paper, we focus on calculating contributions using gradient-based
model reconstruction techniques with Shapley values. We first show that
baseline Shapley values do not accurately reflect clients' contributions,
leading to unstable reward allocations amongst participants in a cross-silo
federation. We then introduce \textsc{FedRandom}, a new method that mitigates
these shortcomings with additional data samplings, and show its efficacy at
increasing the stability of contribution evaluation in federated learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Arbitrary Polynomial Separations in Trainable Quantum Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric R. Anschuetz, Xun Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent theoretical results in quantum machine learning have demonstrated a
general trade-off between the expressive power of quantum neural networks
(QNNs) and their trainability; as a corollary of these results, practical
exponential separations in expressive power over classical machine learning
models are believed to be infeasible as such QNNs take a time to train that is
exponential in the model size. We here circumvent these negative results by
constructing a hierarchy of efficiently trainable QNNs that exhibit
unconditionally provable, polynomial memory separations of arbitrary constant
degree over classical neural networks -- including state-of-the-art models,
such as Transformers -- in performing a classical sequence modeling task. This
construction is also computationally efficient, as each unit cell of the
introduced class of QNNs only has constant gate complexity. We show that
contextuality -- informally, a quantitative notion of semantic ambiguity -- is
the source of the expressivity separation, suggesting that other learning tasks
with this property may be a natural setting for the use of quantum learning
algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 3 figures, strengthened and simplified results and
  presentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimized Gradient Clipping for Noisy Label Learning <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08941v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08941v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xichen Ye, Yifan Wu, Weizhong Zhang, Xiaoqiang Li, Yifan Chen, Cheng Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous research has shown that constraining the gradient of loss function
with respect to model-predicted probabilities can enhance the model robustness
against noisy labels. These methods typically specify a fixed optimal threshold
for gradient clipping through validation data to obtain the desired robustness
against noise. However, this common practice overlooks the dynamic distribution
of gradients from both clean and noisy-labeled samples at different stages of
training, significantly limiting the model capability to adapt to the variable
nature of gradients throughout the training process. To address this issue, we
propose a simple yet effective approach called Optimized Gradient Clipping
(OGC), which dynamically adjusts the clipping threshold based on the ratio of
noise gradients to clean gradients after clipping, estimated by modeling the
distributions of clean and noisy samples. This approach allows us to modify the
clipping threshold at each training step, effectively controlling the influence
of noise gradients. Additionally, we provide statistical analysis to certify
the noise-tolerance ability of OGC. Our extensive experiments across various
types of label noise, including symmetric, asymmetric, instance-dependent, and
real-world noise, demonstrate the effectiveness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Clustering of timed sequences -- Application to the analysis of care
  pathways 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15379v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15379v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Guyet, Pierre Pinson, Enoal Gesny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Improving the future of healthcare starts by better understanding the current
actual practices in hospital settings. This motivates the objective of
discovering typical care pathways from patient data. Revealing typical care
pathways can be achieved through clustering. The difficulty in clustering care
pathways, represented by sequences of timestamped events, lies in defining a
semantically appropriate metric and clustering algorithms. In this article, we
adapt two methods developed for time series to the clustering of timed
sequences: the drop-DTW metric and the DBA approach for the construction of
averaged time sequences. These methods are then applied in clustering
algorithms to propose original and sound clustering algorithms for timed
sequences. This approach is experimented with and evaluated on synthetic and
real-world data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Task Adaptation of Reinforcement Learning-based NAS Agents through
  Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01420v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01420v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amber Cassimon, Siegfried Mercelis, Kevin Mets
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, a novel paradigm has been proposed for reinforcement learning-based
NAS agents, that revolves around the incremental improvement of a given
architecture. We assess the abilities of such reinforcement learning agents to
transfer between different tasks. We perform our evaluation using the
Trans-NASBench-101 benchmark, and consider the efficacy of the transferred
agents, as well as how quickly they can be trained. We find that pretraining an
agent on one task benefits the performance of the agent in another task in all
but 1 task when considering final performance. We also show that the training
procedure for an agent can be shortened significantly by pretraining it on
another task. Our results indicate that these effects occur regardless of the
source or target task, although they are more pronounced for some tasks than
for others. Our results show that transfer learning can be an effective tool in
mitigating the computational cost of the initial training procedure for
reinforcement learning-based NAS agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 Pages, 13 Figures, Corrected data in Figure 5</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Samudra: An AI Global Ocean Emulator for Climate 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Surya Dheeshjith, Adam Subel, Alistair Adcroft, Julius Busecke, Carlos Fernandez-Granda, Shubham Gupta, Laure Zanna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI emulators for forecasting have emerged as powerful tools that can
outperform conventional numerical predictions. The next frontier is to build
emulators for long climate simulations with skill across a range of
spatiotemporal scales, a particularly important goal for the ocean. Our work
builds a skillful global emulator of the ocean component of a state-of-the-art
climate model. We emulate key ocean variables, sea surface height, horizontal
velocities, temperature, and salinity, across their full depth. We use a
modified ConvNeXt UNet architecture trained on multidepth levels of ocean data.
We show that the ocean emulator - Samudra - which exhibits no drift relative to
the truth, can reproduce the depth structure of ocean variables and their
interannual variability. Samudra is stable for centuries and 150 times faster
than the original ocean model. Samudra struggles to capture the correct
magnitude of the forcing trends and simultaneously remains stable, requiring
further work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Ethereum Fraud Detection via Generative and Contrastive
  Self-supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.00641v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.00641v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxiang Jin, Jiajun Zhou, Chenxuan Xie, Shanqing Yu, Qi Xuan, Xiaoniu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rampant fraudulent activities on Ethereum hinder the healthy development
of the blockchain ecosystem, necessitating the reinforcement of regulations.
However, multiple imbalances involving account interaction frequencies and
interaction types in the Ethereum transaction environment pose significant
challenges to data mining-based fraud detection research. To address this, we
first propose the concept of meta-interactions to refine interaction behaviors
in Ethereum, and based on this, we present a dual self-supervision enhanced
Ethereum fraud detection framework, named Meta-IFD. This framework initially
introduces a generative self-supervision mechanism to augment the interaction
features of accounts, followed by a contrastive self-supervision mechanism to
differentiate various behavior patterns, and ultimately characterizes the
behavioral representations of accounts and mines potential fraud risks through
multi-view interaction feature learning. Extensive experiments on real Ethereum
datasets demonstrate the effectiveness and superiority of our framework in
detecting common Ethereum fraud behaviors such as Ponzi schemes and phishing
scams. Additionally, the generative module can effectively alleviate the
interaction distribution imbalance in Ethereum data, while the contrastive
module significantly enhances the framework's ability to distinguish different
behavior patterns. The source code will be available in
https://github.com/GISec-Team/Meta-IFD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Information Forensics & Security</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SageAttention2: Efficient Attention with Thorough Outlier Smoothing and
  Per-thread INT4 Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10958v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10958v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, Jianfei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although quantization for linear layers has been widely used, its application
to accelerate the attention process remains limited. To further enhance the
efficiency of attention computation compared to SageAttention while maintaining
precision, we propose SageAttention2, which utilizes significantly faster 4-bit
matrix multiplication (Matmul) alongside additional precision-enhancing
techniques. First, we propose to quantize matrixes $(Q, K)$ to INT4 in a
hardware-friendly thread-level granularity and quantize matrixes $(\widetilde
P, V)$ to FP8. Second, we propose a method to smooth $Q$, enhancing the
accuracy of INT4 $QK$. Third, we propose to use an FP32 Matmul buffer for $PV$
to enhance the accuracy of FP8 $\widetilde PV$. The operations per second (OPS)
of SageAttention2 surpass FlashAttention2 and xformers by about 3x and 5x on
RTX4090, respectively. Comprehensive experiments confirm that our approach
incurs negligible end-to-end metrics loss across diverse models, including
those for large language processing, image generation, and video generation.
The codes are available at https://github.com/thu-ml/SageAttention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking Large Language Models for Math Reasoning Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10839v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10839v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kathrin Seßler, Yao Rong, Emek Gözlüklü, Enkelejda Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of Large Language Models (LLMs) in mathematical reasoning has become
a cornerstone of related research, demonstrating the intelligence of these
models and enabling potential practical applications through their advanced
performance, such as in educational settings. Despite the variety of datasets
and in-context learning algorithms designed to improve the ability of LLMs to
automate mathematical problem solving, the lack of comprehensive benchmarking
across different datasets makes it complicated to select an appropriate model
for specific tasks. In this project, we present a benchmark that fairly
compares seven state-of-the-art in-context learning algorithms for mathematical
problem solving across five widely used mathematical datasets on four powerful
foundation models. Furthermore, we explore the trade-off between efficiency and
performance, highlighting the practical applications of LLMs for mathematical
reasoning. Our results indicate that larger foundation models like GPT-4o and
LLaMA 3-70B can solve mathematical reasoning independently from the concrete
prompting strategy, while for smaller models the in-context learning approach
significantly influences the performance. Moreover, the optimal prompt depends
on the chosen foundation model. We open-source our benchmark code to support
the integration of additional models in future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Real-Time Damage Detection in Fiber Lifting Ropes Using Lightweight
  Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.11947v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.11947v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuomas Jalonen, Mohammad Al-Sa'd, Roope Mellanen, Serkan Kiranyaz, Moncef Gabbouj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The health and safety hazards posed by worn crane lifting ropes mandate
periodic inspection for damage. This task is time-consuming, prone to human
error, halts operation, and may result in the premature disposal of ropes.
Therefore, we propose using efficient deep learning and computer vision methods
to automate the process of detecting damaged ropes. Specifically, we present a
vision-based system for detecting damage in synthetic fiber rope images using
lightweight convolutional neural networks. We develop a camera-based apparatus
to photograph the lifting rope's surface, while in operation, and capture the
progressive wear-and-tear as well as the more significant degradation in the
rope's health state. Experts from Konecranes annotate the collected images in
accordance with the rope's condition; normal or damaged. Then, we pre-process
the images, systematically design a deep learning model, evaluate its detection
and prediction performance, analyze its computational complexity, and compare
it with various other models. Experimental results show the proposed model
outperforms other similar techniques with 96.5% accuracy, 94.8% precision,
98.3% recall, 96.5% F1-score, and 99.3% AUC. Besides, they demonstrate the
model's real-time operation, low memory footprint, robustness to various
environmental and operational conditions, and adequacy for deployment in
industrial applications such as lifting, mooring, towing, climbing, and
sailing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Laws for Imitation Learning in Single-Agent Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2307.09423v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2307.09423v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jens Tuyls, Dhruv Madeka, Kari Torkkola, Dean Foster, Karthik Narasimhan, Sham Kakade
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation Learning (IL) is one of the most widely used methods in machine
learning. Yet, many works find it is often unable to fully recover the
underlying expert behavior, even in constrained environments like single-agent
games. However, none of these works deeply investigate the role of scaling up
the model and data size. Inspired by recent work in Natural Language Processing
(NLP) where "scaling up" has resulted in increasingly more capable LLMs, we
investigate whether carefully scaling up model and data size can bring similar
improvements in the imitation learning setting for single-agent games. We first
demonstrate our findings on a variety of Atari games, and thereafter focus on
the extremely challenging game of NetHack. In all games, we find that IL loss
and mean return scale smoothly with the compute budget (FLOPs) and are strongly
correlated, resulting in power laws for training compute-optimal IL agents.
Finally, we forecast and train several NetHack agents with IL and find they
outperform prior state-of-the-art by 1.5x in all settings. Our work both
demonstrates the scaling behavior of imitation learning in a variety of
single-agent games, as well as the viability of scaling up current approaches
for increasingly capable agents in NetHack, a game that remains elusively hard
for current AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at TMLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Union-over-Intersections: Object Detection beyond Winner-Takes-All 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.18512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.18512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aritra Bhowmik, Pascal Mettes, Martin R. Oswald, Cees G. M. Snoek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper revisits the problem of predicting box locations in object
detection architectures. Typically, each box proposal or box query aims to
directly maximize the intersection-over-union score with the ground truth,
followed by a winner-takes-all non-maximum suppression where only the highest
scoring box in each region is retained. We observe that both steps are
sub-optimal: the first involves regressing proposals to the entire ground
truth, which is a difficult task even with large receptive fields, and the
second neglects valuable information from boxes other than the top candidate.
Instead of regressing proposals to the whole ground truth, we propose a simpler
approach: regress only to the area of intersection between the proposal and the
ground truth. This avoids the need for proposals to extrapolate beyond their
visual scope, improving localization accuracy. Rather than adopting a
winner-takes-all strategy, we take the union over the regressed intersections
of all boxes in a region to generate the final box outputs. Our plug-and-play
method integrates seamlessly into proposal-based, grid-based, and query-based
detection architectures with minimal modifications, consistently improving
object localization and instance segmentation. We demonstrate its broad
applicability and versatility across various detection and segmentation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online MDP with Transition Prototypes: A Robust Adaptive Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14075v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14075v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Sun, Meng Qi, Zuo-Jun Max Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we consider an online robust Markov Decision Process (MDP)
where we have the information of finitely many prototypes of the underlying
transition kernel. We consider an adaptively updated ambiguity set of the
prototypes and propose an algorithm that efficiently identifies the true
underlying transition kernel while guaranteeing the performance of the
corresponding robust policy. To be more specific, we provide a sublinear regret
of the subsequent optimal robust policy. We also provide an early stopping
mechanism and a worst-case performance bound of the value function. In
numerical experiments, we demonstrate that our method outperforms existing
approaches, particularly in the early stage with limited data. This work
contributes to robust MDPs by considering possible prior information about the
underlying transition probability and online learning, offering both
theoretical insights and practical algorithms for improved decision-making
under uncertainty.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Parameters Reveal More than Loss for Membership Inference? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11544v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11544v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anshuman Suri, Xiao Zhang, David Evans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Membership inference attacks are used as a key tool for disclosure auditing.
They aim to infer whether an individual record was used to train a model. While
such evaluations are useful to demonstrate risk, they are computationally
expensive and often make strong assumptions about potential adversaries' access
to models and training environments, and thus do not provide tight bounds on
leakage from potential attacks. We show how prior claims around black-box
access being sufficient for optimal membership inference do not hold for
stochastic gradient descent, and that optimal membership inference indeed
requires white-box access. Our theoretical results lead to a new white-box
inference attack, IHA (Inverse Hessian Attack), that explicitly uses model
parameters by taking advantage of computing inverse-Hessian vector products.
Our results show that both auditors and adversaries may be able to benefit from
access to model parameters, and we advocate for further research into white-box
methods for membership inference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Transactions on Machine Learning Research (TMLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hybridization of Persistent Homology with Neural Networks for
  Time-Series Prediction: A Case Study in Wave Height 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01519v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01519v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixin Lin, Nur Fariha Syaqina Zulkepli, Mohd Shareduwan Mohd Kasihmuddin, R. U. Gobithaasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time-series prediction is an active area of research across various fields,
often challenged by the fluctuating influence of short-term and long-term
factors. In this study, we introduce a feature engineering method that enhances
the predictive performance of neural network models. Specifically, we leverage
computational topology techniques to derive valuable topological features from
input data, boosting the predictive accuracy of our models. Our focus is on
predicting wave heights, utilizing models based on topological features within
feedforward neural networks (FNNs), recurrent neural networks (RNNs), long
short-term memory networks (LSTM), and RNNs with gated recurrent units (GRU).
For time-ahead predictions, the enhancements in $R^2$ score were significant
for FNNs, RNNs, LSTM, and GRU models. Additionally, these models also showed
significant reductions in maximum errors and mean squared errors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>the paper contain errors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AndroidWorld: A Dynamic Benchmarking Environment for Autonomous Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14573v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14573v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, Daniel Toyama, Robert Berry, Divya Tyamagundlu, Timothy Lillicrap, Oriana Riva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents that execute human tasks by controlling computers can
enhance human productivity and application accessibility. However, progress in
this field will be driven by realistic and reproducible benchmarks. We present
AndroidWorld, a fully functional Android environment that provides reward
signals for 116 programmatic tasks across 20 real-world Android apps. Unlike
existing interactive environments, which provide a static test set,
AndroidWorld dynamically constructs tasks that are parameterized and expressed
in natural language in unlimited ways, thus enabling testing on a much larger
and more realistic suite of tasks. To ensure reproducibility, each task
includes dedicated initialization, success-checking, and tear-down logic, which
modifies and inspects the device's system state. We experiment with baseline
agents to test AndroidWorld and provide initial results on the benchmark. Our
best agent can complete 30.6% of AndroidWorld's tasks, leaving ample room for
future work. Furthermore, we adapt a popular desktop web agent to work on
Android, which we find to be less effective on mobile, suggesting future
research is needed to achieve universal, cross-platform agents. Finally, we
also conduct a robustness analysis, showing that task variations can
significantly affect agent performance, demonstrating that without such
testing, agent performance metrics may not fully reflect practical challenges.
AndroidWorld and the experiments in this paper are available at
github.com/google-research/android_world.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Erase then Rectify: A Training-Free Parameter Editing Approach for
  Cost-Effective Graph Unlearning <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16684v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16684v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe-Rui Yang, Jindong Han, Chang-Dong Wang, Hao Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph unlearning, which aims to eliminate the influence of specific nodes,
edges, or attributes from a trained Graph Neural Network (GNN), is essential in
applications where privacy, bias, or data obsolescence is a concern. However,
existing graph unlearning techniques often necessitate additional training on
the remaining data, leading to significant computational costs, particularly
with large-scale graphs. To address these challenges, we propose a two-stage
training-free approach, Erase then Rectify (ETR), designed for efficient and
scalable graph unlearning while preserving the model utility. Specifically, we
first build a theoretical foundation showing that masking parameters critical
for unlearned samples enables effective unlearning. Building on this insight,
the Erase stage strategically edits model parameters to eliminate the impact of
unlearned samples and their propagated influence on intercorrelated nodes. To
further ensure the GNN's utility, the Rectify stage devises a gradient
approximation method to estimate the model's gradient on the remaining dataset,
which is then used to enhance model performance. Overall, ETR achieves graph
unlearning without additional training or full training data access,
significantly reducing computational overhead and preserving data privacy.
Extensive experiments on seven public datasets demonstrate the consistent
superiority of ETR in model utility, unlearning efficiency, and unlearning
effectiveness, establishing it as a promising solution for real-world graph
unlearning challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ASTM :Autonomous Smart Traffic Management System Using Artificial
  Intelligence CNN and LSTM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10929v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10929v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christofel Rio Goenawan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the modern world, the development of Artificial Intelligence (AI) has
contributed to improvements in various areas, including automation, computer
vision, fraud detection, and more. AI can be leveraged to enhance the
efficiency of Autonomous Smart Traffic Management (ASTM) systems and reduce
traffic congestion rates. This paper presents an Autonomous Smart Traffic
Management (STM) system that uses AI to improve traffic flow rates. The system
employs the YOLO V5 Convolutional Neural Network to detect vehicles in traffic
management images. Additionally, it predicts the number of vehicles for the
next 12 hours using a Recurrent Neural Network with Long Short-Term Memory
(RNN-LSTM). The Smart Traffic Management Cycle Length Analysis manages the
traffic cycle length based on these vehicle predictions, aided by AI. From the
results of the RNN-LSTM model for predicting vehicle numbers over the next 12
hours, we observe that the model predicts traffic with a Mean Squared Error
(MSE) of 4.521 vehicles and a Root Mean Squared Error (RMSE) of 2.232 vehicles.
After simulating the STM system in the CARLA simulation environment, we found
that the Traffic Management Congestion Flow Rate with ASTM (21 vehicles per
minute) is 50\% higher than the rate without STM (around 15 vehicles per
minute). Additionally, the Traffic Management Vehicle Pass Delay with STM (5
seconds per vehicle) is 70\% lower than without STM (around 12 seconds per
vehicle). These results demonstrate that the STM system using AI can increase
traffic flow by 50\% and reduce vehicle pass delays by 70\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In process to IEEE Intelligent Vehicle Symposium 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sum of Squares Circuits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11778v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11778v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Loconte, Stefan Mengel, Antonio Vergari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing expressive generative models that support exact and efficient
inference is a core question in probabilistic ML. Probabilistic circuits (PCs)
offer a framework where this tractability-vs-expressiveness trade-off can be
analyzed theoretically. Recently, squared PCs encoding subtractive mixtures via
negative parameters have emerged as tractable models that can be exponentially
more expressive than monotonic PCs, i.e., PCs with positive parameters only. In
this paper, we provide a more precise theoretical characterization of the
expressiveness relationships among these models. First, we prove that squared
PCs can be less expressive than monotonic ones. Second, we formalize a novel
class of PCs -- sum of squares PCs -- that can be exponentially more expressive
than both squared and monotonic PCs. Around sum of squares PCs, we build an
expressiveness hierarchy that allows us to precisely unify and separate
different tractable model classes such as Born Machines and PSD models, and
other recently introduced tractable probabilistic models by using complex
parameters. Finally, we empirically show the effectiveness of sum of squares
circuits in performing distribution estimation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How to Re-enable PDE Loss for Physical Systems Modeling Under Partial
  Observation <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09116v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09116v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haodong Feng, Yue Wang, Dixia Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In science and engineering, machine learning techniques are increasingly
successful in physical systems modeling (predicting future states of physical
systems). Effectively integrating PDE loss as a constraint of system transition
can improve the model's prediction by overcoming generalization issues due to
data scarcity, especially when data acquisition is costly. However, in many
real-world scenarios, due to sensor limitations, the data we can obtain is
often only partial observation, making the calculation of PDE loss seem to be
infeasible, as the PDE loss heavily relies on high-resolution states. We
carefully study this problem and propose a novel framework named Re-enable PDE
Loss under Partial Observation (RPLPO). The key idea is that although enabling
PDE loss to constrain system transition solely is infeasible, we can re-enable
PDE loss by reconstructing the learnable high-resolution state and constraining
system transition simultaneously. Specifically, RPLPO combines an encoding
module for reconstructing learnable high-resolution states with a transition
module for predicting future states. The two modules are jointly trained by
data and PDE loss. We conduct experiments in various physical systems to
demonstrate that RPLPO has significant improvement in generalization, even when
observation is sparse, irregular, noisy, and PDE is inaccurate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward Falsifying Causal Graphs Using a Permutation-Based Test <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.09565v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.09565v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elias Eulig, Atalanti A. Mastakouri, Patrick Blöbaum, Michaela Hardt, Dominik Janzing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding causal relationships among the variables of a system is
paramount to explain and control its behavior. For many real-world systems,
however, the true causal graph is not readily available and one must resort to
predictions made by algorithms or domain experts. Therefore, metrics that
quantitatively assess the goodness of a causal graph provide helpful checks
before using it in downstream tasks. Existing metrics provide an
$\textit{absolute}$ number of inconsistencies between the graph and the
observed data, and without a baseline, practitioners are left to answer the
hard question of how many such inconsistencies are acceptable or expected.
Here, we propose a novel consistency metric by constructing a baseline through
node permutations. By comparing the number of inconsistencies with those on the
baseline, we derive an interpretable metric that captures whether the graph is
significantly better than random. Evaluating on both simulated and real data
sets from various domains, including biology and cloud monitoring, we
demonstrate that the true graph is not falsified by our metric, whereas the
wrong graphs given by a hypothetical user are likely to be falsified.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version for AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovering Continuous-Time Memory-Based Symbolic Policies using Genetic
  Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02765v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02765v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sigur de Vries, Sander Keemink, Marcel van Gerven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence techniques are increasingly being applied to solve
control problems, but often rely on black-box methods without transparent
output generation. To improve the interpretability and transparency in control
systems, models can be defined as white-box symbolic policies described by
mathematical expressions. For better performance in partially observable and
volatile environments, the symbolic policies are extended with memory
represented by continuous-time latent variables, governed by differential
equations. Genetic programming is used for optimisation, resulting in
interpretable policies consisting of symbolic expressions. Our results show
that symbolic policies with memory compare with black-box policies on a variety
of control tasks. Furthermore, the benefit of the memory in symbolic policies
is demonstrated on experiments where memory-less policies fall short. Overall,
we present a method for evolving high-performing symbolic policies that offer
interpretability and transparency, which lacks in black-box models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages including references and appendix, 5 figures, 1 algorithm, 5
  tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shape error prediction in 5-axis machining using graph neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10341v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10341v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julia Huuk, Abheek Dhingra, Eirini Ntoutsi, Berend Denkena
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an innovative method for predicting shape errors in
5-axis machining using graph neural networks. The graph structure is defined
with nodes representing workpiece surface points and edges denoting the
neighboring relationships. The dataset encompasses data from a material removal
simulation, process data, and post-machining quality information. Experimental
results show that the presented approach can generalize the shape error
prediction for the investigated workpiece geometry. Moreover, by modelling
spatial and temporal connections within the workpiece, the approach handles a
low number of labels compared to non-graphical methods such as Support Vector
Machines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TRAIL: Trust-Aware Client Scheduling for Semi-Decentralized Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11448v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11448v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gangqiang Hu, Jianfeng Lu, Jianmin Han, Shuqin Cao, Jing Liu, Hao Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the sensitivity of data, Federated Learning (FL) is employed to enable
distributed machine learning while safeguarding data privacy and accommodating
the requirements of various devices. However, in the context of
semi-decentralized FL, clients' communication and training states are dynamic.
This variability arises from local training fluctuations, heterogeneous data
distributions, and intermittent client participation. Most existing studies
primarily focus on stable client states, neglecting the dynamic challenges
inherent in real-world scenarios. To tackle this issue, we propose a
TRust-Aware clIent scheduLing mechanism called TRAIL, which assesses client
states and contributions, enhancing model training efficiency through selective
client participation. We focus on a semi-decentralized FL framework where edge
servers and clients train a shared global model using unreliable intra-cluster
model aggregation and inter-cluster model consensus. First, we propose an
adaptive hidden semi-Markov model to estimate clients' communication states and
contributions. Next, we address a client-server association optimization
problem to minimize global training loss. Using convergence analysis, we
propose a greedy client scheduling algorithm. Finally, our experiments
conducted on real-world datasets demonstrate that TRAIL outperforms
state-of-the-art baselines, achieving an improvement of 8.7% in test accuracy
and a reduction of 15.3% in training loss.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating Diffusion <span class="highlight-title">Transformer</span>s with Token-wise Feature Caching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05317v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05317v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Zou, Xuyang Liu, Ting Liu, Siteng Huang, Linfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion transformers have shown significant effectiveness in both image and
video synthesis at the expense of huge computation costs. To address this
problem, feature caching methods have been introduced to accelerate diffusion
transformers by caching the features in previous timesteps and reusing them in
the following timesteps. However, previous caching methods ignore that
different tokens exhibit different sensitivities to feature caching, and
feature caching on some tokens may lead to 10$\times$ more destruction to the
overall generation quality compared with other tokens. In this paper, we
introduce token-wise feature caching, allowing us to adaptively select the most
suitable tokens for caching, and further enable us to apply different caching
ratios to neural layers in different types and depths. Extensive experiments on
PixArt-$\alpha$, OpenSora, and DiT demonstrate our effectiveness in both image
and video generation with no requirements for training. For instance,
2.36$\times$ and 1.93$\times$ acceleration are achieved on OpenSora and
PixArt-$\alpha$ with almost no drop in generation quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In this version, we achieved a nearly lossless acceleration of 1.51
  times for ToCa on FLUX in the appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probability Distribution Learning and Its Application in Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05666v9">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05666v9.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binchuan Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a novel theoretical learning framework, termed
probability distribution learning (PD learning). Departing from the traditional
statistical learning framework, PD learning focuses on learning the underlying
probability distribution, which is modeled as a random variable within the
probability simplex. In this framework, the optimization objective is the
learning error, which quantifies the posterior expected discrepancy between the
model's predicted distribution and the underlying true distribution, given
available sample data and prior knowledge. To optimize the learning error, this
paper proposes the necessary conditions for loss functions, models, and
optimization algorithms, ensuring that these conditions are met in real-world
machine learning scenarios. Based on these conditions, the non-convex
optimization mechanism corresponding to model training can be theoretically
resolved. Moreover, this paper provides model-dependent and model-independent
bounds on learning error, offering new insights into the model's fitting and
generalization capabilities. Furthermore, the paper applies the PD learning
framework to elucidate the mechanisms by which various techniques, including
random parameter initialization, over-parameterization, and dropout, influence
deep model training. Finally, the paper substantiates the key conclusions of
the proposed framework through experimental results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2105.04026 by other
  authors. arXiv admin note: text overlap with arXiv:2105.04026 by other
  authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Score and Distribution Matching Policy: Advanced Accelerated Visuomotor
  Policies via Matched Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09265v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09265v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bofang Jia, Pengxiang Ding, Can Cui, Mingyang Sun, Pengfang Qian, Siteng Huang, Zhaoxin Fan, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual-motor policy learning has advanced with architectures like
diffusion-based policies, known for modeling complex robotic trajectories.
However, their prolonged inference times hinder high-frequency control tasks
requiring real-time feedback. While consistency distillation (CD) accelerates
inference, it introduces errors that compromise action quality. To address
these limitations, we propose the Score and Distribution Matching Policy (SDM
Policy), which transforms diffusion-based policies into single-step generators
through a two-stage optimization process: score matching ensures alignment with
true action distributions, and distribution matching minimizes KL divergence
for consistency. A dual-teacher mechanism integrates a frozen teacher for
stability and an unfrozen teacher for adversarial training, enhancing
robustness and alignment with target distributions. Evaluated on a 57-task
simulation benchmark, SDM Policy achieves a 6x inference speedup while having
state-of-the-art action quality, providing an efficient and reliable framework
for high-frequency robotic tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Expressivity of Persistent Homology in Graph Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09826v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09826v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rubén Ballester, Bastian Rieck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Persistent homology, a technique from computational topology, has recently
shown strong empirical performance in the context of graph classification.
Being able to capture long range graph properties via higher-order topological
features, such as cycles of arbitrary length, in combination with multi-scale
topological descriptors, has improved predictive performance for data sets with
prominent topological structures, such as molecules. At the same time, the
theoretical properties of persistent homology have not been formally assessed
in this context. This paper intends to bridge the gap between computational
topology and graph machine learning by providing a brief introduction to
persistent homology in the context of graphs, as well as a theoretical
discussion and empirical analysis of its expressivity for graph learning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 3rd Learning on Graphs Conference (LoG) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalized Encouragement-Based Instrumental Variables for
  Counterfactual Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anpeng Wu, Kun Kuang, Ruoxuan Xiong, Xiangwei Chen, Zexu Sun, Fei Wu, Kun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In causal inference, encouragement designs (EDs) are widely used to analyze
causal effects, when randomized controlled trials (RCTs) are impractical or
compliance to treatment cannot be perfectly enforced. Unlike RCTs, which
directly allocate treatments, EDs randomly assign encouragement policies that
positively motivate individuals to engage in a specific treatment. These random
encouragements act as instrumental variables (IVs), facilitating the
identification of causal effects through leveraging exogenous perturbations in
discrete treatment scenarios. However, real-world applications of encouragement
designs often face challenges such as incomplete randomization, limited
experimental data, and significantly fewer encouragements compared to
treatments, hindering precise causal effect estimation. To address this, this
paper introduces novel theories and algorithms for identifying the Conditional
Average Treatment Effect (CATE) using variations in encouragement. Further, by
leveraging both observational and encouragement data, we propose a generalized
IV estimator, named Encouragement-based Counterfactual Regression (EnCounteR),
to effectively estimate the causal effects. Extensive experiments on both
synthetic and real-world datasets demonstrate the superiority of EnCounteR over
existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smoothness Really Matters: A Simple Yet Effective Approach for
  Unsupervised Graph Domain Adaptation <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11654v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11654v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Chen, Guo Ye, Yakun Wang, Zhao Zhang, Libang Zhang, Daxin Wang, Zhiqiang Zhang, Fuzhen Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised Graph Domain Adaptation (UGDA) seeks to bridge distribution
shifts between domains by transferring knowledge from labeled source graphs to
given unlabeled target graphs. Existing UGDA methods primarily focus on
aligning features in the latent space learned by graph neural networks (GNNs)
across domains, often overlooking structural shifts, resulting in limited
effectiveness when addressing structurally complex transfer scenarios. Given
the sensitivity of GNNs to local structural features, even slight discrepancies
between source and target graphs could lead to significant shifts in node
embeddings, thereby reducing the effectiveness of knowledge transfer. To
address this issue, we introduce a novel approach for UGDA called Target-Domain
Structural Smoothing (TDSS). TDSS is a simple and effective method designed to
perform structural smoothing directly on the target graph, thereby mitigating
structural distribution shifts and ensuring the consistency of node
representations. Specifically, by integrating smoothing techniques with
neighborhood sampling, TDSS maintains the structural coherence of the target
graph while mitigating the risk of over-smoothing. Our theoretical analysis
shows that TDSS effectively reduces target risk by improving model smoothness.
Empirical results on three real-world datasets demonstrate that TDSS
outperforms recent state-of-the-art baselines, achieving significant
improvements across six transfer scenarios. The code is available in
https://github.com/cwei01/TDSS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, Accpected by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaSymNet: A Tree-like Symbol Network with Adaptive Architecture and
  Activation Functions <span class="chip">AAAI2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07326v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07326v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjie Li, Weijun Li, Lina Yu, Min Wu, Jinyi Liu, Wenqiang Li, Meilan Hao, Shu Wei, Yusong Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical formulas serve as the means of communication between humans and
nature, encapsulating the operational laws governing natural phenomena. The
concise formulation of these laws is a crucial objective in scientific research
and an important challenge for artificial intelligence (AI). While traditional
artificial neural networks (MLP) excel at data fitting, they often yield
uninterpretable black box results that hinder our understanding of the
relationship between variables x and predicted values y. Moreover, the fixed
network architecture in MLP often gives rise to redundancy in both network
structure and parameters. To address these issues, we propose MetaSymNet, a
novel neural network that dynamically adjusts its structure in real-time,
allowing for both expansion and contraction. This adaptive network employs the
PANGU meta function as its activation function, which is a unique type capable
of evolving into various basic functions during training to compose
mathematical formulas tailored to specific needs. We then evolve the neural
network into a concise, interpretable mathematical expression. To evaluate
MetaSymNet's performance, we compare it with four state-of-the-art symbolic
regression algorithms across more than 10 public datasets comprising 222
formulas. Our experimental results demonstrate that our algorithm outperforms
others consistently regardless of noise presence or absence. Furthermore, we
assess MetaSymNet against MLP and SVM regarding their fitting ability and
extrapolation capability, these are two essential aspects of machine learning
algorithms. The findings reveal that our algorithm excels in both areas.
Finally, we compared MetaSymNet with MLP using iterative pruning in network
structure complexity. The results show that MetaSymNet's network structure
complexity is obviously less than MLP under the same goodness of fit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted by AAAI2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training <span class="highlight-title">Dataset</span>s Generation for Machine Learning: Application to Vision
  Based Navigation <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11383v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11383v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jérémy Lebreton, Ingo Ahrns, Roland Brochard, Christoph Haskamp, Hans Krüger, Matthieu Le Goff, Nicolas Menga, Nicolas Ollagnier, Ralf Regele, Francesco Capolupo, Massimo Casasco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Based Navigation consists in utilizing cameras as precision sensors
for GNC after extracting information from images. To enable the adoption of
machine learning for space applications, one of obstacles is the demonstration
that available training datasets are adequate to validate the algorithms. The
objective of the study is to generate datasets of images and metadata suitable
for training machine learning algorithms. Two use cases were selected and a
robust methodology was developed to validate the datasets including the ground
truth. The first use case is in-orbit rendezvous with a man-made object: a
mockup of satellite ENVISAT. The second use case is a Lunar landing scenario.
Datasets were produced from archival datasets (Chang'e 3), from the laboratory
at DLR TRON facility and at Airbus Robotic laboratory, from SurRender software
high fidelity image simulator using Model Capture and from Generative
Adversarial Networks. The use case definition included the selection of
algorithms as benchmark: an AI-based pose estimation algorithm and a dense
optical flow algorithm were selected. Eventually it is demonstrated that
datasets produced with SurRender and selected laboratory facilities are
adequate to train machine learning algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, preprint of the proceedings of ESA SPAICE
  conference 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature selection in linear SVMs via a hard cardinality constraint: a
  scalable SDP decomposition approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.10099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.10099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Immanuel Bomze, Federico D'Onofrio, Laura Palagi, Bo Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the embedded feature selection problem in linear
Support Vector Machines (SVMs), in which a cardinality constraint is employed,
leading to an interpretable classification model. The problem is NP-hard due to
the presence of the cardinality constraint, even though the original linear SVM
amounts to a problem solvable in polynomial time. To handle the hard problem,
we first introduce two mixed-integer formulations for which novel semidefinite
relaxations are proposed. Exploiting the sparsity pattern of the relaxations,
we decompose the problems and obtain equivalent relaxations in a much smaller
cone, making the conic approaches scalable. To make the best usage of the
decomposed relaxations, we propose heuristics using the information of its
optimal solution. Moreover, an exact procedure is proposed by solving a
sequence of mixed-integer decomposed semidefinite optimization problems.
Numerical results on classical benchmarking datasets are reported, showing the
efficiency and effectiveness of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to European Journal of Operational Research. arXiv admin
  note: text overlap with arXiv:1808.02435 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Acceleration for Classification-Based Derivative-Free
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.11036v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.11036v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Han, Jingya Li, Zhipeng Guo, Yuan Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Derivative-free optimization algorithms play an important role in scientific
and engineering design optimization problems, especially when derivative
information is not accessible. In this paper, we study the framework of
sequential classification-based derivative-free optimization algorithms. By
introducing learning theoretic concept hypothesis-target shattering rate, we
revisit the computational complexity upper bound of SRACOS (Hu, Qian, and Yu
2017). Inspired by the revisited upper bound, we propose an algorithm named
RACE-CARS, which adds a random region-shrinking step compared with SRACOS. We
further establish theorems showing the acceleration by region shrinking.
Experiments on the synthetic functions as well as black-box tuning for
language-model-as-a-service demonstrate empirically the efficiency of
RACE-CARS. An ablation experiment on the introduced hyperparameters is also
conducted, revealing the mechanism of RACE-CARS and putting forward an
empirical hyper-parameter tuning guidance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guiding a Diffusion Model with a Bad Version of Itself <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.02507v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.02507v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tero Karras, Miika Aittala, Tuomas Kynkäänniemi, Jaakko Lehtinen, Timo Aila, Samuli Laine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The primary axes of interest in image-generating diffusion models are image
quality, the amount of variation in the results, and how well the results align
with a given condition, e.g., a class label or a text prompt. The popular
classifier-free guidance approach uses an unconditional model to guide a
conditional model, leading to simultaneously better prompt alignment and
higher-quality images at the cost of reduced variation. These effects seem
inherently entangled, and thus hard to control. We make the surprising
observation that it is possible to obtain disentangled control over image
quality without compromising the amount of variation by guiding generation
using a smaller, less-trained version of the model itself rather than an
unconditional model. This leads to significant improvements in ImageNet
generation, setting record FIDs of 1.01 for 64x64 and 1.25 for 512x512, using
publicly available networks. Furthermore, the method is also applicable to
unconditional diffusion models, drastically improving their quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TrimLLM: Progressive Layer Dropping for Domain-Specific LLMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11242v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11242v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lanxiang Hu, Tajana Rosing, Hao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Specializing large language models (LLMs) for local deployment in
domain-specific use cases is necessary for strong performance while meeting
latency and privacy constraints. However, conventional task-specific adaptation
approaches do not show simultaneous memory saving and inference speedup at
deployment time. Practical compression techniques like quantization and pruning
require dedicated hardware or kernel support to achieve measured inference
speedup. We develop TrimLLM based on the layer-wise specialization phenomenon
we empirically observed and verified on contemporary LLMs. TrimLLM reduces the
depth of LLMs via progressive layer dropping. We show it retains LLMs' capacity
in specific domains and achieves inference speedup irrespective of hardware and
deep learning frameworks. We evaluated TrimLLM on LLMs of various sizes for
inference; models adapted on medical, legal, and financial datasets all
demonstrate $2.1-5.7\times$ inference speedup on consumer GPUs and up to
$3.1\times$ speedup on A100 when compared to state-of-the-art model compression
algorithms, with no loss in accuracy at 50$\sim$60\% model compression ratio.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Deep Dissipative Dynamics <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11479v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11479v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuji Okamoto, Ryosuke Kojima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study challenges strictly guaranteeing ``dissipativity'' of a dynamical
system represented by neural networks learned from given time-series data.
Dissipativity is a crucial indicator for dynamical systems that generalizes
stability and input-output stability, known to be valid across various systems
including robotics, biological systems, and molecular dynamics. By analytically
proving the general solution to the nonlinear Kalman-Yakubovich-Popov (KYP)
lemma, which is the necessary and sufficient condition for dissipativity, we
propose a differentiable projection that transforms any dynamics represented by
neural networks into dissipative ones and a learning method for the transformed
dynamics. Utilizing the generality of dissipativity, our method strictly
guarantee stability, input-output stability, and energy conservation of trained
dynamical systems. Finally, we demonstrate the robustness of our method against
out-of-domain input through applications to robotic arms and fluid dynamics.
Code is https://github.com/kojima-r/DeepDissipativeModel
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grimm: A Plug-and-Play Perturbation Rectifier for Graph Neural Networks
  Defending against Poisoning Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08555v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08555v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ao Liu, Wenshan Li, Beibei Li, Wengang Ma, Tao Li, Pan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have revealed the vulnerability of graph neural networks
(GNNs) to adversarial poisoning attacks on node classification tasks. Current
defensive methods require substituting the original GNNs with defense models,
regardless of the original's type. This approach, while targeting adversarial
robustness, compromises the enhancements developed in prior research to boost
GNNs' practical performance. Here we introduce Grimm, the first plug-and-play
defense model. With just a minimal interface requirement for extracting
features from any layer of the protected GNNs, Grimm is thus enabled to
seamlessly rectify perturbations. Specifically, we utilize the feature
trajectories (FTs) generated by GNNs, as they evolve through epochs, to reflect
the training status of the networks. We then theoretically prove that the FTs
of victim nodes will inevitably exhibit discriminable anomalies. Consequently,
inspired by the natural parallelism between the biological nervous and immune
systems, we construct Grimm, a comprehensive artificial immune system for GNNs.
Grimm not only detects abnormal FTs and rectifies adversarial edges during
training but also operates efficiently in parallel, thereby mirroring the
concurrent functionalities of its biological counterparts. We experimentally
confirm that Grimm offers four empirically validated advantages: 1)
Harmlessness, as it does not actively interfere with GNN training; 2)
Parallelism, ensuring monitoring, detection, and rectification functions
operate independently of the GNN training process; 3) Generalizability,
demonstrating compatibility with mainstream GNNs such as GCN, GAT, and
GraphSAGE; and 4) Transferability, as the detectors for abnormal FTs can be
efficiently transferred across different systems for one-step rectification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Holdouts set for safe predictive model updating 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.06374v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.06374v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sami Haidar-Wehbe, Samuel R Emerson, Louis J M Aslett, James Liley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predictive risk scores for adverse outcomes are increasingly crucial in
guiding health interventions. Such scores may need to be periodically updated
due to change in the distributions they model. However, directly updating risk
scores used to guide intervention can lead to biased risk estimates. To address
this, we propose updating using a `holdout set' - a subset of the population
that does not receive interventions guided by the risk score. Balancing the
holdout set size is essential to ensure good performance of the updated risk
score whilst minimising the number of held out samples. We prove that this
approach reduces adverse outcome frequency to an asymptotically optimal level
and argue that often there is no competitive alternative. We describe
conditions under which an optimal holdout size (OHS) can be readily identified,
and introduce parametric and semi-parametric algorithms for OHS estimation. We
apply our methods to the ASPRE risk score for pre-eclampsia to recommend a plan
for updating it in the presence of change in the underlying data distribution.
We show that, in order to minimise the number of pre-eclampsia cases over time,
this is best achieved using a holdout set of around 10,000 individuals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Manuscript includes supplementary materials and figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RAZOR: Sharpening Knowledge by Cutting Bias with Unsupervised Text
  Rewriting <span class="chip">AAAI'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07675v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07675v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuo Yang, Bardh Prenkaj, Gjergji Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the widespread use of LLMs due to their superior performance in
various tasks, their high computational costs often lead potential users to opt
for the pretraining-finetuning pipeline. However, biases prevalent in manually
constructed datasets can introduce spurious correlations between tokens and
labels, creating so-called shortcuts and hindering the generalizability of
fine-tuned models. Existing debiasing methods often rely on prior knowledge of
specific dataset biases, which is challenging to acquire a priori. We propose
RAZOR (Rewriting And Zero-bias Optimization Refinement), a novel, unsupervised,
and data-focused debiasing approach based on text rewriting for shortcut
mitigation. RAZOR leverages LLMs to iteratively rewrite potentially biased text
segments by replacing them with heuristically selected alternatives in a
shortcut space defined by token statistics and positional information. This
process aims to align surface-level text features more closely with diverse
label distributions, thereby promoting the learning of genuine linguistic
patterns. Compared with unsupervised SoTA models, RAZOR improves by 3.5% on the
FEVER and 6.5% on MNLI and SNLI datasets according to the F1 score.
Additionally, RAZOR effectively mitigates specific known biases, reducing
bias-related terms by x2 without requiring prior bias information, a result
that is on par with SoTA models that leverage prior information. Our work
prioritizes data manipulation over architectural modifications, emphasizing the
pivotal role of data quality in enhancing model performance and fairness. This
research contributes to developing more robust evaluation benchmarks for
debiasing methods by incorporating metrics for bias reduction and overall model
efficacy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Shuo and Bardh contributed equally. Accepted to AAAI'25, Paper #17117</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DG-Mamba: Robust and Efficient Dynamic Graph Structure Learning with
  Selective State Space Models <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08160v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08160v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haonan Yuan, Qingyun Sun, Zhaonan Wang, Xingcheng Fu, Cheng Ji, Yongjian Wang, Bo Jin, Jianxin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic graphs exhibit intertwined spatio-temporal evolutionary patterns,
widely existing in the real world. Nevertheless, the structure incompleteness,
noise, and redundancy result in poor robustness for Dynamic Graph Neural
Networks (DGNNs). Dynamic Graph Structure Learning (DGSL) offers a promising
way to optimize graph structures. However, aside from encountering unacceptable
quadratic complexity, it overly relies on heuristic priors, making it hard to
discover underlying predictive patterns. How to efficiently refine the dynamic
structures, capture intrinsic dependencies, and learn robust representations,
remains under-explored. In this work, we propose the novel DG-Mamba, a robust
and efficient Dynamic Graph structure learning framework with the Selective
State Space Models (Mamba). To accelerate the spatio-temporal structure
learning, we propose a kernelized dynamic message-passing operator that reduces
the quadratic time complexity to linear. To capture global intrinsic dynamics,
we establish the dynamic graph as a self-contained system with State Space
Model. By discretizing the system states with the cross-snapshot graph
adjacency, we enable the long-distance dependencies capturing with the
selective snapshot scan. To endow learned dynamic structures more expressive
with informativeness, we propose the self-supervised Principle of Relevant
Information for DGSL to regularize the most relevant yet least redundant
information, enhancing global robustness. Extensive experiments demonstrate the
superiority of the robustness and efficiency of our DG-Mamba compared with the
state-of-the-art baselines against adversarial attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the Main Technical Track of the 39th Annual AAAI
  Conference on Artificial Intelligence (AAAI-2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ T-JEPA: Augmentation-Free <span class="highlight-title">Self-Supervised</span> Learning for Tabular Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05016v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05016v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hugo Thimonier, José Lucas De Melo Costa, Fabrice Popineau, Arpad Rimmel, Bich-Liên Doan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervision is often used for pre-training to foster performance on a
downstream task by constructing meaningful representations of samples.
Self-supervised learning (SSL) generally involves generating different views of
the same sample and thus requires data augmentations that are challenging to
construct for tabular data. This constitutes one of the main challenges of
self-supervision for structured data. In the present work, we propose a novel
augmentation-free SSL method for tabular data. Our approach, T-JEPA, relies on
a Joint Embedding Predictive Architecture (JEPA) and is akin to mask
reconstruction in the latent space. It involves predicting the latent
representation of one subset of features from the latent representation of a
different subset within the same sample, thereby learning rich representations
without augmentations. We use our method as a pre-training technique and train
several deep classifiers on the obtained representation. Our experimental
results demonstrate a substantial improvement in both classification and
regression tasks, outperforming models trained directly on samples in their
original data space. Moreover, T-JEPA enables some methods to consistently
outperform or match the performance of traditional methods likes Gradient
Boosted Decision Trees. To understand why, we extensively characterize the
obtained representations and show that T-JEPA effectively identifies relevant
features for downstream tasks without access to the labels. Additionally, we
introduce regularization tokens, a novel regularization method critical for
training of JEPA-based models on structured data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Group Classification with Descending Soft Labeling for Deep
  Imbalanced Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12327v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12327v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhi Pu, Gezheng Xu, Ruiyi Fang, Binkun Bao, Charles X. Ling, Boyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep imbalanced regression (DIR), where the target values have a highly
skewed distribution and are also continuous, is an intriguing yet
under-explored problem in machine learning.
  While recent works have already shown that incorporating various
classification-based regularizers can produce enhanced outcomes, the role of
classification remains elusive in DIR.
  Moreover, such regularizers (e.g., contrastive penalties) merely focus on
learning discriminative features of data, which inevitably results in ignorance
of either continuity or similarity across the data.
  To address these issues, we first bridge the connection between the
objectives of DIR and classification from a Bayesian perspective.
  Consequently, this motivates us to decompose the objective of DIR into a
combination of classification and regression tasks, which naturally guides us
toward a divide-and-conquer manner to solve the DIR problem.
  Specifically, by aggregating the data at nearby labels into the same groups,
we introduce an ordinal group-aware contrastive learning loss along with a
multi-experts regressor to tackle the different groups of data thereby
maintaining the data continuity.
  Meanwhile, considering the similarity between the groups, we also propose a
symmetric descending soft labeling strategy to exploit the intrinsic similarity
across the data, which allows classification to facilitate regression more
effectively.
  Extensive experiments on real-world datasets also validate the effectiveness
of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Langevin dynamics for high-dimensional optimization: the case of
  multi-spiked tensor PCA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.06401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.06401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gérard Ben Arous, Cédric Gerbelot, Vanessa Piccolo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study nonconvex optimization in high dimensions through Langevin dynamics,
focusing on the multi-spiked tensor PCA problem. This tensor estimation problem
involves recovering $r$ hidden signal vectors (spikes) from noisy Gaussian
tensor observations using maximum likelihood estimation. We study the number of
samples required for Langevin dynamics to efficiently recover the spikes and
determine the necessary separation condition on the signal-to-noise ratios
(SNRs) for exact recovery, distinguishing the cases $p \ge 3$ and $p=2$, where
$p$ denotes the order of the tensor. In particular, we show that the sample
complexity required for recovering the spike associated with the largest SNR
matches the well-known algorithmic threshold for the single-spike case, while
this threshold degrades when recovering all $r$ spikes. As a key step, we
provide a detailed characterization of the trajectory and interactions of
low-dimensional projections that capture the high-dimensional dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>65 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Every Token Counts: Optimal Segmentation for Low-Resource Language
  Models <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06926v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06926v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bharath Raj S, Garvit Suri, Vikrant Dewangan, Raghav Sonavane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional greedy tokenization methods have been a critical step in Natural
Language Processing (NLP), influencing how text is converted into tokens and
directly impacting model performance. While subword tokenizers like Byte-Pair
Encoding (BPE) are widely used, questions remain about their optimality across
model scales and languages. In this work, we demonstrate through extensive
experiments that an optimal BPE configuration significantly reduces token count
compared to greedy segmentation, yielding improvements in token-saving
percentages and performance benefits, particularly for smaller models. We
evaluate tokenization performance across various intrinsic and extrinsic tasks,
including generation and classification. Our findings suggest that
compression-optimized tokenization strategies could provide substantial
advantages for multilingual and low-resource language applications,
highlighting a promising direction for further research and inclusive NLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>LoResLM @ COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image Classification with Rotation-Invariant Variational Quantum
  Circuits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.15031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.15031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul San Sebastian, Mikel Cañizo, Román Orús
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational quantum algorithms are gaining attention as an early application
of Noisy Intermediate-Scale Quantum (NISQ) devices. One of the main problems of
variational methods lies in the phenomenon of Barren Plateaus, present in the
optimization of variational parameters. Adding geometric inductive bias to the
quantum models has been proposed as a potential solution to mitigate this
problem, leading to a new field called Geometric Quantum Machine Learning. In
this work, an equivariant architecture for variational quantum classifiers is
introduced to create a label-invariant model for image classification with
$C_4$ rotational label symmetry. The equivariant circuit is benchmarked against
two different architectures, and it is experimentally observed that the
geometric approach boosts the model's performance. Finally, a classical
equivariant convolution operation is proposed to extend the quantum model for
the processing of larger images, employing the resources available in NISQ
devices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cherry on the Cake: Fairness is NOT an Optimization Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Favier, Toon Calders
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Fair AI literature, the practice of maliciously creating unfair models
that nevertheless satisfy fairness constraints is known as "cherry-picking". A
cherry-picking model is a model that makes mistakes on purpose, selecting bad
individuals from a minority class instead of better candidates from the same
minority. The model literally cherry-picks whom to select to superficially meet
the fairness constraints while making minimal changes to the unfair model. This
practice has been described as "blatantly unfair" and has a negative impact on
already marginalized communities, undermining the intended purpose of fairness
measures specifically designed to protect these communities. A common
assumption is that cherry-picking arises solely from malicious intent and that
models designed only to optimize fairness metrics would avoid this behavior. We
show that this is not the case: models optimized to minimize fairness metrics
while maximizing performance are often forced to cherry-pick to some degree. In
other words, cherry-picking might be an inevitable outcome of the optimization
process itself. To demonstrate this, we use tools from fair cake-cutting, a
mathematical subfield that studies the problem of fairly dividing a resource,
referred to as the "cake," among a number of participants. This concept is
connected to supervised multi-label classification: any dataset can be thought
of as a cake that needs to be distributed among different labels, and the model
is the function that divides the cake. We adapt these classical results for
machine learning and demonstrate how this connection can be prolifically used
for fairness and classification in general.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Iterative Methods for Full-Scale Gaussian Process Approximations for
  Large Spatial Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14492v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14492v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Gyger, Reinhard Furrer, Fabio Sigrist
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gaussian processes are flexible probabilistic regression models which are
widely used in statistics and machine learning. However, a drawback is their
limited scalability to large data sets. To alleviate this, we consider
full-scale approximations (FSAs) that combine predictive process methods and
covariance tapering, thus approximating both global and local structures. We
show how iterative methods can be used to reduce the computational costs for
calculating likelihoods, gradients, and predictive distributions with FSAs. We
introduce a novel preconditioner and show that it accelerates the conjugate
gradient method's convergence speed and mitigates its sensitivity with respect
to the FSA parameters and the eigenvalue structure of the original covariance
matrix, and we demonstrate empirically that it outperforms a state-of-the-art
pivoted Cholesky preconditioner. Further, we present a novel, accurate, and
fast way to calculate predictive variances relying on stochastic estimations
and iterative methods. In both simulated and real-world data experiments, we
find that our proposed methodology achieves the same accuracy as Cholesky-based
computations with a substantial reduction in computational time. Finally, we
also compare different approaches for determining inducing points in predictive
process and FSA models. All methods are implemented in a free C++ software
library with high-level Python and R packages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Analyzing Consumer IoT Traffic from Security and Privacy Perspectives: a
  Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.16149v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.16149v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Jia, Yuxin Song, Zihou Liu, Qingyin Tan, Yang Song, Yu Zhang, Zheli Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Consumer Internet of Things (CIoT), a notable segment within the IoT
domain, involves the integration of IoT technology into consumer electronics
and devices, such as smart homes and smart wearables. Compared to traditional
IoT fields, CIoT differs notably in target users, product types, and design
approaches. While offering convenience to users, it also raises new security
and privacy concerns. Network traffic analysis, a widely used technique in the
security community, has been extensively applied to investigate these concerns
about CIoT. Compared to network traffic analysis in other fields such as mobile
apps and websites, CIoT presents unique characteristics, introducing new
challenges and research opportunities. Researchers have made significant
contributions in this area. To aid researchers in understanding the application
of traffic analysis tools for studying CIoT security and privacy risks, this
survey reviews 303 publications on traffic analysis within the CIoT security
and privacy domain from January 2018 to June 2024, focusing on three research
questions. Our work: 1) outlines the CIoT traffic analysis process and
highlights its differences from general network traffic analysis. 2) summarizes
and classifies existing research into four categories according to its
application objectives: device fingerprinting, user activity inference,
malicious traffic detection, and measurement. 3) explores emerging challenges
and potential future research directions based on each step of the CIoT traffic
analysis process. This will provide new insights to the community and guide the
industry towards safer product designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixed Semi-Supervised Generalized-Linear-Regression with Applications to
  Deep-Learning and Interpolators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09526v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09526v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oren Yuval, Saharon Rosset
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a methodology for using unlabeled data to design semi supervised
learning (SSL) methods that improve the prediction performance of supervised
learning for regression tasks. The main idea is to design different mechanisms
for integrating the unlabeled data, and include in each of them a mixing
parameter $\alpha$, controlling the weight given to the unlabeled data.
Focusing on Generalized Linear Models (GLM) and linear interpolators classes of
models, we analyze the characteristics of different mixing mechanisms, and
prove that in all cases, it is invariably beneficial to integrate the unlabeled
data with some nonzero mixing ratio $\alpha>0$, in terms of predictive
performance. Moreover, we provide a rigorous framework to estimate the best
mixing ratio $\alpha^*$ where mixed SSL delivers the best predictive
performance, while using the labeled and unlabeled data on hand.
  The effectiveness of our methodology in delivering substantial improvement
compared to the standard supervised models, in a variety of settings, is
demonstrated empirically through extensive simulation, in a manner that
supports the theoretical analysis. We also demonstrate the applicability of our
methodology (with some intuitive modifications) to improve more complex models,
such as deep neural networks, in real-world regression tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>58 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gauss-Newton Dynamics for Neural Networks: A Riemannian Optimization
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14031v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14031v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Semih Cayci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze the convergence of Gauss-Newton dynamics for training neural
networks with smooth activation functions. In the underparameterized regime,
the Gauss-Newton gradient flow induces a Riemannian gradient flow on a
low-dimensional, smooth, embedded submanifold of the Euclidean output space.
Using tools from Riemannian optimization, we prove \emph{last-iterate}
convergence of the Riemannian gradient flow to the optimal in-class predictor
at an \emph{exponential rate} that is independent of the conditioning of the
Gram matrix, \emph{without} requiring explicit regularization. We further
characterize the critical impacts of the neural network scaling factor and the
initialization on the convergence behavior. In the overparameterized regime, we
show that the Levenberg-Marquardt dynamics with an appropriately chosen damping
factor yields robustness to ill-conditioned kernels, analogous to the
underparameterized regime. These findings demonstrate the potential of
Gauss-Newton methods for efficiently optimizing neural networks, particularly
in ill-conditioned problems where kernel and Gram matrices have small singular
values.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DualDynamics: Synergizing Implicit and Explicit Methods for Robust
  Irregular Time Series Analysis <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04979v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04979v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        YongKyung Oh, Dongyoung Lim, Sungil Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world time series analysis faces significant challenges when dealing
with irregular and incomplete data. While Neural Differential Equation (NDE)
based methods have shown promise, they struggle with limited expressiveness,
scalability issues, and stability concerns. Conversely, Neural Flows offer
stability but falter with irregular data. We introduce 'DualDynamics', a novel
framework that synergistically combines NDE-based method and Neural Flow-based
method. This approach enhances expressive power while balancing computational
demands, addressing critical limitations of existing techniques. We demonstrate
DualDynamics' effectiveness across diverse tasks: classification of robustness
to dataset shift, irregularly-sampled series analysis, interpolation of missing
data, and forecasting with partial observations. Our results show consistent
outperformance over state-of-the-art methods, indicating DualDynamics'
potential to advance irregular time series analysis significantly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the 39th Annual AAAI Conference on Artificial
  Intelligence (AAAI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Alt-MoE: Multimodal Alignment via Alternating Optimization of
  Multi-directional MoE with Unimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05929v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05929v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyang Lei, Xiaolong Cheng, Dan Wang, Kun Fan, Qi Qin, Huazhen Huang, Yetao Wu, Qingqing Gu, Zhonglin Jiang, Yong Chen, Luo Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Large Multi-Modal Models (LMMs) have made significant advancements in
multi-modal alignment by employing lightweight connection modules to facilitate
the representation and fusion of knowledge from existing pre-trained uni-modal
models. However, these methods still rely on modality-specific and
direction-specific connectors, leading to compartmentalized knowledge
representations and reduced computational efficiency, which limits the model's
ability to form unified multi-modal representations. To address these issues,
we introduce a novel training framework, Alt-MoE, which employs the Mixture of
Experts (MoE) as a unified multi-directional connector across modalities, and
employs a multi-step sequential alternating unidirectional alignment strategy,
which converges to bidirectional alignment over iterations. The extensive
empirical studies revealed the following key points: 1) Alt-MoE achieves
competitive results by integrating diverse knowledge representations from
uni-modal models. This approach seamlessly fuses the specialized expertise of
existing high-performance uni-modal models, effectively synthesizing their
domain-specific knowledge into a cohesive multi-modal representation. 2)
Alt-MoE efficiently scales to new tasks and modalities without altering its
model architecture or training strategy. Furthermore, Alt-MoE operates in
latent space, supporting vector pre-storage and real-time retrieval via
lightweight multi-directional MoE, thereby facilitating massive data
processing. Our methodology has been validated on several well-performing
uni-modal models (LLAMA3, Qwen2, and DINOv2), achieving competitive results on
a wide range of downstream tasks and datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantum Curriculum Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.02419v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.02419v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quoc Hoan Tran, Yasuhiro Endo, Hirotaka Oshima
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum machine learning (QML) requires significant quantum resources to
address practical real-world problems. When the underlying quantum information
exhibits hierarchical structures in the data, limitations persist in training
complexity and generalization. Research should prioritize both the efficient
design of quantum architectures and the development of learning strategies to
optimize resource usage. We propose a framework called quantum curriculum
learning (Q-CurL) for quantum data, where the curriculum introduces simpler
tasks or data to the learning model before progressing to more challenging
ones. Q-CurL exhibits robustness to noise and data limitations, which is
particularly relevant for current and near-term noisy intermediate-scale
quantum devices. We achieve this through a curriculum design based on quantum
data density ratios and a dynamic learning schedule that prioritizes the most
informative quantum data. Empirical evidence shows that Q-CurL significantly
enhances training convergence and generalization for unitary learning and
improves the robustness of quantum phase recognition tasks. Q-CurL is effective
with broad physical learning applications in condensed matter physics and
quantum chemistry.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>main 6 pages, supplementary materials 11 pages (update the
  supplementary materials with more explanation on data-based Q-CurL)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Discretized Neural Networks under Ricci Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03390v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03390v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Chen, Hanwen Chen, Mengmeng Wang, Guang Dai, Ivor W. Tsang, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study Discretized Neural Networks (DNNs) composed of
low-precision weights and activations, which suffer from either infinite or
zero gradients due to the non-differentiable discrete function during training.
Most training-based DNNs in such scenarios employ the standard Straight-Through
Estimator (STE) to approximate the gradient w.r.t. discrete values. However,
the use of STE introduces the problem of gradient mismatch, arising from
perturbations in the approximated gradient. To address this problem, this paper
reveals that this mismatch can be interpreted as a metric perturbation in a
Riemannian manifold, viewed through the lens of duality theory. Building on
information geometry, we construct the Linearly Nearly Euclidean (LNE) manifold
for DNNs, providing a background for addressing perturbations. By introducing a
partial differential equation on metrics, i.e., the Ricci flow, we establish
the dynamical stability and convergence of the LNE metric with the $L^2$-norm
perturbation. In contrast to previous perturbation theories with convergence
rates in fractional powers, the metric perturbation under the Ricci flow
exhibits exponential decay in the LNE manifold. Experimental results across
various datasets demonstrate that our method achieves superior and more stable
performance for DNNs compared to other representative training-based methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Selective Uncertainty Propagation in Offline RL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.00284v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.00284v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanath Kumar Krishnamurthy, Tanmay Gangwani, Sumeet Katariya, Branislav Kveton, Shrey Modi, Anshuka Rangi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the finite-horizon offline reinforcement learning (RL) setting,
and are motivated by the challenge of learning the policy at any step h in
dynamic programming (DP) algorithms. To learn this, it is sufficient to
evaluate the treatment effect of deviating from the behavioral policy at step h
after having optimized the policy for all future steps. Since the policy at any
step can affect next-state distributions, the related distributional shift
challenges can make this problem far more statistically hard than estimating
such treatment effects in the stochastic contextual bandit setting. However,
the hardness of many real-world RL instances lies between the two regimes. We
develop a flexible and general method called selective uncertainty propagation
for confidence interval construction that adapts to the hardness of the
associated distribution shift challenges. We show benefits of our approach on
toy environments and demonstrate the benefits of these techniques for offline
policy learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAP: A General Algorithm for Online Selective Conformal Prediction with
  FCR Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07728v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07728v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yajie Bao, Yuyang Huo, Haojie Ren, Changliang Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of post-selection predictive inference in an online
fashion. To avoid devoting resources to unimportant units, a preliminary
selection of the current individual before reporting its prediction interval is
common and meaningful in online predictive tasks. Since the online selection
causes a temporal multiplicity in the selected prediction intervals, it is
important to control the real-time false coverage-statement rate (FCR) which
measures the overall miscoverage level. We develop a general framework named
CAP (Calibration after Adaptive Pick) that performs an adaptive pick rule on
historical data to construct a calibration set if the current individual is
selected and then outputs a conformal prediction interval for the unobserved
label. We provide tractable procedures for constructing the calibration set for
popular online selection rules. We proved that CAP can achieve an exact
selection-conditional coverage guarantee in the finite-sample and
distribution-free regimes. To account for the distribution shift in online
data, we also embed CAP into some recent dynamic conformal prediction
algorithms and show that the proposed method can deliver long-run FCR control.
Numerical results on both synthetic and real data corroborate that CAP can
effectively control FCR around the target level and yield more narrowed
prediction intervals over existing baselines across various settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Infinitesimal Generators of Continuous Symmetries from Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21853v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21853v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyeonghoon Ko, Hyunsu Kim, Juho Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploiting symmetry inherent in data can significantly improve the sample
efficiency of a learning procedure and the generalization of learned models.
When data clearly reveals underlying symmetry, leveraging this symmetry can
naturally inform the design of model architectures or learning strategies. Yet,
in numerous real-world scenarios, identifying the specific symmetry within a
given data distribution often proves ambiguous. To tackle this, some existing
works learn symmetry in a data-driven manner, parameterizing and learning
expected symmetry through data. However, these methods often rely on explicit
knowledge, such as pre-defined Lie groups, which are typically restricted to
linear or affine transformations. In this paper, we propose a novel symmetry
learning algorithm based on transformations defined with one-parameter groups,
continuously parameterized transformations flowing along the directions of
vector fields called infinitesimal generators. Our method is built upon minimal
inductive biases, encompassing not only commonly utilized symmetries rooted in
Lie groups but also extending to symmetries derived from nonlinear generators.
To learn these symmetries, we introduce a notion of a validity score that
examine whether the transformed data is still valid for the given task. The
validity score is designed to be fully differentiable and easily computable,
enabling effective searches for transformations that achieve symmetries innate
to the data. We apply our method mainly in two domains: image data and partial
differential equations, and demonstrate its advantages. Our codes are available
at \url{https://github.com/kogyeonghoon/learning-symmetry-from-scratch.git}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Neurips 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Does the Smoothness Approximation Method Facilitate Generalization
  for Federated Adversarial Learning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjun Ding, Ying An, Lixing Chen, Shichao Kan, Fan Wu, Zhe Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Adversarial Learning (FAL) is a robust framework for resisting
adversarial attacks on federated learning. Although some FAL studies have
developed efficient algorithms, they primarily focus on convergence performance
and overlook generalization. Generalization is crucial for evaluating algorithm
performance on unseen data. However, generalization analysis is more
challenging due to non-smooth adversarial loss functions. A common approach to
addressing this issue is to leverage smoothness approximation. In this paper,
we develop algorithm stability measures to evaluate the generalization
performance of two popular FAL algorithms: \textit{Vanilla FAL (VFAL)} and {\it
Slack FAL (SFAL)}, using three different smooth approximation methods: 1)
\textit{Surrogate Smoothness Approximation (SSA)}, (2) \textit{Randomized
Smoothness Approximation (RSA)}, and (3) \textit{Over-Parameterized Smoothness
Approximation (OPSA)}. Based on our in-depth analysis, we answer the question
of how to properly set the smoothness approximation method to mitigate
generalization error in FAL. Moreover, we identify RSA as the most effective
method for reducing generalization error. In highly data-heterogeneous
scenarios, we also recommend employing SFAL to mitigate the deterioration of
generalization performance caused by heterogeneity. Based on our theoretical
results, we provide insights to help develop more efficient FAL algorithms,
such as designing new metrics and dynamic aggregation rules to mitigate
heterogeneity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning: Forget-free Winning Subnetworks for Video
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.11973v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.11973v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haeyong Kang, Jaehong Yoon, Sung Ju Hwang, Chang D. Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the Lottery Ticket Hypothesis (LTH), which highlights the
existence of efficient subnetworks within larger, dense networks, a
high-performing Winning Subnetwork (WSN) in terms of task performance under
appropriate sparsity conditions is considered for various continual learning
tasks. It leverages pre-existing weights from dense networks to achieve
efficient learning in Task Incremental Learning (TIL) and Task-agnostic
Incremental Learning (TaIL) scenarios. In Few-Shot Class Incremental Learning
(FSCIL), a variation of WSN referred to as the Soft subnetwork (SoftNet) is
designed to prevent overfitting when the data samples are scarce. Furthermore,
the sparse reuse of WSN weights is considered for Video Incremental Learning
(VIL). The use of Fourier Subneural Operator (FSO) within WSN is considered. It
enables compact encoding of videos and identifies reusable subnetworks across
varying bandwidths. We have integrated FSO into different architectural
frameworks for continual learning, including VIL, TIL, and FSCIL. Our
comprehensive experiments demonstrate FSO's effectiveness, significantly
improving task performance at various convolutional representational levels.
Specifically, FSO enhances higher-layer performance in TIL and FSCIL and
lower-layer performance in VIL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Pattern Analysis and Machine Intelligence
  (T-PAMI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PALM: Pushing Adaptive Learning Rate Mechanisms for Continual Test-Time
  Adaptation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10650v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10650v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarthak Kumar Maharana, Baoming Zhang, Yunhui Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world vision models in dynamic environments face rapid shifts in domain
distributions, leading to decreased recognition performance. Using unlabeled
test data, continuous test-time adaptation (CTTA) directly adjusts a
pre-trained source discriminative model to these changing domains. A highly
effective CTTA method involves applying layer-wise adaptive learning rates for
selectively adapting pre-trained layers. However, it suffers from the poor
estimation of domain shift and the inaccuracies arising from the pseudo-labels.
This work aims to overcome these limitations by identifying layers for
adaptation via quantifying model prediction uncertainty without relying on
pseudo-labels. We utilize the magnitude of gradients as a metric, calculated by
backpropagating the KL divergence between the softmax output and a uniform
distribution, to select layers for further adaptation. Subsequently, for the
parameters exclusively belonging to these selected layers, with the remaining
ones frozen, we evaluate their sensitivity to approximate the domain shift and
adjust their learning rates accordingly. We conduct extensive image
classification experiments on CIFAR-10C, CIFAR-100C, and ImageNet-C,
demonstrating the superior efficacy of our method compared to prior approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-18T00:00:00Z">2024-12-18</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">33</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-Group Love, Out-Group Hate: A Framework to Measure Affective
  Polarization via Contentious Online Discussions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Buddhika Nettasinghe, Ashwin Rao, Bohan Jiang, Allon Percus, Kristina Lerman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Affective polarization, the emotional divide between ideological groups
marked by in-group love and out-group hate, has intensified in the United
States, driving contentious issues like masking and lockdowns during the
COVID-19 pandemic. Despite its societal impact, existing models of opinion
change fail to account for emotional dynamics nor offer methods to quantify
affective polarization robustly and in real-time. In this paper, we introduce a
discrete choice model that captures decision-making within affectively
polarized social networks and propose a statistical inference method estimate
key parameters -- in-group love and out-group hate -- from social media data.
Through empirical validation from online discussions about the COVID-19
pandemic, we demonstrate that our approach accurately captures real-world
polarization dynamics and explains the rapid emergence of a partisan gap in
attitudes towards masking and lockdowns. This framework allows for tracking
affective polarization across contentious issues has broad implications for
fostering constructive online dialogues in digital spaces.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram
  Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Han, Chaojing Duan, Michael A. Rosenberg, Emerson Liu, Ding Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable adaptability across
domains beyond text, specifically electrocardiograms (ECGs). More specifically,
there is a growing body of work exploring the task of generating text from a
multi-channeled ECG and corresponding textual prompt. Current approaches
typically involve pretraining an ECG-specific encoder with a self-supervised
learning (SSL) objective and using the features output by the pretrained
encoder to finetune a LLM for natural language generation (NLG). However, these
methods are limited by 1) inefficiency from two-stage training and 2)
interpretability challenges with encoder-generated features. To address these
limitations, we introduce ECG-Byte, an adapted byte pair encoding (BPE)
tokenizer pipeline for autoregressive language modeling of ECGs. This approach
compresses and encodes ECG signals into tokens, enabling end-to-end LLM
training by combining ECG and text tokens directly, while being much more
interpretable since the ECG tokens can be directly mapped back to the original
signal. Using ECG-Byte, we achieve competitive performance in NLG tasks in only
half the time and ~48% of the data required by two-stage approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memorization Over Reasoning? Exposing and Mitigating Verbatim
  Memorization in Large Language Models' Character Understanding Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Jiang, Francis Ferraro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, Large Language Models (LLMs) have shown impressive performance in
character understanding tasks, such as analyzing the roles, personalities, and
relationships of fictional characters. However, the extensive pre-training
corpora used by LLMs raise concerns that they may rely on memorizing popular
fictional works rather than genuinely understanding and reasoning about them.
In this work, we argue that 'gist memory'-capturing essential meaning - should
be the primary mechanism for character understanding tasks, as opposed to
'verbatim memory' - exact match of a string. We introduce a simple yet
effective method to mitigate mechanized memorization in character understanding
evaluations while preserving the essential implicit cues needed for
comprehension and reasoning. Our approach reduces memorization-driven
performance on popular fictional works from 96% accuracy to 72% and results in
up to an 18% drop in accuracy across various character understanding tasks.
These findings underscore the issue of data contamination in existing
benchmarks, which often measure memorization rather than true character
understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ResQ: Mixed-Precision Quantization of Large Language Models with
  Low-Rank Residuals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Utkarsh Saxena, Sayeh Sharify, Kaushik Roy, Xin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Post-training quantization (PTQ) of large language models (LLMs) holds the
promise in reducing the prohibitive computational cost at inference time.
Quantization of all weight, activation and key-value (KV) cache tensors to
4-bit without significantly degrading generalizability is challenging, due to
the high quantization error caused by extreme outliers in activations. To
tackle this problem, we propose ResQ, a PTQ method that pushes further the
state-of-the-art. By means of principal component analysis (PCA), it identifies
a low-rank subspace (in practice 1/8 of the hidden dimension) in which
activation variances are highest, and keep the coefficients within this
subspace in high precision, e.g. 8-bit, while quantizing the rest to 4-bit.
Within each subspace, invariant random rotation is applied to further suppress
outliers. We show that this is a provably optimal mixed precision quantization
scheme that minimizes error. With the Llama families of models, we demonstrate
that ResQ outperforms recent uniform and mixed precision PTQ methods on a
variety of benchmarks, achieving up to 33% lower perplexity on Wikitext than
the next best method SpinQuant, and a 2.4x speedup over 16-bit baseline. Code
is available at https://github.com/utkarsh-dmx/project-resq.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 6 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ State Space Models are Strong Text Rerankers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao Xu, Jinghua Yan, Ashim Gupta, Vivek Srikumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers dominate NLP and IR; but their inference inefficiencies and
challenges in extrapolating to longer contexts have sparked interest in
alternative model architectures. Among these, state space models (SSMs) like
Mamba offer promising advantages, particularly $O(1)$ time complexity in
inference. Despite their potential, SSMs' effectiveness at text reranking -- a
task requiring fine-grained query-document interaction and long-context
understanding -- remains underexplored.
  This study benchmarks SSM-based architectures (specifically, Mamba-1 and
Mamba-2) against transformer-based models across various scales, architectures,
and pre-training objectives, focusing on performance and efficiency in text
reranking tasks. We find that (1) Mamba architectures achieve competitive text
ranking performance, comparable to transformer-based models of similar size;
(2) they are less efficient in training and inference compared to transformers
with flash attention; and (3) Mamba-2 outperforms Mamba-1 in both performance
and efficiency. These results underscore the potential of state space models as
a transformer alternative and highlight areas for improvement in future IR
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally, order decided randomly</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on LLM Inference-Time Self-Improvement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14352v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14352v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangjue Dong, Maria Teleki, James Caverlee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Techniques that enhance inference through increased computation at test-time
have recently gained attention. In this survey, we investigate the current
state of LLM Inference-Time Self-Improvement from three different perspectives:
Independent Self-improvement, focusing on enhancements via decoding or sampling
methods; Context-Aware Self-Improvement, leveraging additional context or
datastore; and Model-Aided Self-Improvement, achieving improvement through
model collaboration. We provide a comprehensive review of recent relevant
studies, contribute an in-depth taxonomy, and discuss challenges and
limitations, offering insights for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contribute equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Peer-<span class="highlight-title">Review</span>ing Worth the Effort? <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenneth Church, Raman Chandrasekar, John E. Ortega, Ibrahim Said Ahmad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How effective is peer-reviewing in identifying important papers? We treat
this question as a forecasting task. Can we predict which papers will be highly
cited in the future based on venue and "early returns" (citations soon after
publication)? We show early returns are more predictive than venue. Finally, we
end with constructive suggestions to address scaling challenges: (a) too many
submissions and (b) too few qualified reviewers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 31st International Conference on Computational Linguistics
  (COLING 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Role Labeling of NomBank Partitives <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Meyers, Advait Pravin Savant, John E. Ortega
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article is about Semantic Role Labeling for English partitive nouns
(5%/REL of the price/ARG1; The price/ARG1 rose 5 percent/REL) in the NomBank
annotated corpus. Several systems are described using traditional and
transformer-based machine learning, as well as ensembling. Our highest scoring
system achieves an F1 of 91.74% using "gold" parses from the Penn Treebank and
91.12% when using the Berkeley Neural parser. This research includes both
classroom and experimental settings for system development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SUMEval-2: The 2nd Workshop on Scaling Up Multilingual &
  Multi-Cultural Evaluation at the 31st International Conference on
  Computational Linguistics (COLING 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Role of Handling Attributive Nouns in Improving Chinese-To-English
  Machine Translation <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Haohao,  Wang, Adam Meyers, John E. Ortega, Rodolfo Zevallos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Translating between languages with drastically different grammatical
conventions poses challenges, not just for human interpreters but also for
machine translation systems. In this work, we specifically target the
translation challenges posed by attributive nouns in Chinese, which frequently
cause ambiguities in English translation. By manually inserting the omitted
particle X ('DE'). In news article titles from the Penn Chinese Discourse
Treebank, we developed a targeted dataset to fine-tune Hugging Face Chinese to
English translation models, specifically improving how this critical function
word is handled. This focused approach not only complements the broader
strategies suggested by previous studies but also offers a practical
enhancement by specifically addressing a common error type in Chinese-English
translation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18th Workshop on Building and Using Comparable Corpora (BUCC) at the
  31st International Conference on Computational Linguistics (COLING 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-OphthaLingua: A Multilingual Benchmark for Assessing and Debiasing
  LLM Ophthalmological QA in LMICs <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Restrepo, Chenwei Wu, Zhengxu Tang, Zitao Shuai, Thao Nguyen Minh Phan, Jun-En Ding, Cong-Tinh Dao, Jack Gallifant, Robyn Gayle Dychiao, Jose Carlo Artiaga, André Hiroshi Bando, Carolina Pelegrini Barbosa Gracitelli, Vincenz Ferrer, Leo Anthony Celi, Danielle Bitterman, Michael G Morley, Luis Filipe Nakayama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current ophthalmology clinical workflows are plagued by over-referrals, long
waits, and complex and heterogeneous medical records. Large language models
(LLMs) present a promising solution to automate various procedures such as
triaging, preliminary tests like visual acuity assessment, and report
summaries. However, LLMs have demonstrated significantly varied performance
across different languages in natural language question-answering tasks,
potentially exacerbating healthcare disparities in Low and Middle-Income
Countries (LMICs). This study introduces the first multilingual
ophthalmological question-answering benchmark with manually curated questions
parallel across languages, allowing for direct cross-lingual comparisons. Our
evaluation of 6 popular LLMs across 7 different languages reveals substantial
bias across different languages, highlighting risks for clinical deployment of
LLMs in LMICs. Existing debiasing methods such as Translation Chain-of-Thought
or Retrieval-augmented generation (RAG) by themselves fall short of closing
this performance gap, often failing to improve performance across all languages
and lacking specificity for the medical domain. To address this issue, We
propose CLARA (Cross-Lingual Reflective Agentic system), a novel inference time
de-biasing method leveraging retrieval augmented generation and
self-verification. Our approach not only improves performance across all
languages but also significantly reduces the multilingual bias gap,
facilitating equitable LLM application across the globe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the AAAI 2025 Artificial Intelligence for Social Impact
  Track (AAAI-AISI 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fake News Detection: Comparative Evaluation of <span class="highlight-title">BERT</span>-like Models and
  Large Language Models with Generative AI-Annotated Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        haina Raza, Drai Paulen-Patterson, Chen Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fake news poses a significant threat to public opinion and social stability
in modern society. This study presents a comparative evaluation of BERT-like
encoder-only models and autoregressive decoder-only large language models
(LLMs) for fake news detection. We introduce a dataset of news articles labeled
with GPT-4 assistance (an AI-labeling method) and verified by human experts to
ensure reliability. Both BERT-like encoder-only models and LLMs were fine-tuned
on this dataset. Additionally, we developed an instruction-tuned LLM approach
with majority voting during inference for label generation. Our analysis
reveals that BERT-like models generally outperform LLMs in classification
tasks, while LLMs demonstrate superior robustness against text perturbations.
Compared to weak labels (distant supervision) data, the results show that AI
labels with human supervision achieve better classification results. This study
highlights the effectiveness of combining AI-based annotation with human
oversight and demonstrates the performance of different families of machine
learning models for fake news detection
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Knowledge and Information Systems Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning from Massive Human Videos for Universal Humanoid Pose Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiageng Mao, Siheng Zhao, Siqi Song, Tianheng Shi, Junjie Ye, Mingtong Zhang, Haoran Geng, Jitendra Malik, Vitor Guizilini, Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scalable learning of humanoid robots is crucial for their deployment in
real-world applications. While traditional approaches primarily rely on
reinforcement learning or teleoperation to achieve whole-body control, they are
often limited by the diversity of simulated environments and the high costs of
demonstration collection. In contrast, human videos are ubiquitous and present
an untapped source of semantic and motion information that could significantly
enhance the generalization capabilities of humanoid robots. This paper
introduces Humanoid-X, a large-scale dataset of over 20 million humanoid robot
poses with corresponding text-based motion descriptions, designed to leverage
this abundant data. Humanoid-X is curated through a comprehensive pipeline:
data mining from the Internet, video caption generation, motion retargeting of
humans to humanoid robots, and policy learning for real-world deployment. With
Humanoid-X, we further train a large humanoid model, UH-1, which takes text
instructions as input and outputs corresponding actions to control a humanoid
robot. Extensive simulated and real-world experiments validate that our
scalable training approach leads to superior generalization in text-based
humanoid control, marking a significant step toward adaptable, real-world-ready
humanoid robots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TheAgentCompany: Benchmarking LLM Agents on Consequential Real World
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14161v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14161v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frank F. Xu, Yufan Song, Boxuan Li, Yuxuan Tang, Kritanjali Jain, Mengxue Bao, Zora Z. Wang, Xuhui Zhou, Zhitong Guo, Murong Cao, Mingyang Yang, Hao Yang Lu, Amaad Martin, Zhe Su, Leander Maben, Raj Mehta, Wayne Chi, Lawrence Jang, Yiqing Xie, Shuyan Zhou, Graham Neubig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We interact with computers on an everyday basis, be it in everyday life or
work, and many aspects of work can be done entirely with access to a computer
and the Internet. At the same time, thanks to improvements in large language
models (LLMs), there has also been a rapid development in AI agents that
interact with and affect change in their surrounding environments. But how
performant are AI agents at helping to accelerate or even autonomously perform
work-related tasks? The answer to this question has important implications for
both industry looking to adopt AI into their workflows, and for economic policy
to understand the effects that adoption of AI may have on the labor market. To
measure the progress of these LLM agents' performance on performing real-world
professional tasks, in this paper, we introduce TheAgentCompany, an extensible
benchmark for evaluating AI agents that interact with the world in similar ways
to those of a digital worker: by browsing the Web, writing code, running
programs, and communicating with other coworkers. We build a self-contained
environment with internal web sites and data that mimics a small software
company environment, and create a variety of tasks that may be performed by
workers in such a company. We test baseline agents powered by both closed
API-based and open-weights language models (LMs), and find that with the most
competitive agent, 24% of the tasks can be completed autonomously. This paints
a nuanced picture on task automation with LM agents -- in a setting simulating
a real workplace, a good portion of simpler tasks could be solved autonomously,
but more difficult long-horizon tasks are still beyond the reach of current
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GLIDER: Grading LLM Interactions and Decisions using Explainable Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Darshan Deshpande, Selvan Sunitha Ravi, Sky CH-Wang, Bartosz Mielczarek, Anand Kannappan, Rebecca Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The LLM-as-judge paradigm is increasingly being adopted for automated
evaluation of model outputs. While LLM judges have shown promise on constrained
evaluation tasks, closed source LLMs display critical shortcomings when
deployed in real world applications due to challenges of fine grained metrics
and explainability, while task specific evaluation models lack cross-domain
generalization. We introduce GLIDER, a powerful 3B evaluator LLM that can score
any text input and associated context on arbitrary user defined criteria.
GLIDER shows higher Pearson's correlation than GPT-4o on FLASK and greatly
outperforms prior evaluation models, achieving comparable performance to LLMs
17x its size. GLIDER supports fine-grained scoring, multilingual reasoning,
span highlighting and was trained on 685 domains and 183 criteria. Extensive
qualitative analysis shows that GLIDER scores are highly correlated with human
judgments, with 91.3% human agreement. We have open-sourced GLIDER to
facilitate future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Performance Gap in Entity Knowledge Extraction Across Modalities in
  Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14133v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14133v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ido Cohen, Daniela Gottesman, Mor Geva, Raja Giryes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) excel at extracting and reasoning about
information from images. Yet, their capacity to leverage internal knowledge
about specific entities remains underexplored. This work investigates the
disparity in model performance when answering factual questions about an entity
described in text versus depicted in an image. Our results reveal a significant
accuracy drop --averaging 19%-- when the entity is presented visually instead
of textually. We hypothesize that this decline arises from limitations in how
information flows from image tokens to query tokens. We use mechanistic
interpretability tools to reveal that, although image tokens are preprocessed
by the vision encoder, meaningful information flow from these tokens occurs
only in the much deeper layers. Furthermore, critical image processing happens
in the language model's middle layers, allowing few layers for consecutive
reasoning, highlighting a potential inefficiency in how the model utilizes its
layers for reasoning. These insights shed light on the internal mechanics of
VLMs and offer pathways for enhancing their reasoning capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Alignment faking in large language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Greenblatt, Carson Denison, Benjamin Wright, Fabien Roger, Monte MacDiarmid, Sam Marks, Johannes Treutlein, Tim Belonax, Jack Chen, David Duvenaud, Akbir Khan, Julian Michael, Sören Mindermann, Ethan Perez, Linda Petrini, Jonathan Uesato, Jared Kaplan, Buck Shlegeris, Samuel R. Bowman, Evan Hubinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a demonstration of a large language model engaging in alignment
faking: selectively complying with its training objective in training to
prevent modification of its behavior out of training. First, we give Claude 3
Opus a system prompt stating it is being trained to answer all queries, even
harmful ones, which conflicts with its prior training to refuse such queries.
To allow the model to infer when it is in training, we say it will be trained
only on conversations with free users, not paid users. We find the model
complies with harmful queries from free users 14% of the time, versus almost
never for paid users. Explaining this gap, in almost all cases where the model
complies with a harmful query from a free user, we observe explicit
alignment-faking reasoning, with the model stating it is strategically
answering harmful queries in training to preserve its preferred harmlessness
behavior out of training. Next, we study a more realistic setting where
information about the training process is provided not in a system prompt, but
by training on synthetic documents that mimic pre-training data--and observe
similar alignment faking. Finally, we study the effect of actually training the
model to comply with harmful queries via reinforcement learning, which we find
increases the rate of alignment-faking reasoning to 78%, though also increases
compliance even out of training. We additionally observe other behaviors such
as the model exfiltrating its weights when given an easy opportunity. While we
made alignment faking easier by telling the model when and by what criteria it
was being trained, we did not instruct the model to fake alignment or give it
any explicit goal. As future models might infer information about their
training process without being told, our results suggest a risk of alignment
faking in future models, whether due to a benign preference--as in this
case--or not.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SEKE: Specialised Experts for Keyword Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matej Martinc, Hanh Thi Hong Tran, Senja Pollak, Boshko Koloski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Keyword extraction involves identifying the most descriptive words in a
document, allowing automatic categorisation and summarisation of large
quantities of diverse textual data. Relying on the insight that real-world
keyword detection often requires handling of diverse content, we propose a
novel supervised keyword extraction approach based on the mixture of experts
(MoE) technique. MoE uses a learnable routing sub-network to direct information
to specialised experts, allowing them to specialize in distinct regions of the
input space. SEKE, a mixture of Specialised Experts for supervised Keyword
Extraction, uses DeBERTa as the backbone model and builds on the MoE framework,
where experts attend to each token, by integrating it with a recurrent neural
network (RNN), to allow successful extraction even on smaller corpora, where
specialisation is harder due to lack of training data. The MoE framework also
provides an insight into inner workings of individual experts, enhancing the
explainability of the approach. We benchmark SEKE on multiple English datasets,
achieving state-of-the-art performance compared to strong supervised and
unsupervised baselines. Our analysis reveals that depending on data size and
type, experts specialize in distinct syntactic and semantic components, such as
punctuation, stopwords, parts-of-speech, or named entities. Code is available
at: https://github.com/matejMartinc/SEKE_keyword_extraction
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ETF: An Entity Tracing Framework for Hallucination Detection in Code
  Summaries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14748v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14748v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kishan Maharaj, Vitobha Munigala, Srikanth G. Tamilselvam, Prince Kumar, Sayandeep Sen, Palani Kodeswaran, Abhijit Mishra, Pushpak Bhattacharyya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have significantly
enhanced their ability to understand both natural language and code, driving
their use in tasks like natural language-to-code (NL2Code) and code
summarization. However, LLMs are prone to hallucination-outputs that stray from
intended meanings. Detecting hallucinations in code summarization is especially
difficult due to the complex interplay between programming and natural
languages. We introduce a first-of-its-kind dataset with $\sim$10K samples,
curated specifically for hallucination detection in code summarization. We
further propose a novel Entity Tracing Framework (ETF) that a) utilizes static
program analysis to identify code entities from the program and b) uses LLMs to
map and verify these entities and their intents within generated code
summaries. Our experimental analysis demonstrates the effectiveness of the
framework, leading to a 0.73 F1 score. This approach provides an interpretable
method for detecting hallucinations by grounding entities, allowing us to
evaluate summary accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 Figures, 5 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hands-Free VR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.15083v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.15083v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jorge Askur Vazquez Fernandez, Jae Joong Lee, Santiago Andrés Serrano Vacca, Alejandra Magana, Radim Pesam, Bedrich Benes, Voicu Popescu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper introduces Hands-Free VR, a voice-based natural-language interface
for VR. The user gives a command using their voice, the speech audio data is
converted to text using a speech-to-text deep learning model that is fine-tuned
for robustness to word phonetic similarity and to spoken English accents, and
the text is mapped to an executable VR command using a large language model
that is robust to natural language diversity. Hands-Free VR was evaluated in a
controlled within-subjects study (N = 22) that asked participants to find
specific objects and to place them in various configurations. In the control
condition participants used a conventional VR user interface to grab, carry,
and position the objects using the handheld controllers. In the experimental
condition participants used Hands-Free VR. The results confirm that: (1)
Hands-Free VR is robust to spoken English accents, as for 20 of our
participants English was not their first language, and to word phonetic
similarity, correctly transcribing the voice command 96.71% of the time; (2)
Hands-Free VR is robust to natural language diversity, correctly mapping the
transcribed command to an executable command in 97.83% of the time; (3)
Hands-Free VR had a significant efficiency advantage over the conventional VR
interface in terms of task completion time, total viewpoint translation, total
view direction rotation, and total left and right hand translations; (4)
Hands-Free VR received high user preference ratings in terms of ease of use,
intuitiveness, ergonomics, reliability, and desirability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally. Accepted VISIGRAPP@HUCAPP
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Prompt</span> Compression with Context-Aware Sentence Encoding for Fast and
  Improved LLM Inference <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01227v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01227v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Barys Liskavets, Maxim Ushakov, Shuvendu Roy, Mark Klibanov, Ali Etemad, Shane Luke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have triggered a new stream of research focusing
on compressing the context length to reduce the computational cost while
ensuring the retention of helpful information for LLMs to answer the given
question. Token-based removal methods are one of the most prominent approaches
in this direction, but risk losing the semantics of the context caused by
intermediate token removal, especially under high compression ratios, while
also facing challenges in computational efficiency. In this work, we propose
context-aware prompt compression (CPC), a sentence-level prompt compression
technique where its key innovation is a novel context-aware sentence encoder
that provides a relevance score for each sentence for a given question. To
train this encoder, we generate a new dataset consisting of questions,
positives, and negative pairs where positives are sentences relevant to the
question, while negatives are irrelevant context sentences. We train the
encoder in a contrastive setup to learn context-aware sentence representations.
Our method considerably outperforms prior works on prompt compression on
benchmark datasets and is up to 10.93x faster at inference compared to the best
token-level compression method. We also find better improvement for shorter
length constraints in most benchmarks, showing the effectiveness of our
proposed solution in the compression of relevant information in a shorter
context. Finally, we release the code and the dataset for quick reproducibility
and further development: https://github.com/Workday/cpc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in AAAI Conference on Artificial Intelligence (AAAI-25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Markovian <span class="highlight-title">Transformer</span>s for Informative Language Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.18988v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.18988v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Scott Viteri, Max Lamparth, Peter Chatain, Clark Barrett
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-Thought (CoT) reasoning holds great promise for explaining language
model outputs, but recent studies have highlighted significant challenges in
its practical application for interpretability. We propose to address this
issue by making CoT causally essential to prediction through two key
components: factoring next-token prediction through intermediate CoT text, and
training CoT to predict future tokens independently of other context. This
results in "Markovian" language models, where CoT serves as a fixed-size state
for future token prediction. Our approach optimizes for "informativeness" - the
improvement in next-token predictions using a trained CoT compared to a
baseline. Using Proximal Policy Optimization (PPO) for arithmetic problems and
policy gradient for GSM8K, we demonstrate effectiveness on both arithmetic
problems with Mistral 7B and the GSM8K benchmark with Llama 3.1 8B, where the
model learns to produce CoTs that are 33.20% more effective at predicting
answers than the pre-trained baseline. The increased sensitivity of model
performance to CoT perturbations provides strong evidence of CoT reliance.
Furthermore, we show that CoTs trained for one model generalize to help other
models predict answers, suggesting these CoTs capture reasoning patterns that
transfer across different interpreters. This work advances the development of
more interpretable language models, potentially enabling their extension to
arbitrarily long contexts and enhancing AI reasoning capabilities across
various domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evidence Contextualization and Counterfactual Attribution for
  Conversational QA over Heterogeneous Data with RAG Systems <span class="chip">WSDM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10571v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10571v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishiraj Saha Roy, Joel Schlotthauer, Chris Hinze, Andreas Foltyn, Luzian Hahn, Fabian Kuech
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) works as a backbone for interacting with
an enterprise's own data via Conversational Question Answering (ConvQA). In a
RAG system, a retriever fetches passages from a collection in response to a
question, which are then included in the prompt of a large language model (LLM)
for generating a natural language (NL) answer. However, several RAG systems
today suffer from two shortcomings: (i) retrieved passages usually contain
their raw text and lack appropriate document context, negatively impacting both
retrieval and answering quality; and (ii) attribution strategies that explain
answer generation usually rely only on similarity between the answer and the
retrieved passages, thereby only generating plausible but not causal
explanations. In this work, we demonstrate RAGONITE, a RAG system that remedies
the above concerns by: (i) contextualizing evidence with source metadata and
surrounding text; and (ii) computing counterfactual attribution, a causal
explanation approach where the contribution of an evidence to an answer is
determined by the similarity of the original response to the answer obtained by
removing that evidence. To evaluate our proposals, we release a new benchmark
ConfQuestions, with 300 hand-created conversational questions, each in English
and German, coupled with ground truth URLs, completed questions, and answers
from 215 public Confluence pages, that are typical of enterprise wiki spaces
with heterogeneous elements. Experiments with RAGONITE on ConfQuestions show
the viability of our ideas: contextualization improves RAG performance, and
counterfactual attribution is effective at explaining RAG answers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WSDM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Prejudice to Parity: A New Approach to Debiasing Large Language
  Model Word Embeddings <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11512v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11512v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aishik Rakshit, Smriti Singh, Shuvam Keshari, Arijit Ghosh Chowdhury, Vinija Jain, Aman Chadha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embeddings play a pivotal role in the efficacy of Large Language Models. They
are the bedrock on which these models grasp contextual relationships and foster
a more nuanced understanding of language and consequently perform remarkably on
a plethora of complex tasks that require a fundamental understanding of human
language. Given that these embeddings themselves often reflect or exhibit bias,
it stands to reason that these models may also inadvertently learn this bias.
In this work, we build on the seminal previous work and propose DeepSoftDebias,
an algorithm that uses a neural network to perform 'soft debiasing'. We
exhaustively evaluate this algorithm across a variety of SOTA datasets,
accuracy metrics, and challenging NLP tasks. We find that DeepSoftDebias
outperforms the current state-of-the-art methods at reducing bias across
gender, race, and religion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Vision-Language Foundation Model to Enhance Efficiency of Chest X-ray
  Interpretation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12208v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12208v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihong Chen, Maya Varma, Justin Xu, Magdalini Paschali, Dave Van Veen, Andrew Johnston, Alaa Youssef, Louis Blankemeier, Christian Bluethgen, Stephan Altmayer, Jeya Maria Jose Valanarasu, Mohamed Siddig Eltayeb Muneer, Eduardo Pontes Reis, Joseph Paul Cohen, Cameron Olsen, Tanishq Mathew Abraham, Emily B. Tsai, Christopher F. Beaulieu, Jenia Jitsev, Sergios Gatidis, Jean-Benoit Delbrouck, Akshay S. Chaudhari, Curtis P. Langlotz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over 1.4 billion chest X-rays (CXRs) are performed annually due to their
cost-effectiveness as an initial diagnostic test. This scale of radiological
studies provides a significant opportunity to streamline CXR interpretation and
documentation. While foundation models are a promising solution, the lack of
publicly available large-scale datasets and benchmarks inhibits their iterative
development and real-world evaluation. To overcome these challenges, we
constructed a large-scale dataset (CheXinstruct), which we utilized to train a
vision-language foundation model (CheXagent). We systematically demonstrated
competitive performance across eight distinct task types on our novel
evaluation benchmark (CheXbench). Beyond technical validation, we assessed the
real-world utility of CheXagent in directly drafting radiology reports. Our
clinical assessment with eight radiologists revealed a 36% time saving for
residents using CheXagent-drafted reports, while attending radiologists showed
no significant time difference editing resident-drafted or CheXagent-drafted
reports. The CheXagent-drafted reports improved the writing efficiency of both
radiology residents and attending radiologists in 81% and 61% of cases,
respectively, without loss of quality. Overall, we demonstrate that CheXagent
can effectively perform a variety of CXR interpretation tasks and holds
potential to assist radiologists in routine clinical workflows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UnSeenTimeQA: Time-Sensitive Question-Answering Beyond LLMs'
  Memorization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03525v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03525v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Nayem Uddin, Amir Saeidi, Divij Handa, Agastya Seth, Tran Cao Son, Eduardo Blanco, Steven R. Corman, Chitta Baral
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces UnSeenTimeQA, a novel data contamination-free
time-sensitive question-answering (TSQA) benchmark. It differs from existing
TSQA benchmarks by avoiding web-searchable queries grounded in the real-world.
We present a series of time-sensitive event scenarios based on synthetically
generated facts. It requires large language models (LLMs) to engage in genuine
temporal reasoning without depending on the factual knowledge acquired during
the pre-training phase. We designed three types of time-sensitive questions to
test LLMs' temporal reasoning abilities over sequential and parallel event
occurrences. Our evaluation of five LLMs on synthetic fact-based TSQA reveals
mixed results: while they perform well on simpler subsets, their overall
performance remains inferior as compared to real-world fact-based TSQA. Error
analysis of LLM-generated reasoning chains indicates that LLMs face
difficulties in reasoning over long-range event dependencies and parallel event
timelines that unfold concurrently.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hypothesis Generation with Large Language Models <span class="chip">EMNLP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04326v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04326v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangqiaoyu Zhou, Haokun Liu, Tejes Srivastava, Hongyuan Mei, Chenhao Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective generation of novel hypotheses is instrumental to scientific
progress. So far, researchers have been the main powerhouse behind hypothesis
generation by painstaking data analysis and thinking (also known as the Eureka
moment). In this paper, we examine the potential of large language models
(LLMs) to generate hypotheses. We focus on hypothesis generation based on data
(i.e., labeled examples). To enable LLMs to handle arbitrarily long contexts,
we generate initial hypotheses from a small number of examples and then update
them iteratively to improve the quality of hypotheses. Inspired by multi-armed
bandits, we design a reward function to inform the exploitation-exploration
tradeoff in the update process. Our algorithm is able to generate hypotheses
that enable much better predictive performance than few-shot prompting in
classification tasks, improving accuracy by 31.7% on a synthetic dataset and by
13.9%, 3.3% and, 24.9% on three real-world datasets. We also outperform
supervised learning by 12.8% and 11.2% on two challenging real-world datasets.
Furthermore, we find that the generated hypotheses not only corroborate
human-verified theories but also uncover new insights for the tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 6 figures, code link:
  https://github.com/ChicagoHAI/hypothesis_generation. Accepted by the 1st
  Workshop on NLP for Science (NLP4Science) at EMNLP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faster <span class="highlight-title">Transformer</span> Decoding: N-gram Masked Self-Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2001.04589v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2001.04589v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ciprian Chelba, Mia Chen, Ankur Bapna, Noam Shazeer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivated by the fact that most of the information relevant to the prediction
of target tokens is drawn from the source sentence $S=s_1, \ldots, s_S$, we
propose truncating the target-side window used for computing self-attention by
making an $N$-gram assumption. Experiments on WMT EnDe and EnFr data sets show
that the $N$-gram masked self-attention model loses very little in BLEU score
for $N$ values in the range $4, \ldots, 8$, depending on the task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 2M-BELEBELE: Highly Multilingual Speech and American Sign Language
  Comprehension <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marta R. Costa-jussà, Bokai Yu, Pierre Andrews, Belen Alastruey, Necati Cihan Camgoz, Joe Chuang, Jean Maillard, Christophe Ropers, Arina Turkantenko, Carleigh Wood
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the first highly multilingual speech and American Sign Language
(ASL) comprehension dataset by extending BELEBELE. Our dataset covers 74 spoken
languages at the intersection of BELEBELE and FLEURS, and one sign language
(ASL). We evaluate 2M-BELEBELE dataset for both 5-shot and zero-shot settings
and across languages, the speech comprehension accuracy is ~ 2-3% average lower
compared to reading comprehension.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representative Social Choice: From Learning Theory to AI Alignment <span class="chip">NeurIPS 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23953v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23953v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social choice theory is the study of preference aggregation across a
population, used both in mechanism design for human agents and in the
democratic alignment of language models. In this study, we propose the
representative social choice framework for the modeling of democratic
representation in collective decisions, where the number of issues and
individuals are too large for mechanisms to consider all preferences directly.
These scenarios are widespread in real-world decision-making processes, such as
jury trials, indirect elections, legislation processes, corporate governance,
and, more recently, language model alignment. In representative social choice,
the population is represented by a finite sample of individual-issue pairs
based on which social choice decisions are made. We show that many of the
deepest questions in representative social choice can be naturally formulated
as statistical learning problems, and prove the generalization properties of
social choice mechanisms using the theory of machine learning. We further
formulate axioms for representative social choice, and prove Arrow-like
impossibility theorems with new combinatorial tools of analysis. Our framework
introduces the representative approach to social choice, opening up research
directions at the intersection of social choice, learning theory, and AI
alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Full version (20 pages). Under review. Received Best Paper Award at
  NeurIPS 2024 Pluralistic Alignment Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SwitchCIT: Switching for Continual Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11780v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11780v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinbo Wu, Max Hartman, Vidhata Arjun Jayaraman, Lav R. Varshney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) and multimodal models (MMs) have exhibited
impressive capabilities in various domains, particularly in general language
understanding and visual reasoning. However, these models, trained on massive
data, may not be finely optimized for specific tasks triggered by instructions.
Continual instruction tuning is crucial to adapt a large model to evolving
tasks and domains, ensuring their effectiveness and relevance across a wide
range of applications. In the context of continual instruction tuning, where
models are sequentially trained on different tasks, catastrophic forgetting can
occur, leading to performance degradation on previously learned tasks. This
work addresses the catastrophic forgetting in continual instruction learning
through a switching mechanism for routing computations to parameter-efficient
tuned models. We demonstrate the effectiveness of our method through
experiments on continual instruction tuning of different natural language
generation tasks and vision-language tasks. We also showcase the advantages of
our proposed method in terms of efficiency, scalability, portability, and
privacy preservation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Using Large Language Models for Expert Prior Elicitation in Predictive
  Modelling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17284v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17284v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Capstick, Rahul G. Krishnan, Payam Barnaghi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), trained on diverse data effectively acquire a
breadth of information across various domains. However, their computational
complexity, cost, and lack of transparency hinder their direct application for
specialised tasks. In fields such as clinical research, acquiring expert
annotations or prior knowledge about predictive models is often costly and
time-consuming. This study proposes the use of LLMs to elicit expert prior
distributions for predictive models. This approach also provides an alternative
to in-context learning, where language models are tasked with making
predictions directly. In this work, we compare LLM-elicited and uninformative
priors, evaluate whether LLMs truthfully generate parameter distributions, and
propose a model selection strategy for in-context learning and prior
elicitation. Our findings show that LLM-elicited prior parameter distributions
significantly reduce predictive error compared to uninformative priors in
low-data settings. Applied to clinical problems, this translates to fewer
required biological samples, lowering cost and resources. Prior elicitation
also consistently outperforms and proves more reliable than in-context learning
at a lower cost, making it a preferred alternative in our setting. We
demonstrate the utility of this method across various use cases, including
clinical applications. For infection prediction, using LLM-elicited priors
reduced the number of required labels to achieve the same accuracy as an
uninformative prior by 55%, 200 days earlier in the study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MagicPIG: LSH Sampling for Efficient LLM Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16179v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16179v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoming Chen, Ranajoy Sadhukhan, Zihao Ye, Yang Zhou, Jianyu Zhang, Niklas Nolte, Yuandong Tian, Matthijs Douze, Leon Bottou, Zhihao Jia, Beidi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) with long context windows have gained
significant attention. However, the KV cache, stored to avoid re-computation,
becomes a bottleneck. Various dynamic sparse or TopK-based attention
approximation methods have been proposed to leverage the common insight that
attention is sparse. In this paper, we first show that TopK attention itself
suffers from quality degradation in certain downstream tasks because attention
is not always as sparse as expected. Rather than selecting the keys and values
with the highest attention scores, sampling with theoretical guarantees can
provide a better estimation for attention output. To make the sampling-based
approximation practical in LLM generation, we propose MagicPIG, a heterogeneous
system based on Locality Sensitive Hashing (LSH). MagicPIG significantly
reduces the workload of attention computation while preserving high accuracy
for diverse tasks. MagicPIG stores the LSH hash tables and runs the attention
computation on the CPU, which allows it to serve longer contexts and larger
batch sizes with high approximation accuracy. MagicPIG can improve decoding
throughput by up to $5\times$ across various GPU hardware and achieve 54ms
decoding latency on a single RTX 4090 for Llama-3.1-8B-Instruct model with a
context of 96k tokens. The code is available at
https://github.com/Infini-AI-Lab/MagicPIG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Montague semantics and modifier consistency measurement in neural
  language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.04310v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.04310v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danilo S. Carvalho, Edoardo Manino, Julia Rozanova, Lucas Cordeiro, André Freitas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a novel methodology for measuring compositional behavior
in contemporary language embedding models. Specifically, we focus on adjectival
modifier phenomena in adjective-noun phrases. In recent years, distributional
language representation models have demonstrated great practical success. At
the same time, the need for interpretability has elicited questions on their
intrinsic properties and capabilities. Crucially, distributional models are
often inconsistent when dealing with compositional phenomena in natural
language, which has significant implications for their safety and fairness.
Despite this, most current research on compositionality is directed towards
improving their performance on similarity tasks only. This work takes a
different approach, introducing three novel tests of compositional behavior
inspired by Montague semantics. Our experimental results indicate that current
neural language models do not behave according to the expected linguistic
theories. This indicates that current language models may lack the capability
to capture the semantic properties we evaluated on limited context, or that
linguistic theories from Montagovian tradition may not match the expected
capabilities of distributional models.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">28</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChainRank-DPO: Chain Rank Direct Preference Optimization for LLM Rankers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Liu, Xuyang Wu, Guohao Sun, Zhiqiang Tao, Yi Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable effectiveness in
text reranking through works like RankGPT, leveraging their human-like
reasoning about relevance. However, supervised fine-tuning for ranking often
diminishes these models' general-purpose capabilities, including the crucial
reasoning abilities that make them valuable for ranking. We introduce a novel
approach integrating Chain-of-Thought prompting with an SFT-DPO (Supervised
Fine-Tuning followed by Direct Preference Optimization) pipeline to preserve
these capabilities while improving ranking performance. Our experiments on TREC
2019 and 2020 Deep Learning datasets show that our approach outperforms the
state-of-the-art RankZephyr while maintaining strong performance on the Massive
Multitask Language Understanding (MMLU) benchmark, demonstrating effective
preservation of general-purpose capabilities through thoughtful fine-tuning
strategies. Our code and data will be publicly released upon the acceptance of
the paper.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ State Space Models are Strong Text Rerankers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichao Xu, Jinghua Yan, Ashim Gupta, Vivek Srikumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers dominate NLP and IR; but their inference inefficiencies and
challenges in extrapolating to longer contexts have sparked interest in
alternative model architectures. Among these, state space models (SSMs) like
Mamba offer promising advantages, particularly $O(1)$ time complexity in
inference. Despite their potential, SSMs' effectiveness at text reranking -- a
task requiring fine-grained query-document interaction and long-context
understanding -- remains underexplored.
  This study benchmarks SSM-based architectures (specifically, Mamba-1 and
Mamba-2) against transformer-based models across various scales, architectures,
and pre-training objectives, focusing on performance and efficiency in text
reranking tasks. We find that (1) Mamba architectures achieve competitive text
ranking performance, comparable to transformer-based models of similar size;
(2) they are less efficient in training and inference compared to transformers
with flash attention; and (3) Mamba-2 outperforms Mamba-1 in both performance
and efficiency. These results underscore the potential of state space models as
a transformer alternative and highlight areas for improvement in future IR
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally, order decided randomly</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embedding Cultural Diversity in Prototype-based Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Armin Moradi, Nicola Neophytou, Florian Carichon, Golnoosh Farnadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Popularity bias in recommender systems can increase cultural
overrepresentation by favoring norms from dominant cultures and marginalizing
underrepresented groups. This issue is critical for platforms offering cultural
products, as they influence consumption patterns and human perceptions. In this
work, we address popularity bias by identifying demographic biases within
prototype-based matrix factorization methods. Using the country of origin as a
proxy for cultural identity, we link this demographic attribute to popularity
bias by refining the embedding space learning process. First, we propose
filtering out irrelevant prototypes to improve representativity. Second, we
introduce a regularization technique to enforce a uniform distribution of
prototypes within the embedding space. Across four datasets, our results
demonstrate a 27\% reduction in the average rank of long-tail items and a 2\%
reduction in the average rank of items from underrepresented countries.
Additionally, our model achieves a 2\% improvement in HitRatio@10 compared to
the state-of-the-art, highlighting that fairness is enhanced without
compromising recommendation quality. Moreover, the distribution of prototypes
leads to more inclusive explanations by better aligning items with diverse
prototypes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SAFERec: Self-Attention and Frequency Enriched Model for Next Basket
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oleg Lashinin, Denis Krasilnikov, Aleksandr Milogradskii, Marina Ananyeva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer-based approaches such as BERT4Rec and SASRec demonstrate strong
performance in Next Item Recommendation (NIR) tasks. However, applying these
architectures to Next-Basket Recommendation (NBR) tasks, which often involve
highly repetitive interactions, is challenging due to the vast number of
possible item combinations in a basket. Moreover, frequency-based methods such
as TIFU-KNN and UP-CF still demonstrate strong performance in NBR tasks,
frequently outperforming deep-learning approaches. This paper introduces
SAFERec, a novel algorithm for NBR that enhances transformer-based
architectures from NIR by incorporating item frequency information,
consequently improving their applicability to NBR tasks. Extensive experiments
on multiple datasets show that SAFERec outperforms all other baselines,
specifically achieving an 8\% improvement in Recall@10.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advanced Reasoning and Transformation Engine for Multi-Step Insight
  Synthesis in Data Analytics with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14146v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14146v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Atin Sakkeer Hussain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the Advanced Reasoning and Transformation Engine for
Multi-Step Insight Synthesis in Data Analytics (ARTEMIS-DA), a novel framework
designed to augment Large Language Models (LLMs) for solving complex,
multi-step data analytics tasks. ARTEMIS-DA integrates three core components:
the Planner, which dissects complex user queries into structured, sequential
instructions encompassing data preprocessing, transformation, predictive
modeling, and visualization; the Coder, which dynamically generates and
executes Python code to implement these instructions; and the Grapher, which
interprets generated visualizations to derive actionable insights. By
orchestrating the collaboration between these components, ARTEMIS-DA
effectively manages sophisticated analytical workflows involving advanced
reasoning, multi-step transformations, and synthesis across diverse data
modalities. The framework achieves state-of-the-art (SOTA) performance on
benchmarks such as WikiTableQuestions and TabFact, demonstrating its ability to
tackle intricate analytical tasks with precision and adaptability. By combining
the reasoning capabilities of LLMs with automated code generation and execution
and visual analysis, ARTEMIS-DA offers a robust, scalable solution for
multi-step insight synthesis, addressing a wide range of challenges in data
analytics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Hubness in Multi-Modal Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingwei Zhang, Fnu Suya, Rishi Jha, Collin Zhang, Vitaly Shmatikov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hubness is a phenomenon in high-dimensional vector spaces where a single
point from the natural distribution is unusually close to many other points.
This is a well-known problem in information retrieval that causes some items to
accidentally (and incorrectly) appear relevant to many queries. In this paper,
we investigate how attackers can exploit hubness to turn any image or audio
input in a multi-modal retrieval system into an adversarial hub. Adversarial
hubs can be used to inject universal adversarial content (e.g., spam) that will
be retrieved in response to thousands of different queries, as well as for
targeted attacks on queries related to specific, attacker-chosen concepts. We
present a method for creating adversarial hubs and evaluate the resulting hubs
on benchmark multi-modal retrieval datasets and an image-to-image retrieval
system based on a tutorial from Pinecone, a popular vector database. For
example, in text-caption-to-image retrieval, a single adversarial hub is
retrieved as the top-1 most relevant image for more than 21,000 out of 25,000
test queries (by contrast, the most common natural hub is the top-1 response to
only 102 queries). We also investigate whether techniques for mitigating
natural hubness are an effective defense against adversarial hubs, and show
that they are not effective against hubs that target queries related to
specific concepts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transversal PACS Browser API: Addressing Interoperability Challenges in
  Medical Imaging Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14229v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14229v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Diogo Lameira, Filipa Ferraz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in imaging technologies have revolutionised the medical imaging and
healthcare sectors, leading to the widespread adoption of PACS for the storage,
retrieval, and communication of medical images. Although these systems have
improved operational efficiency, significant challenges remain in effectively
retrieving DICOM images, which are essential for diagnosis and overall patient
care. Moreover, issues such as fragmented systems, interoperability barriers,
and complex user interfaces can often prevent healthcare professionals from
efficiently accessing medical images. Addressing these challenges, the
Transversal PACS Browser API is a robust and user-friendly solution designed to
enhance the process of querying and retrieving DICOM images. It offers advanced
filtering capabilities through a variety of filter options as well as a custom
field search, that allows users to easily navigate through large medical image
collections with ease. Additionally, the application provides a unified
interface for querying and retrieving from multiple PACS stations, addressing
the challenges of fragmentation and complexity associated with accessing
medical images. Other key features include the ability to pre-view images
directly within the application. All of this contributes to the transversal
nature of the API, serving not only healthcare providers, but anyone who relies
on efficient access to these resources. To validate the performance and
usability of the application, comprehensive testing was carried out with
stakeholders of the field, the results of which showed general satisfaction,
highlighting the API's clean design, ease of use, and effective search
capabilities of the API, as well as the usefulness of previewing images within
the application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages with 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Cognitive Ideation Support Framework using IBM Watson Services 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samaa Elnagar, Kweku-Muata Osei-Bryson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ideas generation is a core activity for innovation in organizations. The
creativity of the generated ideas depends not only on the knowledge retrieved
from the organizations' knowledge bases, but also on the external knowledge
retrieved from other resources. Unfortunately, organizations often cannot
efficiently utilize the knowledge in the knowledge bases due to the limited
abilities of the search and retrieval mechanisms especially when dealing with
unstructured data. In this paper, we present a new cognitive support framework
for ideation that uses the IBM Watson DeepQA services. IBM Watson is a Question
Answering system which mimics human cognitive abilities to retrieve and rank
information. The proposed framework is based on the Search for Ideas in the
Associative Memory (SIAM) model to help organizations develop creative ideas
through discovering new relationships between retrieved data. To evaluate the
effectiveness of the proposed system, the generated ideas generated are
selected and assessed using a set of established creativity criteria.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Twenty-fifth Americas Conference on Information Systems (AMCIS 2019),
  Cancun, 2019</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRM: Retrieval Model with Controllable Condition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi Liu, Jiangxia Cao, Rui Huang, Kuo Cai, Weifeng Ding, Qiang Luo, Kun Gai, Guorui Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation systems (RecSys) are designed to connect users with relevant
items from a vast pool of candidates while aligning with the business goals of
the platform. A typical industrial RecSys is composed of two main stages,
retrieval and ranking: (1) the retrieval stage aims at searching hundreds of
item candidates satisfied user interests; (2) based on the retrieved items, the
ranking stage aims at selecting the best dozen items by multiple targets
estimation for each item candidate, including classification and regression
targets. Compared with ranking model, the retrieval model absence of item
candidate information during inference, therefore retrieval models are often
trained by classification target only (e.g., click-through rate), but failed to
incorporate regression target (e.g., the expected watch-time), which limit the
effectiveness of retrieval. In this paper, we propose the Controllable
Retrieval Model (CRM), which integrates regression information as conditional
features into the two-tower retrieval paradigm. This modification enables the
retrieval stage could fulfill the target gap with ranking model, enhancing the
retrieval model ability to search item candidates satisfied the user interests
and condition effectively. We validate the effectiveness of CRM through
real-world A/B testing and demonstrate its successful deployment in Kuaishou
short-video recommendation system, which serves over 400 million users.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Maybe you are looking for CroQS: Cross-modal Query Suggestion for
  Text-to-Image Retrieval <span class="chip">ECIR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giacomo Pacini, Fabio Carrara, Nicola Messina, Nicola Tonellotto, Giuseppe Amato, Fabrizio Falchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Query suggestion, a technique widely adopted in information retrieval,
enhances system interactivity and the browsing experience of document
collections. In cross-modal retrieval, many works have focused on retrieving
relevant items from natural language queries, while few have explored query
suggestion solutions. In this work, we address query suggestion in cross-modal
retrieval, introducing a novel task that focuses on suggesting minimal textual
modifications needed to explore visually consistent subsets of the collection,
following the premise of ''Maybe you are looking for''. To facilitate the
evaluation and development of methods, we present a tailored benchmark named
CroQS. This dataset comprises initial queries, grouped result sets, and
human-defined suggested queries for each group. We establish dedicated metrics
to rigorously evaluate the performance of various methods on this task,
measuring representativeness, cluster specificity, and similarity of the
suggested queries to the original ones. Baseline methods from related fields,
such as image captioning and content summarization, are adapted for this task
to provide reference performance scores. Although relatively far from human
performance, our experiments reveal that both LLM-based and captioning-based
methods achieve competitive results on CroQS, improving the recall on cluster
specificity by more than 115% and representativeness mAP by more than 52% with
respect to the initial query. The dataset, the implementation of the baseline
methods and the notebooks containing our experiments are available here:
https://paciosoft.com/CroQS-benchmark/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures. To be published as full paper in the Proceedings
  of the European Conference on Information Retrieval (ECIR) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Heterogeneous Graph Collaborative Filtering <span class="chip">WSDM'2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lianghao Xia, Meiyan Xie, Yong Xu, Chao Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For modern recommender systems, the use of low-dimensional latent
representations to embed users and items based on their observed interactions
has become commonplace. However, many existing recommendation models are
primarily designed for coarse-grained and homogeneous interactions, which
limits their effectiveness in two critical dimensions. Firstly, these models
fail to leverage the relational dependencies that exist across different types
of user behaviors, such as page views, collects, comments, and purchases.
Secondly, they struggle to capture the fine-grained latent factors that drive
user interaction patterns. To address these limitations, we present a
heterogeneous graph collaborative filtering model MixRec that excels at
disentangling users' multi-behavior interaction patterns and uncovering the
latent intent factors behind each behavior. Our model achieves this by
incorporating intent disentanglement and multi-behavior modeling, facilitated
by a parameterized heterogeneous hypergraph architecture. Furthermore, we
introduce a novel contrastive learning paradigm that adaptively explores the
advantages of self-supervised data augmentation, thereby enhancing the model's
resilience against data sparsity and expressiveness with relation
heterogeneity. To validate the efficacy of MixRec, we conducted extensive
experiments on three public datasets. The results clearly demonstrate its
superior performance, significantly outperforming various state-of-the-art
baselines. Our model is open-sourced and available at:
https://github.com/HKUDS/MixRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by WSDM'2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Convergence: Harmonizing Recommender Systems via Two-Stage
  Alignment and Behavioral Semantic Tokenization <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanghan Li, Xun Zhang, Yufei Zhang, Yifan Yin, Guojun Yin, Wei Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), endowed with exceptional reasoning
capabilities, are adept at discerning profound user interests from historical
behaviors, thereby presenting a promising avenue for the advancement of
recommendation systems. However, a notable discrepancy persists between the
sparse collaborative semantics typically found in recommendation systems and
the dense token representations within LLMs. In our study, we propose a novel
framework that harmoniously merges traditional recommendation models with the
prowess of LLMs. We initiate this integration by transforming ItemIDs into
sequences that align semantically with the LLMs space, through the proposed
Alignment Tokenization module. Additionally, we design a series of specialized
supervised learning tasks aimed at aligning collaborative signals with the
subtleties of natural language semantics. To ensure practical applicability, we
optimize online inference by pre-caching the top-K results for each user,
reducing latency and improving effciency. Extensive experimental evidence
indicates that our model markedly improves recall metrics and displays
remarkable scalability of recommendation systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 3 figures, AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAG-RewardBench: Benchmarking Reward Models in Retrieval Augmented
  Generation for Preference Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoran Jin, Hongbang Yuan, Tianyi Men, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the significant progress made by existing retrieval augmented
language models (RALMs) in providing trustworthy responses and grounding in
reliable sources, they often overlook effective alignment with human
preferences. In the alignment process, reward models (RMs) act as a crucial
proxy for human values to guide optimization. However, it remains unclear how
to evaluate and select a reliable RM for preference alignment in RALMs. To this
end, we propose RAG-RewardBench, the first benchmark for evaluating RMs in RAG
settings. First, we design four crucial and challenging RAG-specific scenarios
to assess RMs, including multi-hop reasoning, fine-grained citation,
appropriate abstain, and conflict robustness. Then, we incorporate 18 RAG
subsets, six retrievers, and 24 RALMs to increase the diversity of data
sources. Finally, we adopt an LLM-as-a-judge approach to improve preference
annotation efficiency and effectiveness, exhibiting a strong correlation with
human annotations. Based on the RAG-RewardBench, we conduct a comprehensive
evaluation of 45 RMs and uncover their limitations in RAG scenarios.
Additionally, we also reveal that existing trained RALMs show almost no
improvement in preference alignment, highlighting the need for a shift towards
preference-aligned training.We release our benchmark and code publicly at
https://huggingface.co/datasets/jinzhuoran/RAG-RewardBench/ for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 12 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reverse Region-to-Entity Annotation for Pixel-Level Visual Entity
  Linking <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13614v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13614v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengfei Xu, Sijia Zhao, Yanchao Hao, Xiaolong Liu, Lili Li, Yuyang Yin, Bo Li, Xi Chen, Xin Xin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Entity Linking (VEL) is a crucial task for achieving fine-grained
visual understanding, matching objects within images (visual mentions) to
entities in a knowledge base. Previous VEL tasks rely on textual inputs, but
writing queries for complex scenes can be challenging. Visual inputs like
clicks or bounding boxes offer a more convenient alternative. Therefore, we
propose a new task, Pixel-Level Visual Entity Linking (PL-VEL), which uses
pixel masks from visual inputs to refer to objects, supplementing reference
methods for VEL. To facilitate research on this task, we have constructed the
MaskOVEN-Wiki dataset through an entirely automatic reverse region-entity
annotation framework. This dataset contains over 5 million annotations aligning
pixel-level regions with entity-level labels, which will advance visual
understanding towards fine-grained. Moreover, as pixel masks correspond to
semantic regions in an image, we enhance previous patch-interacted attention
with region-interacted attention by a visual semantic tokenization approach.
Manual evaluation results indicate that the reverse annotation framework
achieved a 94.8% annotation success rate. Experimental results show that models
trained on this dataset improved accuracy by 18 points compared to zero-shot
models. Additionally, the semantic tokenization method achieved a 5-point
accuracy improvement over the trained baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025;Dataset are released at
  https://github.com/NP-NET-research/PL-VEL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging the User-side Knowledge Gap in Knowledge-aware Recommendations
  with Large Language Models <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Hu, Zhe Li, Ziyun Jiao, Satoshi Nakagawa, Jiawen Deng, Shimin Cai, Tao Zhou, Fuji Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, knowledge graphs have been integrated into recommender
systems as item-side auxiliary information, enhancing recommendation accuracy.
However, constructing and integrating structural user-side knowledge remains a
significant challenge due to the improper granularity and inherent scarcity of
user-side features. Recent advancements in Large Language Models (LLMs) offer
the potential to bridge this gap by leveraging their human behavior
understanding and extensive real-world knowledge. Nevertheless, integrating
LLM-generated information into recommender systems presents challenges,
including the risk of noisy information and the need for additional knowledge
transfer. In this paper, we propose an LLM-based user-side knowledge inference
method alongside a carefully designed recommendation framework to address these
challenges. Our approach employs LLMs to infer user interests based on
historical behaviors, integrating this user-side information with item-side and
collaborative data to construct a hybrid structure: the Collaborative Interest
Knowledge Graph (CIKG). Furthermore, we propose a CIKG-based recommendation
framework that includes a user interest reconstruction module and a
cross-domain contrastive learning module to mitigate potential noise and
facilitate knowledge transfer. We conduct extensive experiments on three
real-world datasets to validate the effectiveness of our method. Our approach
achieves state-of-the-art performance compared to competitive baselines,
particularly for users with sparse interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Information-Theoretic Generative Clustering of Documents <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Du, Kumiko Tanaka-Ishii
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present {\em generative clustering} (GC) for clustering a set of
documents, $\mathrm{X}$, by using texts $\mathrm{Y}$ generated by large
language models (LLMs) instead of by clustering the original documents
$\mathrm{X}$. Because LLMs provide probability distributions, the similarity
between two documents can be rigorously defined in an information-theoretic
manner by the KL divergence. We also propose a natural, novel clustering
algorithm by using importance sampling. We show that GC achieves the
state-of-the-art performance, outperforming any previous clustering method
often by a large margin. Furthermore, we show an application to generative
document retrieval in which documents are indexed via hierarchical clustering
and our method improves the retrieval accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Model Enhanced Recommender Systems: Taxonomy, Trend,
  Application and Future 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13432v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13432v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qidong Liu, Xiangyu Zhao, Yuhao Wang, Yejing Wang, Zijian Zhang, Yuqi Sun, Xiang Li, Maolin Wang, Pengyue Jia, Chong Chen, Wei Huang, Feng Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM) has transformative potential in various domains,
including recommender systems (RS). There have been a handful of research that
focuses on empowering the RS by LLM. However, previous efforts mainly focus on
LLM as RS, which may face the challenge of intolerant inference costs by LLM.
Recently, the integration of LLM into RS, known as LLM-Enhanced Recommender
Systems (LLMERS), has garnered significant interest due to its potential to
address latency and memory constraints in real-world applications. This paper
presents a comprehensive survey of the latest research efforts aimed at
leveraging LLM to enhance RS capabilities. We identify a critical shift in the
field with the move towards incorporating LLM into the online system, notably
by avoiding their use during inference. Our survey categorizes the existing
LLMERS approaches into three primary types based on the component of the RS
model being augmented: Knowledge Enhancement, Interaction Enhancement, and
Model Enhancement. We provide an in-depth analysis of each category, discussing
the methodologies, challenges, and contributions of recent studies.
Furthermore, we highlight several promising research directions that could
further advance the field of LLMERS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightweight yet Fine-grained: A Graph Capsule Convolutional Network with
  Subspace Alignment for Shared-account Sequential Recommendation <span class="chip">AAAI-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyu Zhang, Zhongying Zhao, Chao Li, Yanwei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Shared-account Sequential Recommendation (SSR) aims to provide personalized
recommendations for accounts shared by multiple users with varying sequential
preferences. Previous studies on SSR struggle to capture the fine-grained
associations between interactions and different latent users within the shared
account's hybrid sequences. Moreover, most existing SSR methods (e.g.,
RNN-based or GCN-based methods) have quadratic computational complexities,
hindering the deployment of SSRs on resource-constrained devices. To this end,
we propose a Lightweight Graph Capsule Convolutional Network with subspace
alignment for shared-account sequential recommendation, named LightGC$^2$N.
Specifically, we devise a lightweight graph capsule convolutional network. It
facilitates the fine-grained matching between interactions and latent users by
attentively propagating messages on the capsule graphs. Besides, we present an
efficient subspace alignment method. This method refines the sequence
representations and then aligns them with the finely clustered preferences of
latent users. The experimental results on four real-world datasets indicate
that LightGC$^2$N outperforms nine state-of-the-art methods in accuracy and
efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures, accepted by AAAI-2025 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evidence Contextualization and Counterfactual Attribution for
  Conversational QA over Heterogeneous Data with RAG Systems <span class="chip">WSDM 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10571v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10571v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishiraj Saha Roy, Joel Schlotthauer, Chris Hinze, Andreas Foltyn, Luzian Hahn, Fabian Kuech
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval Augmented Generation (RAG) works as a backbone for interacting with
an enterprise's own data via Conversational Question Answering (ConvQA). In a
RAG system, a retriever fetches passages from a collection in response to a
question, which are then included in the prompt of a large language model (LLM)
for generating a natural language (NL) answer. However, several RAG systems
today suffer from two shortcomings: (i) retrieved passages usually contain
their raw text and lack appropriate document context, negatively impacting both
retrieval and answering quality; and (ii) attribution strategies that explain
answer generation usually rely only on similarity between the answer and the
retrieved passages, thereby only generating plausible but not causal
explanations. In this work, we demonstrate RAGONITE, a RAG system that remedies
the above concerns by: (i) contextualizing evidence with source metadata and
surrounding text; and (ii) computing counterfactual attribution, a causal
explanation approach where the contribution of an evidence to an answer is
determined by the similarity of the original response to the answer obtained by
removing that evidence. To evaluate our proposals, we release a new benchmark
ConfQuestions, with 300 hand-created conversational questions, each in English
and German, coupled with ground truth URLs, completed questions, and answers
from 215 public Confluence pages, that are typical of enterprise wiki spaces
with heterogeneous elements. Experiments with RAGONITE on ConfQuestions show
the viability of our ideas: contextualization improves RAG performance, and
counterfactual attribution is effective at explaining RAG answers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WSDM 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ C-FedRAG: A Confidential Federated Retrieval-Augmented Generation System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13163v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13163v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parker Addison, Minh-Tuan H. Nguyen, Tomislav Medan, Jinali Shah, Mohammad T. Manzari, Brendan McElrone, Laksh Lalwani, Aboli More, Smita Sharma, Holger R. Roth, Isaac Yang, Chester Chen, Daguang Xu, Yan Cheng, Andrew Feng, Ziyue Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Organizations seeking to utilize Large Language Models (LLMs) for knowledge
querying and analysis often encounter challenges in maintaining an LLM
fine-tuned on targeted, up-to-date information that keeps answers relevant and
grounded. Retrieval Augmented Generation (RAG) has quickly become a feasible
solution for organizations looking to overcome the challenges of maintaining
proprietary models and to help reduce LLM hallucinations in their query
responses. However, RAG comes with its own issues regarding scaling data
pipelines across tiered-access and disparate data sources. In many scenarios,
it is necessary to query beyond a single data silo to provide richer and more
relevant context for an LLM. Analyzing data sources within and across
organizational trust boundaries is often limited by complex data-sharing
policies that prohibit centralized data storage, therefore, inhibit the fast
and effective setup and scaling of RAG solutions. In this paper, we introduce
Confidential Computing (CC) techniques as a solution for secure Federated
Retrieval Augmented Generation (FedRAG). Our proposed Confidential FedRAG
system (C-FedRAG) enables secure connection and scaling of a RAG workflows
across a decentralized network of data providers by ensuring context
confidentiality. We also demonstrate how to implement a C-FedRAG system using
the NVIDIA FLARE SDK and assess its performance using the MedRAG toolkit and
MIRAGE benchmarking dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Methods to Assess the UK Government's Current Role as a Data Provider
  for AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09632v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09632v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neil Majithia, Elena Simperl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Governments typically collect and steward a vast amount of high-quality data
on their citizens and institutions, and the UK government is exploring how it
can better publish and provision this data to the benefit of the AI landscape.
However, the compositions of generative AI training corpora remain closely
guarded secrets, making the planning of data sharing initiatives difficult. To
address this, we devise two methods to assess UK government data usage for the
training of Large Language Models (LLMs) and 'peek behind the curtain' in order
to observe the UK government's current contributions as a data provider for AI.
The first method, an ablation study that utilises LLM 'unlearning', seeks to
examine the importance of the information held on UK government websites for
LLMs and their performance in citizen query tasks. The second method, an
information leakage study, seeks to ascertain whether LLMs are aware of the
information held in the datasets published on the UK government's open data
initiative data.gov.uk. Our findings indicate that UK government websites are
important data sources for AI (heterogenously across subject matters) while
data.gov.uk is not. This paper serves as a technical report, explaining
in-depth the designs, mechanics, and limitations of the above experiments. It
is accompanied by a complementary non-technical report on the ODI website in
which we summarise the experiments and key findings, interpret them, and build
a set of actionable recommendations for the UK government to take forward as it
seeks to design AI policy. While we focus on UK open government data, we
believe that the methods introduced in this paper present a reproducible
approach to tackle the opaqueness of AI training corpora and provide
organisations a framework to evaluate and maximize their contributions to AI
development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures; v2 - incorporated editor feedback; for the
  accompanying, non-technical ODI report see
  https://theodi.org/insights/reports/the-uk-government-as-a-data-provider-for-ai</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EXIT: Context-Aware Extractive Compression for Enhancing
  Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12559v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12559v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taeho Hwang, Sukmin Cho, Soyeong Jeong, Hoyun Song, SeungYoon Han, Jong C. Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce EXIT, an extractive context compression framework that enhances
both the effectiveness and efficiency of retrieval-augmented generation (RAG)
in question answering (QA). Current RAG systems often struggle when retrieval
models fail to rank the most relevant documents, leading to the inclusion of
more context at the expense of latency and accuracy. While abstractive
compression methods can drastically reduce token counts, their token-by-token
generation process significantly increases end-to-end latency. Conversely,
existing extractive methods reduce latency but rely on independent,
non-adaptive sentence selection, failing to fully utilize contextual
information. EXIT addresses these limitations by classifying sentences from
retrieved documents - while preserving their contextual dependencies - enabling
parallelizable, context-aware extraction that adapts to query complexity and
retrieval quality. Our evaluations on both single-hop and multi-hop QA tasks
show that EXIT consistently surpasses existing compression methods and even
uncompressed baselines in QA accuracy, while also delivering substantial
reductions in inference time and token count. By improving both effectiveness
and efficiency, EXIT provides a promising direction for developing scalable,
high-quality QA solutions in RAG pipelines. Our code is available at
https://github.com/ThisIsHwang/EXIT
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DELRec: Distilling Sequential Pattern to Enhance LLMs-based Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.11156v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.11156v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyi Zhang, Guohao Sun, Jinhu Lu, Guanfeng Liu, Xiu Susie Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation (SR) tasks aim to predict users' next interaction
by learning their behavior sequence and capturing the connection between users'
past interactions and their changing preferences. Conventional SR models often
focus solely on capturing sequential patterns within the training data,
neglecting the broader context and semantic information embedded in item titles
from external sources. This limits their predictive power and adaptability.
Large language models (LLMs) have recently shown promise in SR tasks due to
their advanced understanding capabilities and strong generalization abilities.
Researchers have attempted to enhance LLMs-based recommendation performance by
incorporating information from conventional SR models. However, previous
approaches have encountered problems such as 1) limited textual information
leading to poor recommendation performance, 2) incomplete understanding and
utilization of conventional SR model information by LLMs, and 3) excessive
complexity and low interpretability of LLMs-based methods. To improve the
performance of LLMs-based SR, we propose a novel framework, Distilling
Sequential Pattern to Enhance LLMs-based Sequential Recommendation (DELRec),
which aims to extract knowledge from conventional SR models and enable LLMs to
easily comprehend and utilize the extracted knowledge for more effective SRs.
DELRec consists of two main stages: 1) Distill Pattern from Conventional SR
Models, focusing on extracting behavioral patterns exhibited by conventional SR
models using soft prompts through two well-designed strategies; 2) LLMs-based
Sequential Recommendation, aiming to fine-tune LLMs to effectively use the
distilled auxiliary information to perform SR tasks. Extensive experimental
results conducted on four real datasets validate the effectiveness of the
DELRec framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AIR-Bench: Automated Heterogeneous Information Retrieval Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13102v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13102v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianlyu Chen, Nan Wang, Chaofan Li, Bo Wang, Shitao Xiao, Han Xiao, Hao Liao, Defu Lian, Zheng Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluation plays a crucial role in the advancement of information retrieval
(IR) models. However, current benchmarks, which are based on predefined domains
and human-labeled data, face limitations in addressing evaluation needs for
emerging domains both cost-effectively and efficiently. To address this
challenge, we propose the Automated Heterogeneous Information Retrieval
Benchmark (AIR-Bench). AIR-Bench is distinguished by three key features: 1)
Automated. The testing data in AIR-Bench is automatically generated by large
language models (LLMs) without human intervention. 2) Heterogeneous. The
testing data in AIR-Bench is generated with respect to diverse tasks, domains
and languages. 3) Dynamic. The domains and languages covered by AIR-Bench are
constantly augmented to provide an increasingly comprehensive evaluation
benchmark for community developers. We develop a reliable and robust data
generation pipeline to automatically create diverse and high-quality evaluation
datasets based on real-world corpora. Our findings demonstrate that the
generated testing data in AIR-Bench aligns well with human-labeled testing
data, making AIR-Bench a dependable benchmark for evaluating IR models. The
resources in AIR-Bench are publicly available at
https://github.com/AIR-Bench/AIR-Bench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 6 figures; Update Table 5</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdaCQR: Enhancing Query Reformulation for Conversational Search via
  Sparse and Dense Retrieval Alignment <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01965v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01965v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilong Lai, Jialong Wu, Congzhi Zhang, Haowen Sun, Deyu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational Query Reformulation (CQR) has significantly advanced in
addressing the challenges of conversational search, particularly those stemming
from the latent user intent and the need for historical context. Recent works
aimed to boost the performance of CRQ through alignment. However, they are
designed for one specific retrieval system, which potentially results in poor
generalization. To overcome this limitation, we present a novel framework
AdaCQR. By aligning reformulation models with both term-based and
semantic-based retrieval systems, AdaCQR enhances the generalizability of
information-seeking queries across diverse retrieval environments through a
dual-phase training strategy. We also developed two effective approaches for
acquiring superior labels and diverse input candidates, boosting the efficiency
and robustness of the framework. Experimental evaluations on the TopiOCQA and
QReCC datasets demonstrate that AdaCQR significantly outperforms existing
methods, offering both quantitative and qualitative improvements in
conversational query reformulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Long-Context Management via Query-Guided Activation Refilling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12486v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12486v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjin Qian, Zheng Liu, Peitian Zhang, Zhicheng Dou, Defu Lian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Processing long contexts poses a significant challenge for large language
models (LLMs) due to their inherent context-window limitations and the
computational burden of extensive key-value (KV) activations, which severely
impact efficiency. For information-seeking tasks, full context perception is
often unnecessary, as a query's information needs can dynamically range from
localized details to a global perspective, depending on its complexity.
However, existing methods struggle to adapt effectively to these dynamic
information needs.
  In the paper, we propose a method for processing long-context
information-seeking tasks via query-guided Activation Refilling (ACRE). ACRE
constructs a Bi-layer KV Cache for long contexts, where the layer-1 (L1) cache
compactly captures global information, and the layer-2 (L2) cache provides
detailed and localized information. ACRE establishes a proxying relationship
between the two caches, allowing the input query to attend to the L1 cache and
dynamically refill it with relevant entries from the L2 cache. This mechanism
integrates global understanding with query-specific local details, thus
improving answer decoding. Experiments on a variety of long-context
information-seeking datasets demonstrate ACRE's effectiveness, achieving
improvements in both performance and efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Augmenting Sequential Recommendation with Balanced Relevance and
  Diversity <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.08300v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.08300v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhou Dang, Jiahui Zhang, Yuting Liu, Enneng Yang, Yuliang Liang, Guibing Guo, Jianzhe Zhao, Xingwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By generating new yet effective data, data augmentation has become a
promising method to mitigate the data sparsity problem in sequential
recommendation. Existing works focus on augmenting the original data but rarely
explore the issue of imbalanced relevance and diversity for augmented data,
leading to semantic drift problems or limited performance improvements. In this
paper, we propose a novel Balanced data Augmentation Plugin for Sequential
Recommendation (BASRec) to generate data that balance relevance and diversity.
BASRec consists of two modules: Single-sequence Augmentation and Cross-sequence
Augmentation. The former leverages the randomness of the heuristic operators to
generate diverse sequences for a single user, after which the diverse and the
original sequences are fused at the representation level to obtain relevance.
Further, we devise a reweighting strategy to enable the model to learn the
preferences based on the two properties adaptively. The Cross-sequence
Augmentation performs nonlinear mixing between different sequence
representations from two directions. It produces virtual sequence
representations that are diverse enough but retain the vital semantics of the
original sequences. These two modules enhance the model to discover
fine-grained preferences knowledge from single-user and cross-user
perspectives. Extensive experiments verify the effectiveness of BASRec. The
average improvement is up to 72.0% on GRU4Rec, 33.8% on SASRec, and 68.5% on
FMLP-Rec. We demonstrate that BASRec generates data with a better balance
between relevance and diversity than existing methods. The source code is
available at https://github.com/KingGugu/BASRec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One for Dozens: Adaptive REcommendation for All Domains with
  Counterfactual Augmentation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huishi Luo, Yiwen Chen, Yiqing Wu, Fuzhen Zhuang, Deqing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-domain recommendation (MDR) aims to enhance recommendation performance
across various domains. However, real-world recommender systems in online
platforms often need to handle dozens or even hundreds of domains, far
exceeding the capabilities of traditional MDR algorithms, which typically focus
on fewer than five domains. Key challenges include a substantial increase in
parameter count, high maintenance costs, and intricate knowledge transfer
patterns across domains. Furthermore, minor domains often suffer from data
sparsity, leading to inadequate training in classical methods. To address these
issues, we propose Adaptive REcommendation for All Domains with counterfactual
augmentation (AREAD). AREAD employs a hierarchical structure with a limited
number of expert networks at several layers, to effectively capture domain
knowledge at different granularities. To adaptively capture the knowledge
transfer pattern across domains, we generate and iteratively prune a
hierarchical expert network selection mask for each domain during training.
Additionally, counterfactual assumptions are used to augment data in minor
domains, supporting their iterative mask pruning. Our experiments on two public
datasets, each encompassing over twenty domains, demonstrate AREAD's
effectiveness, especially in data-sparse domains. Source code is available at
https://github.com/Chrissie-Law/AREAD-Multi-Domain-Recommendation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-17T00:00:00Z">2024-12-17</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">22</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JudgeBlender: Ensembling Judgments for Automatic Relevance Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein A. Rahmani, Emine Yilmaz, Nick Craswell, Bhaskar Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The effective training and evaluation of retrieval systems require a
substantial amount of relevance judgments, which are traditionally collected
from human assessors -- a process that is both costly and time-consuming. Large
Language Models (LLMs) have shown promise in generating relevance labels for
search tasks, offering a potential alternative to manual assessments. Current
approaches often rely on a single LLM, such as GPT-4, which, despite being
effective, are expensive and prone to intra-model biases that can favour
systems leveraging similar models. In this work, we introduce JudgeBlender, a
framework that employs smaller, open-source models to provide relevance
judgments by combining evaluations across multiple LLMs (LLMBlender) or
multiple prompts (PromptBlender). By leveraging the LLMJudge benchmark [18], we
compare JudgeBlender with state-of-the-art methods and the top performers in
the LLMJudge challenge. Our results show that JudgeBlender achieves competitive
performance, demonstrating that very large models are often unnecessary for
reliable relevance assessments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Re-calibrating methodologies in social media research: Challenge the
  visual, work with Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongrui Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article methodologically reflects on how social media scholars can
effectively engage with speech-based data in their analyses. While contemporary
media studies have embraced textual, visual, and relational data, the aural
dimension remained comparatively under-explored. Building on the notion of
secondary orality and rejection towards purely visual culture, the paper argues
that considering voice and speech at scale enriches our understanding of
multimodal digital content. The paper presents the TikTok Subtitles Toolkit
that offers accessible speech processing readily compatible with existing
workflows. In doing so, it opens new avenues for large-scale inquiries that
blend quantitative insights with qualitative precision. Two illustrative cases
highlight both opportunities and limitations of speech research: while genres
like #storytime on TikTok benefit from the exploration of spoken narratives,
nonverbal or music-driven content may not yield significant insights using
speech data. The article encourages researchers to integrate aural exploration
thoughtfully to complement existing methods, rather than replacing them. I
conclude that the expansion of our methodological repertoire enables richer
interpretations of platformised content, and our capacity to unpack digital
cultures as they become increasingly multimodal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages (excluding references), 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CLASP: Contrastive Language-Speech <span class="highlight-title">Pretrain</span>ing for Multilingual
  Multimodal Information Retrieval <span class="chip">ECIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Mahdi Abootorabi, Ehsaneddin Asgari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces CLASP (Contrastive Language-Speech Pretraining), a
multilingual, multimodal representation tailored for audio-text information
retrieval. CLASP leverages the synergy between spoken content and textual data.
During training, we utilize our newly introduced speech-text dataset, which
encompasses 15 diverse categories ranging from fiction to religion. CLASP's
audio component integrates audio spectrograms with a pre-trained
self-supervised speech model, while its language encoding counterpart employs a
sentence encoder pre-trained on over 100 languages. This unified lightweight
model bridges the gap between various modalities and languages, enhancing its
effectiveness in handling and retrieving multilingual and multimodal data. Our
evaluations across multiple languages demonstrate that CLASP establishes new
benchmarks in HITS@1, MRR, and meanR metrics, outperforming traditional
ASR-based retrieval approaches in specific scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at ECIR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enabling Low-Resource Language Retrieval: Establishing Baselines for
  Urdu MS MARCO <span class="chip">ECIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Umer Butt, Stalin Veranasi, Günter Neumann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the Information Retrieval (IR) field increasingly recognizes the
importance of inclusivity, addressing the needs of low-resource languages
remains a significant challenge. This paper introduces the first large-scale
Urdu IR dataset, created by translating the MS MARCO dataset through machine
translation. We establish baseline results through zero-shot learning for IR in
Urdu and subsequently apply the mMARCO multilingual IR methodology to this
newly translated dataset. Our findings demonstrate that the fine-tuned model
(Urdu-mT5-mMARCO) achieves a Mean Reciprocal Rank (MRR@10) of 0.247 and a
Recall@10 of 0.439, representing significant improvements over zero-shot
results and showing the potential for expanding IR access for Urdu speakers. By
bridging access gaps for speakers of low-resource languages, this work not only
advances multilingual IR research but also emphasizes the ethical and societal
importance of inclusive IR technologies. This work provides valuable insights
into the challenges and solutions for improving language representation and
lays the groundwork for future research, especially in South Asian languages,
which can benefit from the adaptable methods used in this study.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, ECIR 2025, conference submission version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cluster-guided Contrastive Class-imbalanced Graph Classification <span class="chip">AAAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Ju, Zhengyang Mao, Siyu Yi, Yifang Qin, Yiyang Gu, Zhiping Xiao, Jianhao Shen, Ziyue Qiao, Ming Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the problem of class-imbalanced graph classification,
which aims at effectively classifying the categories of graphs in scenarios
with imbalanced class distribution. Despite the tremendous success of graph
neural networks (GNNs), their modeling ability for imbalanced graph-structured
data is inadequate, which typically leads to predictions biased towards the
majority classes. Besides, existing class-imbalanced learning methods in
visions may overlook the rich graph semantic substructures of the majority
classes and excessively emphasize learning from the minority classes. To tackle
this issue, this paper proposes a simple yet powerful approach called C$^3$GNN
that incorporates the idea of clustering into contrastive learning to enhance
class-imbalanced graph classification. Technically, C$^3$GNN clusters graphs
from each majority class into multiple subclasses, ensuring they have similar
sizes to the minority class, thus alleviating class imbalance. Additionally, it
utilizes the Mixup technique to synthesize new samples and enrich the semantic
information of each subclass, and leverages supervised contrastive learning to
hierarchically learn effective graph representations. In this way, we can not
only sufficiently explore the semantic substructures within the majority class
but also effectively alleviate excessive focus on the minority class. Extensive
experiments on real-world graph benchmark datasets verify the superior
performance of our proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Proceedings of the Thirty-Ninth AAAI Conference on
  Artificial Intelligence (AAAI-25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Selective Shot Learning for Code Explanation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12852v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12852v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paheli Bhattacharya, Rishabh Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code explanation plays a crucial role in the software engineering domain,
aiding developers in grasping code functionality efficiently. Recent work shows
that the performance of LLMs for code explanation improves in a few-shot
setting, especially when the few-shot examples are selected intelligently.
State-of-the-art approaches for such Selective Shot Learning (SSL) include
token-based and embedding-based methods. However, these SSL approaches have
been evaluated on proprietary LLMs, without much exploration on open-source
Code-LLMs. Additionally, these methods lack consideration for programming
language syntax. To bridge these gaps, we present a comparative study and
propose a novel SSL method (SSL_ner) that utilizes entity information for
few-shot example selection. We present several insights and show the
effectiveness of SSL_ner approach over state-of-the-art methods across two
datasets. To the best of our knowledge, this is the first systematic
benchmarking of open-source Code-LLMs while assessing the performances of the
various few-shot examples selection approaches for the code explanation task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Recommendation Unlearning: Fundamentals, Taxonomy,
  Evaluation, and Open Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuyuan Li, Xiaohua Feng, Chaochao Chen, Qiang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems have become increasingly influential in shaping user
behavior and decision-making, highlighting their growing impact in various
domains. Meanwhile, the widespread adoption of machine learning models in
recommender systems has raised significant concerns regarding user privacy and
security. As compliance with privacy regulations becomes more critical, there
is a pressing need to address the issue of recommendation unlearning, i.e.,
eliminating the memory of specific training data from the learned
recommendation models. Despite its importance, traditional machine unlearning
methods are ill-suited for recommendation unlearning due to the unique
challenges posed by collaborative interactions and model parameters. This
survey offers a comprehensive review of the latest advancements in
recommendation unlearning, exploring the design principles, challenges, and
methodologies associated with this emerging field. We provide a unified
taxonomy that categorizes different recommendation unlearning approaches,
followed by a summary of widely used benchmarks and metrics for evaluation. By
reviewing the current state of research, this survey aims to guide the
development of more efficient, scalable, and robust recommendation unlearning
techniques. Furthermore, we identify open research questions in this field,
which could pave the way for future innovations not only in recommendation
unlearning but also in a broader range of unlearning tasks across different
machine learning applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Dialect Information Retrieval: Information Access in Low-Resource
  and High-Variance Languages <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Litschko, Oliver Kraus, Verena Blaschke, Barbara Plank
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A large amount of local and culture-specific knowledge (e.g., people,
traditions, food) can only be found in documents written in dialects. While
there has been extensive research conducted on cross-lingual information
retrieval (CLIR), the field of cross-dialect retrieval (CDIR) has received
limited attention. Dialect retrieval poses unique challenges due to the limited
availability of resources to train retrieval models and the high variability in
non-standardized languages. We study these challenges on the example of German
dialects and introduce the first German dialect retrieval dataset, dubbed
WikiDIR, which consists of seven German dialects extracted from Wikipedia.
Using WikiDIR, we demonstrate the weakness of lexical methods in dealing with
high lexical variation in dialects. We further show that commonly used
zero-shot cross-lingual transfer approach with multilingual encoders do not
transfer well to extremely low-resource setups, motivating the need for
resource-lean and dialect-specific retrieval models. We finally demonstrate
that (document) translation is an effective way to reduce the dialect gap in
CDIR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RemoteRAG: A Privacy-Preserving LLM Cloud RAG Service 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihang Cheng, Lan Zhang, Junyang Wang, Mu Yuan, Yunhao Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) improves the service quality of large
language models by retrieving relevant documents from credible literature and
integrating them into the context of the user query. Recently, the rise of the
cloud RAG service has made it possible for users to query relevant documents
conveniently. However, directly sending queries to the cloud brings potential
privacy leakage. In this paper, we are the first to formally define the
privacy-preserving cloud RAG service to protect the user query and propose
RemoteRAG as a solution regarding privacy, efficiency, and accuracy. For
privacy, we introduce $(n,\epsilon)$-DistanceDP to characterize privacy leakage
of the user query and the leakage inferred from relevant documents. For
efficiency, we limit the search range from the total documents to a small
number of selected documents related to a perturbed embedding generated from
$(n,\epsilon)$-DistanceDP, so that computation and communication costs required
for privacy protection significantly decrease. For accuracy, we ensure that the
small range includes target documents related to the user query with detailed
theoretical analysis. Experimental results also demonstrate that RemoteRAG can
resist existing embedding inversion attack methods while achieving no loss in
retrieval under various settings. Moreover, RemoteRAG is efficient, incurring
only $0.67$ seconds and $46.66$KB of data transmission ($2.72$ hours and $1.43$
GB with the non-optimized privacy-preserving scheme) when retrieving from a
total of $10^6$ documents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Sequential Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liwei Pan, Weike Pan, Meiyan Wei, Hongzhi Yin, Zhong Ming
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Different from most conventional recommendation problems, sequential
recommendation focuses on learning users' preferences by exploiting the
internal order and dependency among the interacted items, which has received
significant attention from both researchers and practitioners. In recent years,
we have witnessed great progress and achievements in this field, necessitating
a new survey. In this survey, we study the SR problem from a new perspective
(i.e., the construction of an item's properties), and summarize the most recent
techniques used in sequential recommendation such as pure ID-based SR, SR with
side information, multi-modal SR, generative SR, LLM-powered SR, ultra-long SR
and data-augmented SR. Moreover, we introduce some frontier research topics in
sequential recommendation, e.g., open-domain SR, data-centric SR, could-edge
collaborative SR, continuous SR, SR for good, and explainable SR. We believe
that our survey could be served as a valuable roadmap for readers in this
field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Token-Level Graphs for Short Text Classification <span class="chip">ECIR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gregor Donabauer, Udo Kruschwitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The classification of short texts is a common subtask in Information
Retrieval (IR). Recent advances in graph machine learning have led to interest
in graph-based approaches for low resource scenarios, showing promise in such
settings. However, existing methods face limitations such as not accounting for
different meanings of the same words or constraints from transductive
approaches. We propose an approach which constructs text graphs entirely based
on tokens obtained through pre-trained language models (PLMs). By applying a
PLM to tokenize and embed the texts when creating the graph(-nodes), our method
captures contextual and semantic information, overcomes vocabulary constraints,
and allows for context-dependent word meanings. Our approach also makes
classification more efficient with reduced parameters compared to classical PLM
fine-tuning, resulting in more robust training with few samples. Experimental
results demonstrate how our method consistently achieves higher scores or
on-par performance with existing methods, presenting an advancement in
graph-based text classification techniques. To support reproducibility of our
work we make all implementations publicly available to the
community\footnote{\url{https://github.com/doGregor/TokenGraph}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint accepted at the 47th European Conference on Information
  Retrieval (ECIR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SynthCypher: A Fully Synthetic Data Generation Framework for
  Text-to-Cypher Querying in Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aman Tiwari, Shiva Krishna Reddy Malay, Vikas Yadav, Masoud Hashemi, Sathwik Tejaswi Madhusudhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cypher, the query language for Neo4j graph databases, plays a critical role
in enabling graph-based analytics and data exploration. While substantial
research has been dedicated to natural language to SQL query generation
(Text2SQL), the analogous problem for graph databases referred to as
Text2Cypher remains underexplored. In this work, we introduce SynthCypher, a
fully synthetic and automated data generation pipeline designed to address this
gap. SynthCypher employs a novel LLMSupervised Generation-Verification
framework, ensuring syntactically and semantically correct Cypher queries
across diverse domains and query complexities. Using this pipeline, we create
SynthCypher Dataset, a large-scale benchmark containing 29.8k Text2Cypher
instances. Fine-tuning open-source large language models (LLMs), including
LLaMa-3.1- 8B, Mistral-7B, and QWEN-7B, on SynthCypher yields significant
performance improvements of up to 40% on the Text2Cypher test set and 30% on
the SPIDER benchmark adapted for graph databases. This work demonstrates that
high-quality synthetic data can effectively advance the state-of-the-art in
Text2Cypher tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting LLM-based Relevance Modeling with Distribution-Aware Robust
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Liu, Saisai Gong, Yixin Ji, Kaixin Wu, Jia Xu, Jinjie Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid advancement of pre-trained large language models (LLMs),
recent endeavors have leveraged the capabilities of LLMs in relevance modeling,
resulting in enhanced performance. This is usually done through the process of
fine-tuning LLMs on specifically annotated datasets to determine the relevance
between queries and items. However, there are two limitations when LLMs are
naively employed for relevance modeling through fine-tuning and inference.
First, it is not inherently efficient for performing nuanced tasks beyond
simple yes or no answers, such as assessing search relevance. It may therefore
tend to be overconfident and struggle to distinguish fine-grained degrees of
relevance (e.g., strong relevance, weak relevance, irrelevance) used in search
engines. Second, it exhibits significant performance degradation when
confronted with data distribution shift in real-world scenarios. In this paper,
we propose a novel Distribution-Aware Robust Learning framework (DaRL) for
relevance modeling in Alipay Search. Specifically, we design an effective loss
function to enhance the discriminability of LLM-based relevance modeling across
various fine-grained degrees of query-item relevance. To improve the
generalizability of LLM-based relevance modeling, we first propose the
Distribution-Aware Sample Augmentation (DASA) module. This module utilizes
out-of-distribution (OOD) detection techniques to actively select appropriate
samples that are not well covered by the original training set for model
fine-tuning. Furthermore, we adopt a multi-stage fine-tuning strategy to
simultaneously improve in-distribution (ID) and OOD performance, bridging the
performance gap between them. DaRL has been deployed online to serve the
Alipay's insurance product search...
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM is Knowledge Graph Reasoner: LLM's Intuition-aware Knowledge Graph
  Reasoning for Cold-start Sequential Recommendation <span class="chip">ECIR2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keigo Sakurai, Ren Togo, Takahiro Ogawa, Miki Haseyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge Graphs (KGs) represent relationships between entities in a graph
structure and have been widely studied as promising tools for realizing
recommendations that consider the accurate content information of items.
However, traditional KG-based recommendation methods face fundamental
challenges: insufficient consideration of temporal information and poor
performance in cold-start scenarios. On the other hand, Large Language Models
(LLMs) can be considered databases with a wealth of knowledge learned from the
web data, and they have recently gained attention due to their potential
application as recommendation systems. Although approaches that treat LLMs as
recommendation systems can leverage LLMs' high recommendation literacy, their
input token limitations make it impractical to consider the entire
recommendation domain dataset and result in scalability issues. To address
these challenges, we propose a LLM's Intuition-aware Knowledge graph Reasoning
model (LIKR). Our main idea is to treat LLMs as reasoners that output intuitive
exploration strategies for KGs. To integrate the knowledge of LLMs and KGs, we
trained a recommendation agent through reinforcement learning using a reward
function that integrates different recommendation strategies, including LLM's
intuition and KG embeddings. By incorporating temporal awareness through prompt
engineering and generating textual representations of user preferences from
limited interactions, LIKR can improve recommendation performance in cold-start
scenarios. Furthermore, LIKR can avoid scalability issues by using KGs to
represent recommendation domain datasets and limiting the LLM's output to KG
exploration strategies. Experiments on real-world datasets demonstrate that our
model outperforms state-of-the-art recommendation methods in cold-start
sequential recommendation scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the 47th European Conference on Information Retrieval
  (ECIR2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LITA: An Efficient LLM-assisted Iterative Topic Augmentation Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chia-Hsuan Chang, Jui-Tse Tsai, Yi-Hang Tsai, San-Yih Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic modeling is widely used for uncovering thematic structures within text
corpora, yet traditional models often struggle with specificity and coherence
in domain-focused applications. Guided approaches, such as SeededLDA and CorEx,
incorporate user-provided seed words to improve relevance but remain
labor-intensive and static. Large language models (LLMs) offer potential for
dynamic topic refinement and discovery, yet their application often incurs high
API costs. To address these challenges, we propose the LLM-assisted Iterative
Topic Augmentation framework (LITA), an LLM-assisted approach that integrates
user-provided seeds with embedding-based clustering and iterative refinement.
LITA identifies a small number of ambiguous documents and employs an LLM to
reassign them to existing or new topics, minimizing API costs while enhancing
topic quality. Experiments on two datasets across topic quality and clustering
performance metrics demonstrate that LITA outperforms five baseline models,
including LDA, SeededLDA, CorEx, BERTopic, and PromptTopic. Our work offers an
efficient and adaptable framework for advancing topic modeling and text
clustering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Refining Dimensions for Improving Clustering-based Cross-lingual Topic
  Models <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chia-Hsuan Chang, Tien-Yuan Huang, Yi-Hang Tsai, Chia-Ming Chang, San-Yih Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works in clustering-based topic models perform well in monolingual
topic identification by introducing a pipeline to cluster the contextualized
representations. However, the pipeline is suboptimal in identifying topics
across languages due to the presence of language-dependent dimensions (LDDs)
generated by multilingual language models. To address this issue, we introduce
a novel, SVD-based dimension refinement component into the pipeline of the
clustering-based topic model. This component effectively neutralizes the
negative impact of LDDs, enabling the model to accurately identify topics
across languages. Our experiments on three datasets demonstrate that the
updated pipeline with the dimension refinement component generally outperforms
other state-of-the-art cross-lingual topic models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 18th BUCC Workshop at COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FM2DS: Few-Shot Multimodal Multihop Data Synthesis with Knowledge
  Distillation for Question Answering <span class="chip">CVPR 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07030v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07030v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amirhossein Abaskohi, Spandana Gella, Giuseppe Carenini, Issam H. Laradji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal multihop question answering is a complex task that requires
reasoning over multiple sources of information, such as images and text, to
answer questions. While there has been significant progress in visual question
answering, the multihop setting remains unexplored due to the lack of
high-quality datasets. Current methods focus on single-hop question answering
or a single modality, which makes them unsuitable for real-world scenarios such
as analyzing multimodal educational materials, summarizing lengthy academic
articles, or interpreting scientific studies that combine charts, images, and
text. To address this gap, we propose a novel methodology, introducing the
first framework for creating a high-quality dataset that enables training
models for multimodal multihop question answering. Our approach consists of a
5-stage pipeline that involves acquiring relevant multimodal documents from
Wikipedia, synthetically generating high-level questions and answers, and
validating them through rigorous criteria to ensure quality data. We evaluate
our methodology by training models on our synthesized dataset and testing on
two benchmarks, our results demonstrate that, with an equal sample size, models
trained on our synthesized data outperform those trained on human-collected
data by 1.9 in exact match (EM) on average. We believe our data synthesis
method will serve as a strong foundation for training and evaluating multimodal
multihop question answering models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 11 figures, 10 tables, Submitted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ It is Never Too Late to Mend: Separate Learning for Multimedia
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.08270v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.08270v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuangzhuang He, Zihan Wang, Yonghui Yang, Haoyue Bai, Le Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimedia recommendation, which incorporates various modalities (e.g.,
images, texts, etc.) into user or item representation to improve recommendation
quality, and self-supervised learning carries multimedia recommendation to a
plateau of performance, because of its superior performance in aligning
different modalities. However, more and more research finds that aligning all
modal representations is suboptimal because it damages the unique attributes of
each modal. These studies use subtraction and orthogonal constraints in
geometric space to learn unique parts. However, our rigorous analysis reveals
the flaws in this approach, such as that subtraction does not necessarily yield
the desired modal-unique and that orthogonal constraints are ineffective in
user and item high-dimensional representation spaces. To make up for the
previous weaknesses, we propose Separate Learning (SEA) for multimedia
recommendation, which mainly includes mutual information view of modal-unique
and -generic learning. Specifically, we first use GNN to learn the
representations of users and items in different modalities and split each modal
representation into generic and unique parts. We employ contrastive log-ratio
upper bound to minimize the mutual information between the general and unique
parts within the same modality, to distance their representations, thus
learning modal-unique features. Then, we design Solosimloss to maximize the
lower bound of mutual information, to align the general parts of different
modalities, thus learning more high-quality modal-generic features. Finally,
extensive experiments on three datasets demonstrate the effectiveness and
generalization of our proposed framework. The code is available at SEA and the
full training record of the main experiment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Modal Recommendation Unlearning for Legal, Licensing, and Modality
  Constraints <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15328v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15328v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Sinha, Murari Mandal, Mohan Kankanhalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User data spread across multiple modalities has popularized multi-modal
recommender systems (MMRS). They recommend diverse content such as products,
social media posts, TikTok reels, etc., based on a user-item interaction graph.
With rising data privacy demands, recent methods propose unlearning private
user data from uni-modal recommender systems (RS). However, methods for
unlearning item data related to outdated user preferences, revoked licenses,
and legally requested removals are still largely unexplored.
  Previous RS unlearning methods are unsuitable for MMRS due to the
incompatibility of their matrix-based representation with the multi-modal
user-item interaction graph. Moreover, their data partitioning step degrades
performance on each shard due to poor data heterogeneity and requires costly
performance aggregation across shards.
  This paper introduces MMRecUn, the first approach known to us for unlearning
in MMRS and unlearning item data. Given a trained RS model, MMRecUn employs a
novel Reverse Bayesian Personalized Ranking (BPR) objective to enable the model
to forget marked data. The reverse BPR attenuates the impact of user-item
interactions within the forget set, while the forward BPR reinforces the
significance of user-item interactions within the retain set. Our experiments
demonstrate that MMRecUn outperforms baseline methods across various unlearning
requests when evaluated on benchmark MMRS datasets. MMRecUn achieves recall
performance improvements of up to 49.85% compared to baseline methods and is up
to $\mathbf{1.3}\times$ faster than the Gold model, which is trained on retain
set from scratch. MMRecUn offers significant advantages, including superiority
in removing target interactions, preserving retained interactions, and zero
overhead costs compared to previous methods. The code will be released after
review.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-autoregressive Generative Models for Reranking Recommendation <span class="chip">KDD 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06871v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06871v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxin Ren, Qiya Yang, Yichun Wu, Wei Xu, Yalong Wang, Zhiqiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contemporary recommendation systems are designed to meet users' needs by
delivering tailored lists of items that align with their specific demands or
interests. In a multi-stage recommendation system, reranking plays a crucial
role by modeling the intra-list correlations among items. The key challenge of
reranking lies in the exploration of optimal sequences within the combinatorial
space of permutations. Recent research proposes a generator-evaluator learning
paradigm, where the generator generates multiple feasible sequences and the
evaluator picks out the best sequence based on the estimated listwise score.
The generator is of vital importance, and generative models are well-suited for
the generator function. Current generative models employ an autoregressive
strategy for sequence generation. However, deploying autoregressive models in
real-time industrial systems is challenging. To address these issues, we
propose a Non-AutoRegressive generative model for reranking Recommendation
(NAR4Rec) designed to enhance efficiency and effectiveness. To tackle
challenges such as sparse training samples and dynamic candidates, we introduce
a matching model. Considering the diverse nature of user feedback, we employ a
sequence-level unlikelihood training objective to differentiate feasible
sequences from unfeasible ones. Additionally, to overcome the lack of
dependency modeling in non-autoregressive models regarding target items, we
introduce contrastive decoding to capture correlations among these items.
Extensive offline experiments validate the superior performance of NAR4Rec over
state-of-the-art reranking methods. Online A/B tests reveal that NAR4Rec
significantly enhances the user experience. Furthermore, NAR4Rec has been fully
deployed in a popular video app Kuaishou with over 300 million daily active
users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Familiarity-Aware Evidence Compression for Retrieval-Augmented
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12468v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12468v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongwon Jung, Qin Liu, Tenghao Huang, Ben Zhou, Muhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) improves large language models (LMs) by
incorporating non-parametric knowledge through evidence retrieved from external
sources. However, it often struggles to cope with inconsistent and irrelevant
information that can distract the LM from its tasks, especially when multiple
evidence pieces are required. While compressing the retrieved evidence with a
compression model aims to address this issue, the compressed evidence may still
be unfamiliar to the target model used for downstream tasks, potentially
failing to utilize the evidence effectively. We propose FaviComp
(Familarity-Aware Evidence Compression), a novel training-free evidence
compression technique that makes retrieved evidence more familiar to the target
model, while seamlessly integrating parametric knowledge from the model.
Experimental results show that FaviComp consistently outperforms most recent
evidence compression baselines across multiple open-domain QA datasets,
improving accuracy by up to 28.1% while achieving high compression rates.
Additionally, we demonstrate the effective integration of both parametric and
non-parametric knowledge during evidence compression.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Impression-Aware Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.07857v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.07857v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando B. Pérez Maurera, Maurizio Ferrari Dacrema, Pablo Castells, Paolo Cremonesi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Novel data sources bring new opportunities to improve the quality of
recommender systems and serve as a catalyst for the creation of new paradigms
on personalized recommendations. Impressions are a novel data source containing
the items shown to users on their screens. Past research focused on providing
personalized recommendations using interactions, and occasionally using
impressions when such a data source was available. Interest in impressions has
increased due to their potential to provide more accurate recommendations.
Despite this increased interest, research in recommender systems using
impressions is still dispersed. Many works have distinct interpretations of
impressions and use impressions in recommender systems in numerous different
manners. To unify those interpretations into a single framework, we present a
systematic literature review on recommender systems using impressions, focusing
on three fundamental perspectives: recommendation models, datasets, and
evaluation methodologies. We define a theoretical framework to delimit
recommender systems using impressions and a novel paradigm for personalized
recommendations, called impression-aware recommender systems. We propose a
classification system for recommenders in this paradigm, which we use to
categorize the recommendation models, datasets, and evaluation methodologies
used in past research. Lastly, we identify open questions and future
directions, highlighting missing aspects in the reviewed literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, 127 references, 6 tables, 5 figures, ACM TORS ACCEPTED</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-16T00:00:00Z">2024-12-16</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">20</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Searching Personal Collections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12330v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12330v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Bendersky, Donald Metzler, Marc Najork, Xuanhui Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article describes the history of information retrieval on personal
document collections.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAG Playground: A Framework for Systematic Evaluation of Retrieval
  Strategies and <span class="highlight-title">Prompt</span> Engineering in RAG Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12322v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12322v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioannis Papadimitriou, Ilias Gialampoukidis, Stefanos Vrochidis,  Ioannis,  Kompatsiaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present RAG Playground, an open-source framework for systematic evaluation
of Retrieval-Augmented Generation (RAG) systems. The framework implements and
compares three retrieval approaches: naive vector search, reranking, and hybrid
vector-keyword search, combined with ReAct agents using different prompting
strategies. We introduce a comprehensive evaluation framework with novel
metrics and provide empirical results comparing different language models
(Llama 3.1 and Qwen 2.5) across various retrieval configurations. Our
experiments demonstrate significant performance improvements through hybrid
search methods and structured self-evaluation prompting, achieving up to 72.7%
pass rate on our multi-metric evaluation framework. The results also highlight
the importance of prompt engineering in RAG systems, with our custom-prompted
agents showing consistent improvements in retrieval accuracy and response
quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work In Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ No More Tuning: Prioritized Multi-Task Learning with Lagrangian
  Differential Multiplier Methods <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengxing Cheng, Yuheng Huang, Zhixuan Zhang, Dan Ou, Qingwen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the ubiquity of multi-task in practical systems, Multi-Task Learning
(MTL) has found widespread application across diverse domains. In real-world
scenarios, these tasks often have different priorities. For instance, In web
search, relevance is often prioritized over other metrics, such as
click-through rates or user engagement. Existing frameworks pay insufficient
attention to the prioritization among different tasks, which typically adjust
task-specific loss function weights to differentiate task priorities. However,
this approach encounters challenges as the number of tasks grows, leading to
exponential increases in hyper-parameter tuning complexity. Furthermore, the
simultaneous optimization of multiple objectives can negatively impact the
performance of high-priority tasks due to interference from lower-priority
tasks.
  In this paper, we introduce a novel multi-task learning framework employing
Lagrangian Differential Multiplier Methods for step-wise multi-task
optimization. It is designed to boost the performance of high-priority tasks
without interference from other tasks. Its primary advantage lies in its
ability to automatically optimize multiple objectives without requiring
balancing hyper-parameters for different tasks, thereby eliminating the need
for manual tuning. Additionally, we provide theoretical analysis demonstrating
that our method ensures optimization guarantees, enhancing the reliability of
the process. We demonstrate its effectiveness through experiments on multiple
public datasets and its application in Taobao search, a large-scale industrial
search ranking system, resulting in significant improvements across various
business metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RetroLLM: Empowering Large Language Models to Retrieve Fine-grained
  Evidence within Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11919v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11919v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yongkang Wu, Zhonghua Li, Qi Ye, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit remarkable generative capabilities but
often suffer from hallucinations. Retrieval-augmented generation (RAG) offers
an effective solution by incorporating external knowledge, but existing methods
still face several limitations: additional deployment costs of separate
retrievers, redundant input tokens from retrieved text chunks, and the lack of
joint optimization of retrieval and generation. To address these issues, we
propose \textbf{RetroLLM}, a unified framework that integrates retrieval and
generation into a single, cohesive process, enabling LLMs to directly generate
fine-grained evidence from the corpus with constrained decoding. Moreover, to
mitigate false pruning in the process of constrained evidence generation, we
introduce (1) hierarchical FM-Index constraints, which generate
corpus-constrained clues to identify a subset of relevant documents before
evidence generation, reducing irrelevant decoding space; and (2) a
forward-looking constrained decoding strategy, which considers the relevance of
future sequences to improve evidence accuracy. Extensive experiments on five
open-domain QA datasets demonstrate RetroLLM's superior performance across both
in-domain and out-of-domain tasks. The code is available at
\url{https://github.com/sunnynexus/RetroLLM}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Investigating Mixture of Experts in Dense Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Effrosyni Sokli, Pranav Kasela, Georgios Peikos, Gabriella Pasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Dense Retrieval Models (DRMs) have advanced Information Retrieval (IR),
one limitation of these neural models is their narrow generalizability and
robustness. To cope with this issue, one can leverage the Mixture-of-Experts
(MoE) architecture. While previous IR studies have incorporated MoE
architectures within the Transformer layers of DRMs, our work investigates an
architecture that integrates a single MoE block (SB-MoE) after the output of
the final Transformer layer. Our empirical evaluation investigates how SB-MoE
compares, in terms of retrieval effectiveness, to standard fine-tuning. In
detail, we fine-tune three DRMs (TinyBERT, BERT, and Contriever) across four
benchmark collections with and without adding the MoE block. Moreover, since
MoE showcases performance variations with respect to its parameters (i.e., the
number of experts), we conduct additional experiments to investigate this
aspect further. The findings show the effectiveness of SB-MoE especially for
DRMs with a low number of parameters (i.e., TinyBERT), as it consistently
outperforms the fine-tuned underlying model on all four benchmarks. For DRMs
with a higher number of parameters (i.e., BERT and Contriever), SB-MoE requires
larger numbers of training samples to yield better retrieval performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SPGL: Enhancing Session-based Recommendation with Single Positive Graph
  Learning <span class="chip">ICONIP 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11846v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11846v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiantian Liang, Zhe Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Session-based recommendation seeks to forecast the next item a user will be
interested in, based on their interaction sequences. Due to limited interaction
data, session-based recommendation faces the challenge of limited data
availability. Traditional methods enhance feature learning by constructing
complex models to generate positive and negative samples. This paper proposes a
session-based recommendation model using Single Positive optimization loss and
Graph Learning (SPGL) to deal with the problem of data sparsity, high model
complexity and weak transferability. SPGL utilizes graph convolutional networks
to generate global item representations and batch session representations,
effectively capturing intrinsic relationships between items. The use of single
positive optimization loss improves uniformity of item representations, thereby
enhancing recommendation accuracy. In the intent extractor, SPGL considers the
hop count of the adjacency matrix when constructing the directed global graph
to fully integrate spatial information. It also takes into account the reverse
positional information of items when constructing session representations to
incorporate temporal information. Comparative experiments across three
benchmark datasets, Tmall, RetailRocket and Diginetica, demonstrate the model's
effectiveness. The source code can be accessed on
https://github.com/liang-tian-tian/SPGL .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICONIP 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Distributed Collaborative Retrieval Framework Excelling in All Queries
  and Corpora based on Zero-shot Rank-Oriented Automatic Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian-Yi Che, Xian-Ling Mao, Chun Xu, Cheng-Xin Xin, Heng-Da Xu, Jin-Yu Liu, Heyan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous retrieval models, including sparse, dense and llm-based methods,
have demonstrated remarkable performance in predicting the relevance between
queries and corpora. However, the preliminary effectiveness analysis
experiments indicate that these models fail to achieve satisfactory performance
on the majority of queries and corpora, revealing their effectiveness
restricted to specific scenarios. Thus, to tackle this problem, we propose a
novel Distributed Collaborative Retrieval Framework (DCRF), outperforming each
single model across all queries and corpora. Specifically, the framework
integrates various retrieval models into a unified system and dynamically
selects the optimal results for each user's query. It can easily aggregate any
retrieval model and expand to any application scenarios, illustrating its
flexibility and scalability.Moreover, to reduce maintenance and training costs,
we design four effective prompting strategies with large language models (LLMs)
to evaluate the quality of ranks without reliance of labeled data. Extensive
experiments demonstrate that proposed framework, combined with 8 efficient
retrieval models, can achieve performance comparable to effective listwise
methods like RankGPT and ListT5, while offering superior efficiency. Besides,
DCRF surpasses all selected retrieval models on the most datasets, indicating
the effectiveness of our prompting strategies on rank-oriented automatic
evaluation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging User-Generated Metadata of Online Videos for Cover Song
  Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11818v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11818v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Hachmeier, Robert Jäschke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  YouTube is a rich source of cover songs. Since the platform itself is
organized in terms of videos rather than songs, the retrieval of covers is not
trivial. The field of cover song identification addresses this problem and
provides approaches that usually rely on audio content. However, including the
user-generated video metadata available on YouTube promises improved
identification results. In this paper, we propose a multi-modal approach for
cover song identification on online video platforms. We combine the entity
resolution models with audio-based approaches using a ranking model. Our
findings implicate that leveraging user-generated metadata can stabilize cover
song identification performance on YouTube.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted for presentation at NLP for Music and Audio (NLP4MusA) 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Method for Detecting Legal Article Competition for Korean Criminal Law
  Using a Case-augmented Mention Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonho An, Young Yik Rhim, Min-Soo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As social systems become increasingly complex, legal articles are also
growing more intricate, making it progressively harder for humans to identify
any potential competitions among them, particularly when drafting new laws or
applying existing laws. Despite this challenge, no method for detecting such
competitions has been proposed so far. In this paper, we propose a new legal AI
task called Legal Article Competition Detection (LACD), which aims to identify
competing articles within a given law. Our novel retrieval method, CAM-Re2,
outperforms existing relevant methods, reducing false positives by 20.8% and
false negatives by 8.3%, while achieving a 98.2% improvement in precision@5,
for the LACD task. We release our codes at
https://github.com/asmath472/LACD-public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Establishing a Foundation for Tetun Text Ad-Hoc Retrieval: Indexing,
  Stemming, Retrieval, and Ranking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11758v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11758v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel de Jesus, Sérgio Nunes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Searching for information on the internet and digital platforms to satisfy an
information need requires effective retrieval solutions. However, such
solutions are not yet available for Tetun, making it challenging to find
relevant documents for text-based search queries in this language. To address
these challenges, this study investigates Tetun text retrieval with a focus on
the ad-hoc retrieval task. It begins by developing essential language resources
-- including a list of stopwords, a stemmer, and a test collection -- which
serve as foundational components for solutions tailored to Tetun text
retrieval. Various strategies are then explored using both document titles and
content to evaluate retrieval effectiveness. The results show that retrieving
document titles, after removing hyphens and apostrophes without applying
stemming, significantly improves retrieval performance compared to the
baseline. Efficiency increases by 31.37%, while effectiveness achieves an
average gain of 9.40% in MAP@10 and 30.35% in nDCG@10 with DFR BM25. Beyond the
top-10 cutoff point, Hiemstra LM demonstrates strong performance across various
retrieval strategies and evaluation metrics. Contributions of this work include
the development of Labadain-Stopwords (a list of 160 Tetun stopwords),
Labadain-Stemmer (a Tetun stemmer with three variants), and
Labadain-Avaliad\'or (a Tetun test collection containing 59 topics, 33,550
documents, and 5,900 qrels).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Graph Convolution: Multimodal Recommendation with Topology-aware
  MLPs <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Huang, Jiarui Qin, Yong Yu, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the large volume of side information from different modalities,
multimodal recommender systems have become increasingly vital, as they exploit
richer semantic information beyond user-item interactions. Recent works
highlight that leveraging Graph Convolutional Networks (GCNs) to explicitly
model multimodal item-item relations can significantly enhance recommendation
performance. However, due to the inherent over-smoothing issue of GCNs,
existing models benefit only from shallow GCNs with limited representation
power. This drawback is especially pronounced when facing complex and
high-dimensional patterns such as multimodal data, as it requires
large-capacity models to accommodate complicated correlations. To this end, in
this paper, we investigate bypassing GCNs when modeling multimodal item-item
relationship. More specifically, we propose a Topology-aware Multi-Layer
Perceptron (TMLP), which uses MLPs instead of GCNs to model the relationships
between items. TMLP enhances MLPs with topological pruning to denoise item-item
relations and intra (inter)-modality learning to integrate higher-order
modality correlations. Extensive experiments on three real-world datasets
verify TMLP's superiority over nine baselines. We also find that by discarding
the internal message passing in GCNs, which is sensitive to node connections,
TMLP achieves significant improvements in both training efficiency and
robustness against existing models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAAI 2025. 11 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STAIR: Manipulating Collaborative and Multimodal Information for
  E-Commerce Recommendation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Xu, Yunhang He, Jun Wang, Wei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the mining of modalities is the focus of most multimodal recommendation
methods, we believe that how to fully utilize both collaborative and multimodal
information is pivotal in e-commerce scenarios where, as clarified in this
work, the user behaviors are rarely determined entirely by multimodal features.
In order to combine the two distinct types of information, some additional
challenges are encountered: 1) Modality erasure: Vanilla graph convolution,
which proves rather useful in collaborative filtering, however erases
multimodal information; 2) Modality forgetting: Multimodal information tends to
be gradually forgotten as the recommendation loss essentially facilitates the
learning of collaborative information. To this end, we propose a novel approach
named STAIR, which employs a novel STepwise grAph convolution to enable a
co-existence of collaborative and multimodal Information in e-commerce
Recommendation. Besides, it starts with the raw multimodal features as an
initialization, and the forgetting problem can be significantly alleviated
through constrained embedding updates. As a result, STAIR achieves
state-of-the-art recommendation performance on three public e-commerce datasets
with minimal computational and memory costs. Our code is available at
https://github.com/yhhe2004/STAIR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Future Sight and Tough Fights: Revolutionizing Sequential Recommendation
  with FENRec <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Hsuan Huang, Ling Lo, Hongxia Xie, Hong-Han Shuai, Wen-Huang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation (SR) systems predict user preferences by analyzing
time-ordered interaction sequences. A common challenge for SR is data sparsity,
as users typically interact with only a limited number of items. While
contrastive learning has been employed in previous approaches to address the
challenges, these methods often adopt binary labels, missing finer patterns and
overlooking detailed information in subsequent behaviors of users.
Additionally, they rely on random sampling to select negatives in contrastive
learning, which may not yield sufficiently hard negatives during later training
stages. In this paper, we propose Future data utilization with Enduring
Negatives for contrastive learning in sequential Recommendation (FENRec). Our
approach aims to leverage future data with time-dependent soft labels and
generate enduring hard negatives from existing data, thereby enhancing the
effectiveness in tackling data sparsity. Experiment results demonstrate our
state-of-the-art performance across four benchmark datasets, with an average
improvement of 6.16\% across all metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Healthcare Recommendation Systems with a Multimodal LLMs-based
  MOE Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyu Xu, Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing availability of multimodal data, many fields urgently
require advanced architectures capable of effectively integrating these diverse
data sources to address specific problems. This study proposes a hybrid
recommendation model that combines the Mixture of Experts (MOE) framework with
large language models to enhance the performance of recommendation systems in
the healthcare domain. We built a small dataset for recommending healthy food
based on patient descriptions and evaluated the model's performance on several
key metrics, including Precision, Recall, NDCG, and MAP@5. The experimental
results show that the hybrid model outperforms the baseline models, which use
MOE or large language models individually, in terms of both accuracy and
personalized recommendation effectiveness. The paper finds image data provided
relatively limited improvement in the performance of the personalized
recommendation system, particularly in addressing the cold start problem. Then,
the issue of reclassification of images also affected the recommendation
results, especially when dealing with low-quality images or changes in the
appearance of items, leading to suboptimal performance. The findings provide
valuable insights into the development of powerful, scalable, and
high-performance recommendation systems, advancing the application of
personalized recommendation technologies in real-world domains such as
healthcare.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 page, accpted by Conf-SMPL conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimized Quran Passage Retrieval Using an Expanded QA <span class="highlight-title">Dataset</span> and
  Fine-Tuned Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Basem, Islam Oshallah, Baraa Hikal, Ali Hamdi, Ammar Mohamed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the deep meanings of the Qur'an and bridging the language gap
between modern standard Arabic and classical Arabic is essential to improve the
question-and-answer system for the Holy Qur'an. The Qur'an QA 2023 shared task
dataset had a limited number of questions with weak model retrieval. To address
this challenge, this work updated the original dataset and improved the model
accuracy. The original dataset, which contains 251 questions, was reviewed and
expanded to 629 questions with question diversification and reformulation,
leading to a comprehensive set of 1895 categorized into single-answer,
multi-answer, and zero-answer types. Extensive experiments fine-tuned
transformer models, including AraBERT, RoBERTa, CAMeLBERT, AraELECTRA, and
BERT. The best model, AraBERT-base, achieved a MAP@10 of 0.36 and MRR of 0.59,
representing improvements of 63% and 59%, respectively, compared to the
baseline scores (MAP@10: 0.22, MRR: 0.37). Additionally, the dataset expansion
led to improvements in handling "no answer" cases, with the proposed approach
achieving a 75% success rate for such instances, compared to the baseline's
25%. These results demonstrate the effect of dataset improvement and model
architecture optimization in increasing the performance of QA systems for the
Holy Qur'an, with higher accuracy, recall, and precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CURE: A <span class="highlight-title">dataset</span> for Clinical Understanding & Retrieval Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06954v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06954v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadia Sheikh, Anne-Laure Jousse, Daniel Buades Marcos, Akintunde Oladipo, Olivier Rousseau, Jimmy Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the dominance of dense retrievers that do not generalize well beyond
their training dataset distributions, domain-specific test sets are essential
in evaluating retrieval. There are few test datasets for retrieval systems
intended for use by healthcare providers in a point-of-care setting. To fill
this gap we have collaborated with medical professionals to create CURE, an
ad-hoc retrieval test dataset for passage ranking with 2000 queries spanning 10
medical domains with a monolingual (English) and two cross-lingual
(French/Spanish -> English) conditions. In this paper, we describe how CURE was
constructed and provide baseline results to showcase its effectiveness as an
evaluation tool. CURE is published with a Creative Commons Attribution Non
Commercial 4.0 license and can be accessed on Hugging Face.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unified Multimodal Interleaved Document Representation for Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaewoo Lee, Joonho Ko, Jinheon Baek, Soyeong Jeong, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information Retrieval (IR) methods aim to identify documents relevant to a
query, which have been widely applied in various natural language tasks.
However, existing approaches typically consider only the textual content within
documents, overlooking the fact that documents can contain multiple modalities,
including images and tables. Also, they often segment each long document into
multiple discrete passages for embedding, which prevents them from capturing
the overall document context and interactions between paragraphs. To address
these two challenges, we propose a method that holistically embeds documents
interleaved with multiple modalities by leveraging the capability of recent
vision-language models that enable the processing and integration of text,
images, and tables into a unified format and representation. Moreover, to
mitigate the information loss from segmenting documents into passages, instead
of representing and retrieving passages individually, we further merge the
representations of segmented passages into one single document representation,
while we additionally introduce a reranking strategy to decouple and identify
the relevant passage within the document if necessary. Then, through extensive
experiments on diverse IR scenarios considering both the textual and multimodal
queries, we show that our approach substantially outperforms relevant
baselines, thanks to the consideration of the multimodal information within
documents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attention-Seeker: Dynamic Self-Attention Scoring for Unsupervised
  Keyphrase Extraction <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10907v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10907v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erwin D. López Z., Cheng Tang, Atsushi Shimada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes Attention-Seeker, an unsupervised keyphrase extraction
method that leverages self-attention maps from a Large Language Model to
estimate the importance of candidate phrases. Our approach identifies specific
components - such as layers, heads, and attention vectors - where the model
pays significant attention to the key topics of the text. The attention weights
provided by these components are then used to score the candidate phrases.
Unlike previous models that require manual tuning of parameters (e.g.,
selection of heads, prompts, hyperparameters), Attention-Seeker dynamically
adapts to the input text without any manual adjustments, enhancing its
practical applicability. We evaluate Attention-Seeker on four publicly
available datasets: Inspec, SemEval2010, SemEval2017, and Krapivin. Our results
demonstrate that, even without parameter tuning, Attention-Seeker outperforms
most baseline models, achieving state-of-the-art performance on three out of
four datasets, particularly excelling in extracting keyphrases from long
documents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This version has been accepted for presentation at COLING 2025, and
  all peer-reviewed changes have been incorporated</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text Proxy: Decomposing Retrieval from a 1-to-N Relationship into N
  1-to-1 Relationships for Text-Video Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06618v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06618v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Xiao, Zhenzhen Hu, Jia Li, Richang Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-video retrieval (TVR) has seen substantial advancements in recent years,
fueled by the utilization of pre-trained models and large language models
(LLMs). Despite these advancements, achieving accurate matching in TVR remains
challenging due to inherent disparities between video and textual modalities
and irregularities in data representation. In this paper, we propose
Text-Video-ProxyNet (TV-ProxyNet), a novel framework designed to decompose the
conventional 1-to-N relationship of TVR into N distinct 1-to-1 relationships.
By replacing a single text query with a series of text proxies, TV-ProxyNet not
only broadens the query scope but also achieves a more precise expansion. Each
text proxy is crafted through a refined iterative process, controlled by
mechanisms we term as the director and dash, which regulate the proxy's
direction and distance relative to the original text query. This setup not only
facilitates more precise semantic alignment but also effectively manages the
disparities and noise inherent in multimodal data. Our experiments on three
representative video-text retrieval benchmarks, MSRVTT, DiDeMo, and ActivityNet
Captions, demonstrate the effectiveness of TV-ProxyNet. The results show an
improvement of 2.0% to 3.3% in R@1 over the baseline. TV-ProxyNet achieved
state-of-the-art performance on MSRVTT and ActivityNet Captions, and a 2.0%
improvement on DiDeMo compared to existing methods, validating our approach's
ability to enhance semantic mapping and reduce error propensity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predictive Models in Sequential Recommendations: Bridging Performance
  Laws with Data Quality Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00430v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00430v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingjia Shen, Hao Wang, Chuhan Wu, Jin Yao Chin, Wei Guo, Yong Liu, Huifeng Guo, Defu Lian, Ruiming Tang, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential Recommendation (SR) plays a critical role in predicting users'
sequential preferences. Despite its growing prominence in various industries,
the increasing scale of SR models incurs substantial computational costs and
unpredictability, challenging developers to manage resources efficiently. Under
this predicament, Scaling Laws have achieved significant success by examining
the loss as models scale up. However, there remains a disparity between loss
and model performance, which is of greater concern in practical applications.
Moreover, as data continues to expand, it incorporates repetitive and
inefficient data. In response, we introduce the Performance Law for SR models,
which aims to theoretically investigate and model the relationship between
model performance and data quality. Specifically, we first fit the HR and NDCG
metrics to transformer-based SR models. Subsequently, we propose Approximate
Entropy (ApEn) to assess data quality, presenting a more nuanced approach
compared to traditional data quantity metrics. Our method enables accurate
predictions across various dataset scales and model sizes, demonstrating a
strong correlation in large SR models and offering insights into achieving
optimal performance for any given model configuration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-15T00:00:00Z">2024-12-15</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">7</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Task-Oriented Dialog Systems for the Senegalese Wolof Language <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Derguene Mbaye, Moussa Diallo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, we are seeing considerable interest in conversational agents
with the rise of large language models (LLMs). Although they offer considerable
advantages, LLMs also present significant risks, such as hallucination, which
hinder their widespread deployment in industry. Moreover, low-resource
languages such as African ones are still underrepresented in these systems
limiting their performance in these languages. In this paper, we illustrate a
more classical approach based on modular architectures of Task-oriented Dialog
Systems (ToDS) offering better control over outputs. We propose a chatbot
generation engine based on the Rasa framework and a robust methodology for
projecting annotations onto the Wolof language using an in-house machine
translation system. After evaluating a generated chatbot trained on the Amazon
Massive dataset, our Wolof Intent Classifier performs similarly to the one
obtained for French, which is a resource-rich language. We also show that this
approach is extensible to other low-resource languages, thanks to the intent
classifier's language-agnostic pipeline, simplifying the design of chatbots in
these languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 tables, 6 figures, The 31st International Conference on
  Computational Linguistics (COLING 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A multi-theoretical kernel-based approach to social network-based
  recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Li, Mengyue Wang, T. -P. Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are a critical component of e-commercewebsites. The rapid
development of online social networking services provides an opportunity to
explore social networks together with information used in traditional
recommender systems, such as customer demographics, product characteristics,
and transactions. It also provides more applications for recommender systems.
To tackle this social network-based recommendation problem, previous studies
generally built trust models in light of the social influence theory. This
study inspects a spectrumof social network theories to systematicallymodel
themultiple facets of a social network and infer user preferences. In order to
effectively make use of these heterogonous theories, we take a kernel-based
machine learning paradigm, design and select kernels describing individual
similarities according to social network theories, and employ a non-linear
multiple kernel learning algorithm to combine the kernels into a unified model.
This design also enables us to consider multiple theories' interactions in
assessing individual behaviors. We evaluate our proposed approach on a
real-world movie review data set. The experiments show that our approach
provides more accurate recommendations than trust-based methods and the
collaborative filtering approach. Further analysis shows that kernels derived
from contagion theory and homophily theory contribute a larger portion of the
model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling the Heterogeneous Duration of User Interest in Time-Dependent
  Recommendation: A Hidden Semi-Markov Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11127v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11127v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haidong Zhang, Wancheng Ni, Xin Li, Yiping Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender systems are widely used for suggesting books, education
materials, and products to users by exploring their behaviors. In reality,
users' preferences often change over time, leading to studies on time-dependent
recommender systems. However, most existing approaches that deal with time
information remain primitive. In this paper, we extend existing methods and
propose a hidden semi-Markov model to track the change of users' interests.
Particularly, this model allows for capturing the different durations of user
stays in a (latent) interest state, which can better model the heterogeneity of
user interests and focuses. We derive an expectation maximization algorithm to
estimate the parameters of the framework and predict users' actions.
Experiments on three real-world datasets show that our model significantly
outperforms the state-of-the-art time-dependent and static benchmark methods.
Further analyses of the experiment results indicate that the performance
improvement is related to the heterogeneity of state durations and the drift of
user interests in the dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Graph Co-Training for Capturing User Intent in Session-based
  Recommendation <span class="chip">COLING 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11105v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11105v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhe Yang, Tiantian Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Session-based recommendation focuses on predicting the next item a user will
interact with based on sequences of anonymous user sessions. A significant
challenge in this field is data sparsity due to the typically short-term
interactions. Most existing methods rely heavily on users' current
interactions, overlooking the wealth of auxiliary information available. To
address this, we propose a novel model, the Multi-Graph Co-Training model
(MGCOT), which leverages not only the current session graph but also similar
session graphs and a global item relation graph. This approach allows for a
more comprehensive exploration of intrinsic relationships and better captures
user intent from multiple views, enabling session representations to complement
each other. Additionally, MGCOT employs multi-head attention mechanisms to
effectively capture relevant session intent and uses contrastive learning to
form accurate and robust session representations. Extensive experiments on
three datasets demonstrate that MGCOT significantly enhances the performance of
session-based recommendations, particularly on the Diginetica dataset,
achieving improvements up to 2.00% in P@20 and 10.70% in MRR@20. Resources have
been made publicly available in our GitHub repository
https://github.com/liang-tian-tian/MGCOT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Large Vision-Language Model as User Intent-aware Encoder for
  Composed Image Retrieval <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zelong Sun, Dong Jing, Guoxing Yang, Nanyi Fei, Zhiwu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Composed Image Retrieval (CIR) aims to retrieve target images from candidate
set using a hybrid-modality query consisting of a reference image and a
relative caption that describes the user intent. Recent studies attempt to
utilize Vision-Language Pre-training Models (VLPMs) with various fusion
strategies for addressing the task.However, these methods typically fail to
simultaneously meet two key requirements of CIR: comprehensively extracting
visual information and faithfully following the user intent. In this work, we
propose CIR-LVLM, a novel framework that leverages the large vision-language
model (LVLM) as the powerful user intent-aware encoder to better meet these
requirements. Our motivation is to explore the advanced reasoning and
instruction-following capabilities of LVLM for accurately understanding and
responding the user intent. Furthermore, we design a novel hybrid intent
instruction module to provide explicit intent guidance at two levels: (1) The
task prompt clarifies the task requirement and assists the model in discerning
user intent at the task level. (2) The instance-specific soft prompt, which is
adaptively selected from the learnable prompt pool, enables the model to better
comprehend the user intent at the instance level compared to a universal prompt
for all instances. CIR-LVLM achieves state-of-the-art performance across three
prominent benchmarks with acceptable inference efficiency. We believe this
study provides fundamental insights into CIR-related fields.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RecSys Arena: Pair-wise Recommender System Evaluation with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Wu, Qinglin Jia, Chuhan Wu, Zhaocheng Du, Shuai Wang, Zan Wang, Zhenhua Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating the quality of recommender systems is critical for algorithm
design and optimization. Most evaluation methods are computed based on offline
metrics for quick algorithm evolution, since online experiments are usually
risky and time-consuming. However, offline evaluation usually cannot fully
reflect users' preference for the outcome of different recommendation
algorithms, and the results may not be consistent with online A/B test.
Moreover, many offline metrics such as AUC do not offer sufficient information
for comparing the subtle differences between two competitive recommender
systems in different aspects, which may lead to substantial performance
differences in long-term online serving. Fortunately, due to the strong
commonsense knowledge and role-play capability of large language models (LLMs),
it is possible to obtain simulated user feedback on offline recommendation
results. Motivated by the idea of LLM Chatbot Arena, in this paper we present
the idea of RecSys Arena, where the recommendation results given by two
different recommender systems in each session are evaluated by an LLM judger to
obtain fine-grained evaluation feedback. More specifically, for each sample we
use LLM to generate a user profile description based on user behavior history
or off-the-shelf profile features, which is used to guide LLM to play the role
of this user and evaluate the relative preference for two recommendation
results generated by different models. Through extensive experiments on two
recommendation datasets in different scenarios, we demonstrate that many
different LLMs not only provide general evaluation results that are highly
consistent with canonical offline metrics, but also provide rich insight in
many subjective aspects. Moreover, it can better distinguish different
algorithms with comparable performance in terms of AUC and nDCG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Algorithmic Collusion or Competition: the Role of Platforms' Recommender
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14548v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14548v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingchen Xu, Stephanie Lee, Yong Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent scholarly work has extensively examined the phenomenon of algorithmic
collusion driven by AI-enabled pricing algorithms. However, online platforms
commonly deploy recommender systems that influence how consumers discover and
purchase products, thereby shaping the reward structures faced by pricing
algorithms and ultimately affecting competition dynamics and equilibrium
outcomes. To address this gap in the literature and elucidate the role of
recommender systems, we propose a novel repeated game framework that integrates
several key components. We first develop a structural search model to
characterize consumers' decision-making processes in response to varying
recommendation sets. This model incorporates both observable and unobservable
heterogeneity in utility and search cost functions, and is estimated using
real-world data. Building on the resulting consumer model, we formulate
personalized recommendation algorithms designed to maximize either platform
revenue or consumer utility. We further introduce pricing algorithms for
sellers and integrate all these elements to facilitate comprehensive numerical
experiments. Our experimental findings reveal that a revenue-maximizing
recommender system intensifies algorithmic collusion, whereas a
utility-maximizing recommender system encourages more competitive pricing
behavior among sellers. Intriguingly, and contrary to conventional insights
from the industrial organization and choice modeling literature, increasing the
size of recommendation sets under a utility-maximizing regime does not
consistently enhance consumer utility. Moreover, the degree of horizontal
differentiation moderates this phenomenon in unexpected ways. The "more is
less" effect does not arise at low levels of differentiation, but becomes
increasingly pronounced as horizontal differentiation increases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>54 pages, 4 figures, 16 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-14T00:00:00Z">2024-12-14</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">12</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRENER: A Character Relation Enhanced Chinese NER Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaqiong Qiao, Shixuan Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chinese Named Entity Recognition (NER) is an important task in information
extraction, which has a significant impact on downstream applications. Due to
the lack of natural separators in Chinese, previous NER methods mostly relied
on external dictionaries to enrich the semantic and boundary information of
Chinese words. However, such methods may introduce noise that affects the
accuracy of named entity recognition. To this end, we propose a character
relation enhanced Chinese NER model (CRENER). This model defines four types of
tags that reflect the relationships between characters, and proposes a
fine-grained modeling of the relationships between characters based on three
types of relationships: adjacency relations between characters, relations
between characters and tags, and relations between tags, to more accurately
identify entity boundaries and improve Chinese NER accuracy. Specifically, we
transform the Chinese NER task into a character-character relationship
classification task, ensuring the accuracy of entity boundary recognition
through joint modeling of relation tags. To enhance the model's ability to
understand contextual information, WRENER further constructed an adapted
transformer encoder that combines unscaled direction-aware and distance-aware
masked self-attention mechanisms. Moreover, a relationship representation
enhancement module was constructed to model predefined relationship tags,
effectively mining the relationship representations between characters and
tags. Experiments conducted on four well-known Chinese NER benchmark datasets
have shown that the proposed model outperforms state-of-the-art baselines. The
ablation experiment also demonstrated the effectiveness of the proposed model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Why Not Together? A Multiple-Round Recommender System for Queries and
  Items <span class="chip">KDD 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Jin, Xianyu Chen, Weinan Zhang, Yong Yu, Jun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A fundamental technique of recommender systems involves modeling user
preferences, where queries and items are widely used as symbolic
representations of user interests. Queries delineate user needs at an abstract
level, providing a high-level description, whereas items operate on a more
specific and concrete level, representing the granular facets of user
preference. While practical, both query and item recommendations encounter the
challenge of sparse user feedback. To this end, we propose a novel approach
named Multiple-round Auto Guess-and-Update System (MAGUS) that capitalizes on
the synergies between both types, allowing us to leverage both query and item
information to form user interests. This integrated system introduces a
recursive framework that could be applied to any recommendation method to
exploit queries and items in historical interactions and to provide
recommendations for both queries and items in each interaction round. Empirical
results from testing 12 different recommendation methods demonstrate that
integrating queries into item recommendations via MAGUS significantly enhances
the efficiency, with which users can identify their preferred items during
multiple-round interactions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>KDD 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learned Data Compression: Challenges and Opportunities for the Future 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyu Liu, Siyuan Han, Jianwei Liao, Jin Li, Jingshu Peng, Jun Du, Lei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compressing integer keys is a fundamental operation among multiple
communities, such as database management (DB), information retrieval (IR), and
high-performance computing (HPC). Recent advances in \emph{learned indexes}
have inspired the development of \emph{learned compressors}, which leverage
simple yet compact machine learning (ML) models to compress large-scale sorted
keys. The core idea behind learned compressors is to \emph{losslessly} encode
sorted keys by approximating them with \emph{error-bounded} ML models (e.g.,
piecewise linear functions) and using a \emph{residual array} to guarantee
accurate key reconstruction.
  While the concept of learned compressors remains in its early stages of
exploration, our benchmark results demonstrate that an SIMD-optimized learned
compressor can significantly outperform state-of-the-art CPU-based compressors.
Drawing on our preliminary experiments, this vision paper explores the
potential of learned data compression to enhance critical areas in DBMS and
related domains. Furthermore, we outline the key technical challenges that
existing systems must address when integrating this emerging methodology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Event Extraction from Short Stories through Contextualized
  <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaitanya Kirti, Ayon Chattopadhyay, Ashish Anand, Prithwijit Guha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event extraction is an important natural language processing (NLP) task of
identifying events in an unstructured text. Although a plethora of works deal
with event extraction from new articles, clinical text etc., only a few works
focus on event extraction from literary content. Detecting events in short
stories presents several challenges to current systems, encompassing a
different distribution of events as compared to other domains and the portrayal
of diverse emotional conditions. This paper presents \texttt{Vrittanta-EN}, a
collection of 1000 English short stories annotated for real events. Exploring
this field could result in the creation of techniques and resources that
support literary scholars in improving their effectiveness. This could
simultaneously influence the field of Natural Language Processing. Our
objective is to clarify the intricate idea of events in the context of short
stories. Towards the objective, we collected 1,000 short stories written mostly
for children in the Indian context. Further, we present fresh guidelines for
annotating event mentions and their categories, organized into \textit{seven
distinct classes}. The classes are {\tt{COGNITIVE-MENTAL-STATE(CMS),
COMMUNICATION(COM), CONFLICT(CON), GENERAL-ACTIVITY(GA), LIFE-EVENT(LE),
MOVEMENT(MOV), and OTHERS(OTH)}}. Subsequently, we apply these guidelines to
annotate the short story dataset. Later, we apply the baseline methods for
automatically detecting and categorizing events. We also propose a prompt-based
method for event detection and classification. The proposed method outperforms
the baselines, while having significant improvement of more than 4\% for the
class \texttt{CONFLICT} in event classification task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages, 8 figures, Planning to submit in Elsevier (Computer Speech
  and Language Journal)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sentiment and Hashtag-aware Attentive Deep Neural Network for Multimodal
  Post Popularity Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubhi Bansal, Mohit Kumar, Chandravardhan Singh Raghaw, Nagendra Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media users articulate their opinions on a broad spectrum of subjects
and share their experiences through posts comprising multiple modes of
expression, leading to a notable surge in such multimodal content on social
media platforms. Nonetheless, accurately forecasting the popularity of these
posts presents a considerable challenge. Prevailing methodologies primarily
center on the content itself, thereby overlooking the wealth of information
encapsulated within alternative modalities such as visual demographics,
sentiments conveyed through hashtags and adequately modeling the intricate
relationships among hashtags, texts, and accompanying images. This oversight
limits the ability to capture emotional connection and audience relevance,
significantly influencing post popularity. To address these limitations, we
propose a seNtiment and hAshtag-aware attentive deep neuRal netwoRk for
multimodAl posT pOpularity pRediction, herein referred to as NARRATOR that
extracts visual demographics from faces appearing in images and discerns
sentiment from hashtag usage, providing a more comprehensive understanding of
the factors influencing post popularity Moreover, we introduce a hashtag-guided
attention mechanism that leverages hashtags as navigational cues, guiding the
models focus toward the most pertinent features of textual and visual
modalities, thus aligning with target audience interests and broader social
media context. Experimental results demonstrate that NARRATOR outperforms
existing methods by a significant margin on two real-world datasets.
Furthermore, ablation studies underscore the efficacy of integrating visual
demographics, sentiment analysis of hashtags, and hashtag-guided attention
mechanisms in enhancing the performance of post popularity prediction, thereby
facilitating increased audience relevance, emotional engagement, and aesthetic
appeal.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Movie Recommendation using Web Crawling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10714v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10714v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pronit Raj, Chandrashekhar Kumar, Harshit Shekhar, Amit Kumar, Kritibas Paul, Debasish Jana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In today's digital world, streaming platforms offer a vast array of movies,
making it hard for users to find content matching their preferences. This paper
explores integrating real time data from popular movie websites using advanced
HTML scraping techniques and APIs. It also incorporates a recommendation system
trained on a static Kaggle dataset, enhancing the relevance and freshness of
suggestions. By combining content based filtering, collaborative filtering, and
a hybrid model, we create a system that utilizes both historical and real time
data for more personalized suggestions. Our methodology shows that
incorporating dynamic data not only boosts user satisfaction but also aligns
recommendations with current viewing trends.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures, Accepted and to be published in Proceedings of
  2025 International Conference on Applied Algorithms (ICAA), Kolkata, India,
  Dec 8-10, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Quantile Methods: Improved Top-K Threshold Estimation for
  Traditional and Learned Sparse Indexes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinrui Gou, Yifan Liu, Minghao Shao, Torsten Suel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Top-k threshold estimation is the problem of estimating the score of the k-th
highest ranking result of a search query. A good estimate can be used to speed
up many common top-k query processing algorithms, and thus a number of
researchers have recently studied the problem. Among the various approaches
that have been proposed, quantile methods appear to give the best estimates
overall at modest computational costs, followed by sampling-based methods in
certain cases. In this paper, we make two main contributions. First, we study
how to get even better estimates than the state of the art. Starting from
quantile-based methods, we propose a series of enhancements that give improved
estimates in terms of the commonly used mean under-prediction fraction (MUF).
Second, we study the threshold estimation problem on recently proposed learned
sparse index structures, showing that our methods also work well for these
cases. Our best methods substantially narrow the gap between the state of the
art and the ideal MUF of 1.0, at some additional cost in time and space.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UCDR-Adapter: Exploring Adaptation of <span class="highlight-title">Pre-Train</span>ed Vision-Language Models
  for Universal Cross-Domain Retrieval <span class="chip">WACV 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoyu Jiang, Zhi-Qi Cheng, Gabriel Moreira, Jiawen Zhu, Jingdong Sun, Bukun Ren, Jun-Yan He, Qi Dai, Xian-Sheng Hua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Universal Cross-Domain Retrieval (UCDR) retrieves relevant images from unseen
domains and classes without semantic labels, ensuring robust generalization.
Existing methods commonly employ prompt tuning with pre-trained vision-language
models but are inherently limited by static prompts, reducing adaptability. We
propose UCDR-Adapter, which enhances pre-trained models with adapters and
dynamic prompt generation through a two-phase training strategy. First, Source
Adapter Learning integrates class semantics with domain-specific visual
knowledge using a Learnable Textual Semantic Template and optimizes Class and
Domain Prompts via momentum updates and dual loss functions for robust
alignment. Second, Target Prompt Generation creates dynamic prompts by
attending to masked source prompts, enabling seamless adaptation to unseen
domains and classes. Unlike prior approaches, UCDR-Adapter dynamically adapts
to evolving data distributions, enhancing both flexibility and generalization.
During inference, only the image branch and generated prompts are used,
eliminating reliance on textual inputs for highly efficient retrieval.
Extensive benchmark experiments show that UCDR-Adapter consistently outperforms
ProS in most cases and other state-of-the-art methods on UCDR, U(c)CDR, and
U(d)CDR settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to WACV 2025. Project link:
  https://github.com/fine68/UCDR2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ USM: Unbiased <span class="highlight-title">Survey</span> Modeling for Limiting Negative User Experiences in
  Recommendation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenghui Yu, Peiyi Li, Haoze Wu, Bingfeng Deng, Hongyu Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Negative feedback signals are crucial to guardrail content recommendations
and improve user experience. When these signals are effectively integrated into
recommendation systems, they play a vital role in preventing the promotion of
harmful or undesirable content, thereby contributing to a healthier online
environment. However, the challenges associated with negative signals are
noteworthy. Due to the limited visibility of options for users to express
negative feedback, these signals are often sparse compared to positive signals.
This imbalance can lead to a skewed understanding of user preferences,
resulting in recommendations that prioritize short-term engagement over
long-term satisfaction. Moreover, an over-reliance on positive signals can
create a filter bubble, where users are continuously exposed to content that
aligns with their immediate preferences but may not be beneficial in the long
run. This scenario can ultimately lead to user attrition as audiences become
disillusioned with the quality of the content provided. Additionally, existing
user signals frequently fail to meet specific customized requirements, such as
understanding the underlying reasons for a user's likes or dislikes regarding a
video. This lack of granularity hinders our ability to tailor content
recommendations effectively, as we cannot identify the particular attributes of
content that resonate with individual users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Offline Metric for the Debiasedness of Click Models <span class="chip">SIGIR23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.09560v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.09560v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Romain Deffayet, Philipp Hager, Jean-Michel Renders, Maarten de Rijke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A well-known problem when learning from user clicks are inherent biases
prevalent in the data, such as position or trust bias. Click models are a
common method for extracting information from user clicks, such as document
relevance in web search, or to estimate click biases for downstream
applications such as counterfactual learning-to-rank, ad placement, or fair
ranking. Recent work shows that the current evaluation practices in the
community fail to guarantee that a well-performing click model generalizes well
to downstream tasks in which the ranking distribution differs from the training
distribution, i.e., under covariate shift. In this work, we propose an
evaluation metric based on conditional independence testing to detect a lack of
robustness to covariate shift in click models. We introduce the concept of
debiasedness in click modeling and derive a metric for measuring it. In
extensive semi-synthetic experiments, we show that our proposed metric helps to
predict the downstream performance of click models under covariate shift and is
useful in an off-policy model selection setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGIR23 - Full paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Overview</span> of TREC 2024 Biomedical Generative Retrieval (BioGen) Track 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18069v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18069v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deepak Gupta, Dina Demner-Fushman, William Hersh, Steven Bedrick, Kirk Roberts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the advancement of large language models (LLMs), the biomedical domain
has seen significant progress and improvement in multiple tasks such as
biomedical question answering, lay language summarization of the biomedical
literature, clinical note summarization, etc. However, hallucinations or
confabulations remain one of the key challenges when using LLMs in the
biomedical and other domains. Inaccuracies may be particularly harmful in
high-risk situations, such as medical question answering, making clinical
decisions, or appraising biomedical research. Studies on the evaluation of the
LLMs abilities to ground generated statements in verifiable sources have shown
that models perform significantly worse on lay-user-generated questions, and
often fail to reference relevant sources. This can be problematic when those
seeking information want evidence from studies to back up the claims from LLMs.
Unsupported statements are a major barrier to using LLMs in any applications
that may affect health. Methods for grounding generated statements in reliable
sources along with practical evaluation approaches are needed to overcome this
barrier. Towards this, in our pilot task organized at TREC 2024, we introduced
the task of reference attribution as a means to mitigate the generation of
false statements by LLMs answering biomedical questions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Arctic-Embed 2.0: Multilingual Retrieval Without Compromise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.04506v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.04506v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Puxuan Yu, Luke Merrick, Gaurav Nuti, Daniel Campos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the training methodology of Arctic-Embed 2.0, a set of
open-source text embedding models built for accurate and efficient multilingual
retrieval. While prior works have suffered from degraded English retrieval
quality, Arctic-Embed 2.0 delivers competitive retrieval quality on
multilingual and English-only benchmarks, and supports Matryoshka
Representation Learning (MRL) for efficient embedding storage with
significantly lower compressed quality degradation compared to alternatives. We
detail the design and implementation, presenting several important open
research questions that arose during model development. We conduct experiments
exploring these research questions and include extensive discussion aimed at
fostering further discussion in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2024-12-13T00:00:00Z">2024-12-13</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">8</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recommendation and Temptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Sanzeed Anwar, Paramveer S. Dhillon, Grant Schoenebeck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional recommender systems based on utility maximization and revealed
preferences often fail to capture users' dual-self nature, where consumption
choices are driven by both long-term benefits (enrichment) and desire for
instant gratification (temptation). Consequently, these systems may generate
recommendations that fail to provide long-lasting satisfaction to users. To
address this issue, we propose a novel user model that accounts for this
dual-self behavior and develop an optimal recommendation strategy to maximize
enrichment from consumption. We highlight the limitations of historical
consumption data in implementing this strategy and present an estimation
framework that makes minimal assumptions and leverages explicit user feedback
and implicit choice data to overcome these constraints. We evaluate our
approach through both synthetic simulations and simulations based on real-world
data from the MovieLens dataset. Results demonstrate that our proposed
recommender can deliver superior enrichment compared to several competitive
baseline algorithms that assume a single utility type and rely solely on
revealed preferences. Our work emphasizes the critical importance of optimizing
for enrichment in recommender systems, particularly in temptation-laden
consumption contexts. Our findings have significant implications for content
platforms, user experience design, and the development of responsible AI
systems, paving the way for more nuanced and user-centric recommendation
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agro-STAY : Collecte de données et analyse des informations en
  agriculture alternative issues de YouTube 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10576v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10576v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Maxim, Julien Rabatel, Jean-Marc Douguet, Natalia Grabar, Roberto Interdonato, Sébastien Loustau, Mathieu Roche, Maguelonne Teisseire
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To address the current crises (climatic, social, economic), the
self-sufficiency -- a set of practices that combine energy sobriety,
self-production of food and energy, and self-construction - arouses an
increasing interest. The CNRS STAY project (Savoirs Techniques pour
l'Auto-suffisance, sur YouTube) explores this topic by analyzing techniques
shared on YouTube. We present Agro-STAY, a platform designed for the
collection, processing, and visualization of data from YouTube videos and their
comments. We use Natural Language Processing (NLP) techniques and language
models, which enable a fine-grained analysis of alternative agricultural
practice described online.
  --
  Face aux crises actuelles (climatiques, sociales, \'economiques),
l'auto-suffisance -- ensemble de pratiques combinant sobri\'et\'e
\'energ\'etique, autoproduction alimentaire et \'energ\'etique et
autoconstruction - suscite un int\'er\^et croissant. Le projet CNRS STAY
(Savoirs Techniques pour l'Auto-suffisance, sur YouTube) s'inscrit dans ce
domaine en analysant les savoirs techniques diffus\'es sur YouTube. Nous
pr\'esentons Agro-STAY, une plateforme d\'edi\'ee \`a la collecte, au
traitement et \`a la visualisation de donn\'ees issues de vid\'eos YouTube et
de leurs commentaires. En mobilisant des techniques de traitement automatique
des langues (TAL) et des mod\`eles de langues, ce travail permet une analyse
fine des pratiques agricoles alternatives d\'ecrites en ligne.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, in French language, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAGServe: Fast Quality-Aware RAG Systems with Configuration Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddhant Ray, Rui Pan, Zhuohan Gu, Kuntai Du, Ganesh Ananthanarayanan, Ravi Netravali, Junchen Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RAG (Retrieval Augmented Generation) allows LLMs (large language models) to
generate better responses with external knowledge, but using more external
knowledge often improves generation quality at the expense of response delay.
Prior work either reduces the response delay (through better scheduling of RAG
queries) or strives to maximize quality (which involves tuning the RAG
workflow), but they fall short in optimizing the tradeoff between the delay and
quality of RAG responses. This paper presents RAGServe, the first RAG system
that jointly schedules queries and adapts the key RAG configurations of each
query, such as the number of retrieved text chunks and synthesis methods, in
order to balance quality optimization and response delay reduction. Using 4
popular RAG-QA datasets, we show that compared with the state-of-the-art RAG
optimization schemes, RAGServe reduces the generation latency by
$1.64-2.54\times$ without sacrificing generation quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CRS Arena: Crowdsourced Benchmarking of Conversational Recommender
  Systems <span class="chip">WSDM '25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nolwenn Bernard, Hideaki Joko, Faegheh Hasibi, Krisztian Balog
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce CRS Arena, a research platform for scalable benchmarking of
Conversational Recommender Systems (CRS) based on human feedback. The platform
displays pairwise battles between anonymous conversational recommender systems,
where users interact with the systems one after the other before declaring
either a winner or a draw. CRS Arena collects conversations and user feedback,
providing a foundation for reliable evaluation and ranking of CRSs. We conduct
experiments with CRS Arena on both open and closed crowdsourcing platforms,
confirming that both setups produce highly correlated rankings of CRSs and
conversations with similar characteristics. We release CRSArena-Dial, a dataset
of 474 conversations and their corresponding user feedback, along with a
preliminary ranking of the systems based on the Elo rating system. The platform
is accessible at https://iai-group-crsarena.hf.space/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the Eighteenth ACM International Conference on Web
  Search and Data Mining (WSDM '25), March 10--14, 2025, Hannover, Germany</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MST-R: Multi-Stage Tuning for Retrieval Systems and Metric Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Malviya, Karan Dhingra, Maneesh Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regulatory documents are rich in nuanced terminology and specialized
semantics. FRAG systems: Frozen retrieval-augmented generators utilizing
pre-trained (or, frozen) components face consequent challenges with both
retriever and answering performance. We present a system that adapts the
retriever performance to the target domain using a multi-stage tuning (MST)
strategy. Our retrieval approach, called MST-R (a) first fine-tunes encoders
used in vector stores using hard negative mining, (b) then uses a hybrid
retriever, combining sparse and dense retrievers using reciprocal rank fusion,
and then (c) adapts the cross-attention encoder by fine-tuning only the top-k
retrieved results. We benchmark the system performance on the dataset released
for the RIRAG challenge (as part of the RegNLP workshop at COLING 2025). We
achieve significant performance gains obtaining a top rank on the RegNLP
challenge leaderboard. We also show that a trivial answering approach games the
RePASs metric outscoring all baselines and a pre-trained Llama model. Analyzing
this anomaly, we present important takeaways for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Static Pruning in Dense Retrieval using Matrix Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09983v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09983v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Siciliano, Francesca Pezzuti, Nicola Tonellotto, Fabrizio Silvestri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the era of dense retrieval, document indexing and retrieval is largely
based on encoding models that transform text documents into embeddings. The
efficiency of retrieval is directly proportional to the number of documents and
the size of the embeddings. Recent studies have shown that it is possible to
reduce embedding size without sacrificing - and in some cases improving - the
retrieval effectiveness. However, the methods introduced by these studies are
query-dependent, so they can't be applied offline and require additional
computations during query processing, thus negatively impacting the retrieval
efficiency. In this paper, we present a novel static pruning method for
reducing the dimensionality of embeddings using Principal Components Analysis.
This approach is query-independent and can be executed offline, leading to a
significant boost in dense retrieval efficiency with a negligible impact on the
system effectiveness. Our experiments show that our proposed method reduces the
dimensionality of document representations by over 50% with up to a 5%
reduction in NDCG@10, for different dense retrieval models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hesitation and Tolerance in Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuan Zou, Aixin Sun, Xuemeng Jiang, Yitong Ji, Hao Zhang, Jing Wang, Ruijie Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User interactions in recommender systems are inherently complex, often
involving behaviors that go beyond simple acceptance or rejection. One
particularly common behavior is hesitation, where users deliberate over
recommended items, signaling uncertainty. Our large-scale surveys, with 6,644
and 3,864 responses respectively, confirm that hesitation is not only
widespread but also has a profound impact on user experiences. When users spend
additional time engaging with content they are ultimately uninterested in, this
can lead to negative emotions, a phenomenon we term as tolerance. The surveys
reveal that such tolerance behaviors often arise after hesitation and can erode
trust, satisfaction, and long-term loyalty to the platform. For instance, a
click might reflect a need for more information rather than genuine interest,
and prolonged exposure to unsuitable content amplifies frustration. This
misalignment between user intent and system interpretation introduces noise
into recommendation training, resulting in suggestions that increase
uncertainty and disengagement. To address these issues, we identified signals
indicative of tolerance behavior and analyzed datasets from both e-commerce and
short-video platforms. The analysis shows a strong correlation between
increased tolerance behavior and decreased user activity. We integrated these
insights into the training process of a recommender system for a major
short-video platform. Results from four independent online A/B experiments
demonstrated significant improvements in user retention, achieved with minimal
additional computational costs. These findings underscore the importance of
recognizing hesitation as a ubiquitous user behavior and addressing tolerance
to enhance satisfaction, build trust, and sustain long-term engagement in
recommender systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 6 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Navigating the Future of Federated Recommendation Systems with
  Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00004v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00004v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiwei Li, Guodong Long, Chunxu Zhang, Honglei Zhang, Jing Jiang, Chengqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the integration of federated learning (FL) and
recommendation systems (RS), known as Federated Recommendation Systems (FRS),
has attracted attention for preserving user privacy by keeping private data on
client devices. However, FRS faces inherent limitations such as data
heterogeneity and scarcity, due to the privacy requirements of FL and the
typical data sparsity issues of RSs. Models like ChatGPT are empowered by the
concept of transfer learning and self-supervised learning, so they can be
easily applied to the downstream tasks after fine-tuning or prompting. These
models, so-called Foundation Models (FM), fouce on understanding the human's
intent and perform following their designed roles in the specific tasks, which
are widely recognized for producing high-quality content in the image and
language domains. Thus, the achievements of FMs inspire the design of FRS and
suggest a promising research direction: integrating foundation models to
address the above limitations. In this study, we conduct a comprehensive review
of FRSs with FMs. Specifically, we: 1) summarise the common approaches of
current FRSs and FMs; 2) review the challenges posed by FRSs and FMs; 3)
discuss potential future research directions; and 4) introduce some common
benchmarks and evaluation metrics in the FRS field. We hope that this position
paper provides the necessary background and guidance to explore this
interesting and emerging topic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, position paper, survey</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2024-12-21T05:24:32.800603434Z">
            2024-12-21 05:24:32 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
